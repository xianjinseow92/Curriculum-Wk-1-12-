{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping 1: BeautifulSoup\n",
    "\n",
    "## Docs and installation\n",
    "\n",
    "[BeautifulSoup documentation](http://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "\n",
    "To install `BeautifulSoup` and the `requests` modules, run the following in your terminal:\n",
    "```bash\n",
    "pip install beautifulsoup4 requests\n",
    "```\n",
    "\n",
    "If you have errors, you many need to install the _parsers_:\n",
    "```bash\n",
    "pip install lxml html5lib\n",
    "```\n",
    "\n",
    "## What is it?\n",
    "\n",
    "BeautifulSoup is an HTML parser, which means that it takes HTML (which is basically a plain text file) and interprets the structure of that file to help you navigate it easily.\n",
    "\n",
    "It **doesn't** actually get the pages from the web. To do that, we usually use the requests library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML\n",
    "\n",
    "HTML is the basic langauge used to create a webpage. It is a hierarchical list of elemnt with properties. A typical format for an element is\n",
    "```html\n",
    "<tag-name attr1=\"value of attr1\" attr2=\"value of attr2\" .... attrN=\"value of attrN\">\n",
    "    Inner text of the tag\n",
    "</tag-name>\n",
    "```\n",
    "\n",
    "We see several things here:\n",
    "* `tag-name`: This tells us what sort of \"thing\" we are represting on a page. Common examples of tags:\n",
    "  * `h1`, `h2`, ...., `h6`: headers\n",
    "  * `a`: Anchors (i.e. links)\n",
    "  * `p`: Paragraphs\n",
    "  * `ol`: ordered lists\n",
    "  * `ul`: unordered lists\n",
    "  * `li`: list items\n",
    "  * `div`: Division (or section) of a page\n",
    "  * `img`: An image\n",
    "  \n",
    "  (Almost) every tag has an beginning (e.g. `<tag>`) and an end (`</tag>`)\n",
    "* `attributes`: Special properties we want this tag to have. There are four really common examples:\n",
    "  * `href`: Hyperlink reference. If you click on this element, where do you go?\n",
    "  * `class`: Style information about an element. Many elements can have the same class\n",
    "  * `id`: Unique identifier for this element\n",
    "  * `style`: Extra styling information we want applied to just this element (doing this is bad practice, you should be using CSS instead)\n",
    "  \n",
    "For example, here is a heading tag\n",
    "```html\n",
    "<h3 style=\"color:red;\" id='top_heading'>This is a heading</h3>\n",
    "```\n",
    "This element has:\n",
    "* a tag `h3`\n",
    "* two attributes, `style` and `heading`\n",
    "* inner text of `This is a heading`\n",
    "Markdown will actually render HTML as well. In the cell below, we create a Markdown cell and copy the HTML:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An example\n",
    "(Copied HTML from above)\n",
    "<h3 style=\"color:red;\" id='top_heading'>This is a heading</h3>\n",
    "(End HTML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self-closing\n",
    "A few tags don't have inner HMTL and are \"self-closing\". That is, there is no `</tag>`.\n",
    "\n",
    "The most well-known one is the image tag:\n",
    "```html\n",
    "<img ....... />\n",
    "```\n",
    "\n",
    "For example, you can probably guess what this will do:\n",
    "```html\n",
    "<img width=\"300px\" src=\"https://imgs.xkcd.com/comics/boyfriend.png\" />\n",
    "```\n",
    "\n",
    "Let's put it in this cell and find out:\n",
    "<img width=\"300px\" src=\"https://imgs.xkcd.com/comics/boyfriend.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Starting to scrape\n",
    "\n",
    "Let's start scraping on some dummy HTML. By that, I mean we are just going to create some HTML as a string in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_html = \"\"\"\n",
    "<html>\n",
    "<head>\n",
    "</head>\n",
    "<body>\n",
    "<Div style=\"border: 1px solid\">\n",
    "There isn't much in this file, except a list of to-do items\n",
    "\n",
    "<ul>\n",
    "  <li>Feed the cat</li>\n",
    "  <li>Wash the dished</li>\n",
    "  <li>Make coffee</li>\n",
    "  <li>Go to the store</li>\n",
    "  <li>Write BeautifulSoup lecture</li>\n",
    "</ul>\n",
    "</div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see our \"webpage\" (i.e. the webpage our string would make)\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(my_html))  # make sure Jupyter knows to display it as HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we wanted to get all five \"todo\" items into a list in Python so we could analyze them. This is a task for BeautifulSoup!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(my_html, 'html5lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a list item: soup.find\n",
    "\n",
    "If we just look at soup, we might be unimpressed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This doesn't look so impressive\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But we can use soup.find to get a specific element:\n",
    "soup.find('li')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also just grab the inner text\n",
    "soup.find('li').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting all the list items: soup.find_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('li')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hey, that is pretty neat! What sort of things are in the list? Strings?\n",
    "type(soup.find_all('li')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a list with just the to-do text, (no <li> ...</li>)\n",
    "todos = []\n",
    "for element in soup.find_all('li'):\n",
    "    todos.append(element.text)\n",
    "\n",
    "print(todos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We did it! But we can use list comprehensions to be tidier:\n",
    "todos = [element.text for element in soup.find_all('li')]\n",
    "todos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: A more complicated example\n",
    "\n",
    "Have a look at the webpage in `first_webpage/page.html`. Can we grab all links from the articles (i.e. the link about Starbucks and the link about Bitcoin). We need a two step process:\n",
    "1. Get the HTML (i.e. read the file)\n",
    "2. Parse it using soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('first_webpage/page.html') as website:\n",
    "    html2 = website.read()\n",
    "soup = BeautifulSoup(html2, 'html5lib')\n",
    "\n",
    "display(HTML(html2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for links, so let's look for 'a' tags\n",
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh-oh! It seems there are other links on the page as well in the sidebar. We want a way of just getting the article links. The last link is in the disclaimer as well.\n",
    "\n",
    "Looking at the source in detail, we can see that all the articles are in a `div` with the class `article`. Let's start with that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab all the div with class \"article\" from the page\n",
    "soup.find_all('div', class_='article')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each eleemnt of this list is _also_ a soup object. So our plan will be\n",
    "- Grab the \"article\" divs\n",
    "- For each article div, grab all the links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in soup.find_all('div', class_='article'):\n",
    "    for link in article.find_all('a'):\n",
    "        \n",
    "        print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did it! \n",
    "\n",
    "We can do better. Let's get the text, and what it points to.\n",
    "e.g.\n",
    "> unicorn frappanccino --> http://starbucks.com/drinks/unicorn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in soup.find_all('div', class_='article'):\n",
    "    for link in article.find_all('a'):\n",
    "        print(f'{link.text:20s} --> {link.get(\"href\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Getting a list of all the presidents\n",
    "\n",
    "Now we are going to get some information from the web, instead of just getting a local file.\n",
    "\n",
    "The requests library is the standard way of grabbing information. The two most common types of requests are \n",
    "* `GET`: Doesn't change information, simply requests new info. When you put a URL into a browser, you are making a \"get\" request\n",
    "* `POST`: Sends info to the website (e.g. when writing an email in Gmail)\n",
    "\n",
    "A get request has the form:\n",
    "`requests.get(url, params = {})`\n",
    "where `params` are arguments placed at the end of the URL e.g. \n",
    "```\n",
    "requests.get(\"http://google.com/search\", {'search': 'metis'})\n",
    "```\n",
    "returns the same website as `http://google.com/search?search=metis`\n",
    "\n",
    "The object returned is typically called response, and has the following properties:\n",
    "* `response.text`: The HTML (if any) that is returned.\n",
    "* `response.json`: The JSON returned (if any). This is typically used by APIs\n",
    "* `response.status_code`: a code that tells you if your request was successful, or the type of error that occured. A 200 means \"success\" (and 2XX is generally successful). A 404 means \"not found\". There are a lot of status errors to help you debug what went wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get('https://www.britannica.com/place/United-States/Presidents-of-the-United-States')\n",
    "\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look at the page\n",
    "display(HTML(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html5lib')\n",
    "\n",
    "table = soup.find('table')\n",
    "\n",
    "president_info = []\n",
    "for president_row in table.find_all('tr')[3:]:\n",
    "    # ignore column 0 with the picture\n",
    "    desired_info = [e.text for e in president_row.find_all('td')][1:]\n",
    "    president_info.append(desired_info)\n",
    "president_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(president_info, columns=['Name', 'Birthplace', 'Party', 'Term'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Gettting info from the box office\n",
    "\n",
    "We are going to use boxofficemojo in order to get information from the web.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed: pip install requests\n",
    "\n",
    "url = 'http://boxofficemojo.com/movies/?id=biglebowski.htm'\n",
    "\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For information on HTTP status codes, see:\n",
    "\n",
    "https://en.wikipedia.org/wiki/List_of_HTTP_status_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed: pip install beautifulsoup4\n",
    "## pip install --upgrade bs4\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(page,\"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `soup.find()`\n",
    "\n",
    "`soup.find()` and its partner, `find_all`, are the most common functions we will use from this package. \n",
    "\n",
    "The syntax is\n",
    "```\n",
    "# Finds the FIRST tagname with attr1 equal to value1 AND attr2 equal to value2\n",
    "soup.find('tagname', attr1='value1', attr2='value2', ...)\n",
    "```\n",
    "\n",
    "Sometimes attributes have names that are not legal Python (e.g. `data-value=\"23\"`). We can use dictionary notation instead:\n",
    "```\n",
    "soup.find('tagname', {'attr1': 'value1', 'attr2':'value2', ....})\n",
    "```\n",
    "For the keyword `class`, it is so common that there is a special keyword for it, namely `class_`.\n",
    "\n",
    "Let's try out some common variations of `soup.find()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup.find() returns the first matched tag it finds.\n",
    "# It searches the entire tree.\n",
    "\n",
    "# Search for a type of tag by using the tag as a string\n",
    "# (like 'body','div','p','a') as an argument.\n",
    "\n",
    "print(soup.find('a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalently:\n",
    "print(soup.a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prettier:\n",
    "print(soup.a.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup.find_all() returns a list of all matches\n",
    "\n",
    "for link in soup.find_all('a'): \n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the url from an anchor tag\n",
    "soup.find('a')['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can match on an attribute like an id or class.\n",
    "# Take a look at what the 'mp_box_content' classes\n",
    "# look like on the webpage, with Inspect Element.\n",
    "\n",
    "for element in soup.find_all(class_='mp_box_content'):\n",
    "   print(element, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can find all the columns in the first mp_box_content table\n",
    "# by \"chaining\" `find` and `find_all`.\n",
    "\n",
    "print(soup.find(class_='mp_box_content').find_all('td'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To extract just the value of interest:\n",
    "\n",
    "soup.find(class_='mp_box_content').find_all('td')[1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful with non-printing characters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find with an \"id\". (ID is unique.)\n",
    "\n",
    "print(soup.find(id='hp_footer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Consistency\n",
    "Web scraping is made simple by the consistent format of information among like pages of a website. \n",
    "\n",
    "###Items to scrape for each movie:\n",
    "* movie title\n",
    "* total domestic gross\n",
    "* release date\n",
    "* runtime\n",
    "* rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movie Title\n",
    "\n",
    "print(soup.find('title'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_string = soup.find('title').text\n",
    "print(title_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(title_string.split('('))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = title_string.split('(')[0].strip()\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domestic Total Gross\n",
    "\n",
    "## text does an exact match search!\n",
    "print(soup.find(text=\"Domestic Total Gross\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You could find a perfect match:\n",
    "\n",
    "print(soup.find(text=\"Domestic Total Gross: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You could also use [regular expressions](https://xkcd.com/208/).\n",
    "\n",
    "import re\n",
    "domestic_total_regex = re.compile('Domestic Total')\n",
    "soup.find(text=domestic_total_regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtg_string = soup.find(text=re.compile('Domestic Total'))\n",
    "print(dtg_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dtg_string.findNextSibling())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtg = dtg_string.findNextSibling().text\n",
    "dtg = dtg.replace('$','').replace(',','')\n",
    "domestic_total_gross = int(dtg)\n",
    "print(domestic_total_gross)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###We can actually do several of these using the text matching method, so let's make a function for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_value(soup, field_name):\n",
    "    '''Grab a value from boxofficemojo HTML\n",
    "    \n",
    "    Takes a string attribute of a movie on the page and\n",
    "    returns the string in the next sibling object\n",
    "    (the value for that attribute)\n",
    "    or None if nothing is found.\n",
    "    '''\n",
    "    obj = soup.find(text=re.compile(field_name))\n",
    "    if not obj: \n",
    "        return None\n",
    "    # this works for most of the values\n",
    "    next_sibling = obj.findNextSibling()\n",
    "    if next_sibling:\n",
    "        return next_sibling.text \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# domestic total gross\n",
    "dtg = get_movie_value(soup,'Domestic Total')\n",
    "print(dtg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runtime\n",
    "runtime = get_movie_value(soup,'Runtime')\n",
    "print(runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rating\n",
    "rating = get_movie_value(soup,'MPAA Rating')\n",
    "print(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating = get_movie_value(soup,'Release Date')\n",
    "print(rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need a few helper methods to parse the strings we've gotten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateutil.parser\n",
    "\n",
    "def to_date(datestring):\n",
    "    date = dateutil.parser.parse(datestring)\n",
    "    return date\n",
    "\n",
    "def money_to_int(moneystring):\n",
    "    moneystring = moneystring.replace('$', '').replace(',', '')\n",
    "    return int(moneystring)\n",
    "\n",
    "def runtime_to_minutes(runtimestring):\n",
    "    runtime = runtimestring.split()\n",
    "    try:\n",
    "        minutes = int(runtime[0])*60 + int(runtime[2])\n",
    "        return minutes\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get these again and format them all in one swoop\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "raw_release_date = get_movie_value(soup,'Release Date')\n",
    "release_date = to_date(raw_release_date)\n",
    "\n",
    "raw_domestic_total_gross = get_movie_value(soup,'Domestic Total')\n",
    "domestic_total_gross = money_to_int(raw_domestic_total_gross)\n",
    "\n",
    "raw_runtime = get_movie_value(soup,'Runtime')\n",
    "runtime = runtime_to_minutes(raw_runtime)\n",
    "\n",
    "headers = ['movie title', 'domestic total gross',\n",
    "           'release date', 'runtime (mins)', 'rating']\n",
    "\n",
    "movie_data = []\n",
    "movie_dict = dict(zip(headers, [title,\n",
    "                                domestic_total_gross,\n",
    "                                release_date,\n",
    "                                runtime,\n",
    "                                rating]))\n",
    "movie_data.append(movie_dict)\n",
    "\n",
    "pprint(movie_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about scraping tables? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.boxofficemojo.com/genres/chart/?id=foreign.htm'\n",
    "\n",
    "response=requests.get(url)\n",
    "page=response.text\n",
    "\n",
    "soup=BeautifulSoup(page,\"lxml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables=soup.find_all(\"table\")\n",
    "rows=[row for row in tables[3].find_all('tr')]\n",
    "\n",
    "# Just want to look at 1st 20 rows for now\n",
    "rows=rows[1:20]\n",
    "\n",
    "movies={}\n",
    "for row in rows:\n",
    "    items=row.find_all('td')\n",
    "    title=items[1].find('a')['href']\n",
    "    movies[title]=[i.text for i in items[1:]]\n",
    "    \n",
    "\n",
    "list(movies.items())[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parting thoughts (optional - self review rather than class)\n",
    "\n",
    "BeautifulSoup is very poewrful. There are a few other methods and hints that are useful.\n",
    "\n",
    "One of the biggest hints is to use strings to make \"fake documents\" to test your scraping. \n",
    "\n",
    "One common problem you will encounter that we haven't discussed is how to get a neighboring element. We caan use the \"test document\" method to practice extracting it. It is pretty common in tables to see things like\n",
    "```html\n",
    "...\n",
    "<tr>\n",
    "    <th class=\"num_sales\">Number of Sales</th>\n",
    "    <td>405</td>\n",
    "</tr>\n",
    "...\n",
    "```\n",
    "We can easily find \"num_sales\" using the class, but we really want the (generic) `td` tag containing the data (405). \n",
    "\n",
    "Some terminology:\n",
    "* The 'th' and 'td' tags are contained inside the 'tr' tag. The 'th' and 'td' tags are referred to as \"children\" of the 'tr' tag (and the 'tr' tag is the parent of the 'td' and 'th' tags).\n",
    "* Continuing the family tree analogy, 'th' and 'td' are called siblings\n",
    "Looking at the BeautifulSoup documentation, we see there are methods `next_sibling` and `previous_sibling`. Let's write a test document and see if we can get the data we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# We will add another row to amke sure we are not just grabbing the first row in the table\n",
    "test_html = '''\n",
    "<table>\n",
    "  <tr>\n",
    "    <th class=\"product_name\">Product name</th>\n",
    "    <td>Peanut butter</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th class=\"num_sales\">Number of Sales</th>\n",
    "    <td>405</td>\n",
    "  </tr>\n",
    "'''\n",
    "\n",
    "soup = BeautifulSoup(test_html, 'html5lib')\n",
    "\n",
    "easy_to_find = soup.find('th', class_='num_sales')\n",
    "easy_to_find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_to_find.next_sibling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weird. Let's try the next, next sibling\n",
    "easy_to_find.next_sibling.next_sibling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# found it!\n",
    "easy_to_find.next_sibling.next_sibling.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metis",
   "language": "python",
   "name": "metis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
