{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some fake data for linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-20,20)\n",
    "y = 0.3 * x + np.random.randn(40)\n",
    "plt.scatter(x,y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, let's assume we know the intercept (zero) and noise distribution (standard normal). Our goal is just to find the slope. So for the $n$th point, we have\n",
    "\n",
    "$$\n",
    "y_n \\sim \\mathcal{N}(\\beta x_n, 1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume the $y$ values are independent. By the current frequentist approach, the paramter $\\beta$ is assumed to be unknown, but fixed. So the probability of the observation is just"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P\\left(y\\ |\\ \\beta\\right)\t\n",
    "= \\prod_{N\\text{ points}} P(y_n\\ |\\ \\beta)\n",
    "=\\prod_{N\\text{ points}}\\mathcal{N}\\left(y_{n}\\ |\\ \\beta x_{n},1\\right)\n",
    "\t=\\prod_{N\\text{ points}}\\left(\\frac{1}{\\sqrt{2\\pi}}\\exp\\left\\{ \\frac{-\\left(y_{n}-\\beta x_{n}\\right)^2}{2}\\right\\} \\right)\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've been considering this as a function of $y$, but since we've observed data, we're really more interested in it as a function of $\\beta$. This is the *likelihood*, \n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\beta|y) = P(y|\\beta)\n",
    "$$\n",
    "\n",
    "An analogy might be helpful in understanding this. If I add up the amount you spend on housing, food, ice cream etc plus the amount saved, it should equal your income. There's no getting around that, just as with a probability distribution. But if I add your ice cream costs to your friends, and add the ice cream costs of everyone any of us know... It's no longer constrained. The total ice cream cost might be more than any one of us makes.\n",
    "\n",
    "This is how likelihood works. It's still a probability (or probability density, for continuous variables), but the thing changing isn't the random variable, but the parameter. So there's no requirement the likelihood values add to one.\n",
    "\n",
    "We're interesting in the *maximum likelihood estimate*, which estimates the parameter by maximizing the likelihood function. In other words, **finding the value of the parameter with the highest probability of generating the data we did, in fact, observe**.\n",
    "\n",
    "So combining what we have to this point, we'd like to estimate $\\beta$ by finding the value that maximizes the likelihood,\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\beta|y) = \\prod_{N\\text{ points}}\\left(\\frac{1}{\\sqrt{2\\pi}}\\exp\\left\\{ \\frac{-\\left(y_{n}-\\beta x_{n}\\right)^2}{2}\\right\\} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, so good. What's a good way to maximize things? There are lots, but since we're working in terms of smooth functions, it will probably be quickest to work in terms of derivatives.\n",
    "\n",
    "But differentiating a product is a mess. Do we really have to do it this way? Spoiler: no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be so much easier to work in terms of a sum, instead of a product. Then the derivative would be easy - we could just calculate term by term.\n",
    "\n",
    "But we *can* work in terms of a sum! Let's just take the log of both sides:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\log \\mathcal{L}(\\beta|y) \n",
    "&= \\log \\prod_{N\\text{ points}}\\left(\\frac{1}{\\sqrt{2\\pi}}\\exp\\left\\{ \\frac{-\\left(y_{n}-\\beta x_{n}\\right)^2}{2}\\right\\} \\right) \n",
    "\\\\ &= \\sum_{N\\text{ points}} \\log \\left(\\frac{1}{\\sqrt{2\\pi}}\\exp\\left\\{ \\frac{-\\left(y_{n}-\\beta x_{n}\\right)^2}{2}\\right\\} \\right) \n",
    "\\\\ &= \\sum_{N\\text{ points}} \\left[ \\log \\frac{1}{\\sqrt{2\\pi}} - \\frac{\\left(y_{n}-\\beta x_{n}\\right)^2}{2} \\right]\n",
    "\\\\ &= C - \\frac{1}{2}\\sum_{N\\text{ points}} \\left(y_{n}-\\beta x_{n}\\right)^2\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, time for the big reveal. Look at that line. To maximize that, we could remove the negative and minimize. \n",
    "\n",
    "**For a normal linear model, maximum liklihood estimation reduces to ordinary least squares**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metis",
   "language": "python",
   "name": "metis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
