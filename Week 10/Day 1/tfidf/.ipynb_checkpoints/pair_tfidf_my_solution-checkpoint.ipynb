{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's give Spark a shot!\n",
    "\n",
    "Your challenge is to calculate TFIDF on the text file in the repo, using Spark.  In that\n",
    "text file, each line is a document.  You should have 4 total documents.  Don't be\n",
    "too concerned about getting the formula right; just start by calculating the number\n",
    "of times each word appears for each document and dividing by the\n",
    "total amount of times the word appears in the whole corpus.  If you can get to that point,\n",
    "then try to get the calculation correct according to one of the true formulas for TFIDF.\n",
    "\n",
    "For this simple TFIDF measure, you should get the following for the word \"of\":\n",
    "\n",
    "```\n",
    "document 0: document count: 0.  Corpus count: 24.  Simple TFIDF: 0/24 = 0\n",
    "document 1: document count: 6.  Corpus count: 24.  Simple TFIDF: 6/24 = 0.25\n",
    "document 2: document count: 14. Corpus count: 24.  Simple TFIDF: 14/24 = 0.58333...\n",
    "document 3: document count: 4.  Corpus count: 24.  Simple TFIDF: 4/24 = 0.166....\n",
    "```\n",
    "\n",
    ">Hint: `zipWithIndex()` called on an RDD will zip the elements of an RDD together with an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df = spark.read.text('dull.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df = doc_df.withColumnRenamed('value','docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                docs|\n",
      "+--------------------+\n",
      "|First line This i...|\n",
      "| This is second line|\n",
      "|          third line|\n",
      "|and subsequent li...|\n",
      "|          and more..|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- docs: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already got this part wrong\n",
    "\n",
    "# Calculate tf\n",
    "for word in docs_concat:\n",
    "    for doc_line in docs:\n",
    "        tf = word / len(doc_line)\n",
    "        if word in doc_line\n",
    "        idf = len(docs_column) / "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w(i,j) = tf(i,j) x log( N   /  df (i)   )\n",
    "  \n",
    "tf(i,j) = count of word in document j  \n",
    "N = total # of documents  \n",
    "df(i) = # of docs containing word i  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dull.txt\n",
      "pair_tfidf_in_spark.md\n",
      "pair_tfidf_my_solution.ipynb\n",
      "readme.md\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = sc.textFile(\"dull.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dull.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text  \n",
    "# text is an RDD\n",
    "# file not read in yet. lazy evaluation.\n",
    "\n",
    "\n",
    "# First thing it needs to do an sc-text file. It has only done that.\n",
    "# If you REALLY want to see the file, then you need to do a .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['First line This is a test file',\n",
       " 'This is second line',\n",
       " 'third line',\n",
       " 'and subsequent line..',\n",
       " 'and more..']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.collect() # does an action to execute the map of this\n",
    "\n",
    "# Try not to anyhow use .collect(). Because collect will collect EVERYTHING.\n",
    "# A .collect() takes every single data from every node and pulls it into the machine that you did the .collect() on.\n",
    "# You don't want to do collect on a whole dataset. You only want to do it on a small sample!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['first', 'line', 'this', 'is', 'a', 'test', 'file'],\n",
       " ['this', 'is', 'second', 'line'],\n",
       " ['third', 'line'],\n",
       " ['and', 'subsequent', 'line'],\n",
       " ['and', 'more']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.map(lambda x: x.replace(',',' ').replace('.', \" \").replace('-', ' ').lower().split()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['first', 'line', 'this', 'is', 'a', 'test', 'file'], 0),\n",
       " (['this', 'is', 'second', 'line'], 1),\n",
       " (['third', 'line'], 2),\n",
       " (['and', 'subsequent', 'line'], 3),\n",
       " (['and', 'more'], 4)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.map(lambda x: x.replace(',',' ').replace('.', \" \").replace('-', ' ').lower().split())\\\n",
    ".zipWithIndex().collect()    # this is basically going to create a tuple with the list and the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, ['first', 'line', 'this', 'is', 'a', 'test', 'file']),\n",
       " (1, ['this', 'is', 'second', 'line']),\n",
       " (2, ['third', 'line']),\n",
       " (3, ['and', 'subsequent', 'line']),\n",
       " (4, ['and', 'more'])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.map(lambda x: x.replace(',',' ').replace('.', \" \").replace('-', ' ').lower().split())\\\n",
    ".zipWithIndex()\\\n",
    ".map(lambda x: (x[1], x[0])).collect()    # swpa the position to put the index infront and the list behind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, ['first', 'line', 'this', 'is', 'a', 'test', 'file'], 'hello'),\n",
       " (1, ['this', 'is', 'second', 'line'], 'hello'),\n",
       " (2, ['third', 'line'], 'hello'),\n",
       " (3, ['and', 'subsequent', 'line'], 'hello'),\n",
       " (4, ['and', 'more'], 'hello')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An extended exmaple of the code above\n",
    "text.map(lambda x: x.replace(',',' ').replace('.', \" \").replace('-', ' ').lower().split())\\\n",
    ".zipWithIndex()\\\n",
    ".map(lambda x: (x[1], x[0], 'hello')).collect()    # swpa the position to put the index infront and the list behind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'first'),\n",
       " (0, 'line'),\n",
       " (0, 'this'),\n",
       " (0, 'is'),\n",
       " (0, 'a'),\n",
       " (0, 'test'),\n",
       " (0, 'file'),\n",
       " (1, 'this'),\n",
       " (1, 'is'),\n",
       " (1, 'second'),\n",
       " (1, 'line'),\n",
       " (2, 'third'),\n",
       " (2, 'line'),\n",
       " (3, 'and'),\n",
       " (3, 'subsequent'),\n",
       " (3, 'line'),\n",
       " (4, 'and'),\n",
       " (4, 'more')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.map(lambda x: x.replace(',',' ').replace('.', \" \").replace('-', ' ').lower().split())\\\n",
    ".zipWithIndex()\\\n",
    ".map(lambda x: (x[1], x[0]))\\\n",
    ".flatMapValues(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-3af2e4729900>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdoc_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m                \u001b[1;33m.\u001b[0m\u001b[0mzipWithIndex\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                \u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                \u001b[1;33m.\u001b[0m\u001b[0mflatMapValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "doc_word = text.map(lambda x: x.replace(',',' ').replace('.', \" \").replace('-', ' ').lower().split())\\\n",
    "               .zipWithIndex\\\n",
    "               .map(lambda x: (x[1], x[0])) \\\n",
    "               .flatMapValues(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_word_df = sqlContext.createDataFrame(doc_wrod ['doc_id', 'word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_word_df.registerTempTable(\"doc_word\")   # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting ocurence of words (global) (doc_word)\n",
    "word_count_df = sqlContext.sql(\"\"\"\n",
    "    SELECT word, count(*) as tot_word_count \n",
    "    FROM doc_word\n",
    "    GROUP BY word\n",
    "\"\"\")\n",
    "word_count_.df.show()  # Will get back the global occurence of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(word_count_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting occruence of words (document level) (doc_Word\n",
    "doc_word_count_df = sqlContext.sql(\"\"\"\n",
    "    SELECT doc_id, word, count(*) as doc_word_count\n",
    "    FROM doc_word\n",
    "    \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combien docw ord count and total word ocunt\n",
    "word_df = sqlContext.sql(\"\"\"\n",
    "    SELECT a.doc_id, a.word, a.doc_word_count, b.tot_word_count\n",
    "    FROM doc_word_count a\n",
    "    INNER JOIN word_count b \n",
    "    ON a.word = b.word\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
