{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional  Neural Networks and Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Today we are going to explore ConvNets for image classification. And how you might be able to use them for your final projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are an insane number of possible arrangements of pixels. for a 512x512 image there are $256^{786432}$ possible arrangments of RGB colors. Even Google only sees and infitesimal fraction of the possible images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://silverspaceship.com/static/shot_1.png)\n",
    "luckily for us, natural images share certain properties that make the problem much more tractable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Nearby pixels tend to be similar to eachother, and higher level patterns build on top of each other. pixels form lines, lines form shapes. It is rare for an image to contain large amounts of static."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "this tendency for nearby pixels to be related, means we can exploit this with something called convolutional filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/conv-net2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](img/3D_Convolution_Animation.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Max pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](img/Max_pooling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What are the layers learning?\n",
    "\n",
    "First layer\n",
    "![](img/filt1.jpg)\n",
    "\n",
    "Looks like Gabor Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Subsequent Layers get harder to visualize. ![But Inception networks can give it a go](http://yosinski.com/static/proj/deepvis_all_layers.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First few layers of a CNN is a FEATURE EXTRACTOR!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Famous Pre-trained Image Models\n",
    "* VGG16\n",
    "* VGG19\n",
    "* InceptionV3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](img/vgg16_croped.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Download VGG-16 trained weights from [Here](https://github.com/fchollet/deep-learning-models/releases)\n",
    "\n",
    "You will want the [tensorflow h5 file](https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5). Note this file is a little over half a gigabyte, so it will take a while to download.\n",
    "\n",
    "Once you have downloaded it, change `weight_file` below to where you saved the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "weight_file = '/Users/damien/Downloads/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "\n",
    "if not os.path.exists(weight_file):\n",
    "    raise FileNotFoundError(\"No file {weight_file} found. Check path again\".format(weight_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download labels too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!curl https://raw.githubusercontent.com/torch/tutorials/master/7_imagenet_classification/synset_words.txt -o synset_words.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Convolutional Neural Network Architecture in Practice\n",
    "\n",
    "### VGG (Loading a Pretrained Network, first the harder & less stable way)\n",
    "\n",
    "See section below for the better way to load a pretrained network using keras utilities.\n",
    "\n",
    "The hard way is to explicitly define the network structure, and then go find and manually load the learned weights that correspond to that structure. Someone has already trained this network on a large dataset (imagenet), has saved those weights, and has made them publicly downloadable.\n",
    "\n",
    "**For your image projects, you would very likely want to use a pretrained network as the basis for an image classifier**. You might use something more recent than VGG like ResNet or Google Inception. You would use transfer learning on top of this, as detailed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Activation\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG_16(weights_path=None):\n",
    "    # This is the structure of the neural network\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1,1),input_shape=(3,224,224)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    # this part is no longer CNN\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))  # randomly drops out half the nodes\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))  # randomly drops out half the nodes. serves to regularize model\n",
    "    model.add(Dense(1000, activation='softmax'))\n",
    "\n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "synset = pd.read_csv('synset_words.txt', skipinitialspace=True, names = ['synset', 'words'])\n",
    "\n",
    "# note that we don't actually train/adjust the weights at all here\n",
    "model = VGG_16(weight_file)  # this part is to load the weights into the model\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def convert_image_to_bgr_numpy_array(image_path, size=(224,224)):\n",
    "    \"\"\"The network has been trained using opencv and BGR images \n",
    "    (i.e. channels order blue, green, red rather than red, green, blue).\n",
    "    The description of why is https://stackoverflow.com/questions/14556545/why-opencv-using-bgr-colour-space-instead-of-rgb\n",
    "    \n",
    "    We can use a simpler image library as long as we manually convert\n",
    "    the data to the expected format.\n",
    "    \"\"\"\n",
    "    image = PIL.Image.open(image_path).resize(size)\n",
    "    img_data = np.array(image.getdata(), np.float32).reshape(*size, -1)\n",
    "    # swap R and B channels\n",
    "    img_data = np.flip(img_data, axis=2)\n",
    "    return img_data\n",
    "\n",
    "def prepare_image(image_path):\n",
    "    im = convert_image_to_bgr_numpy_array(image_path)\n",
    "\n",
    "    # these subtractions are just mean centering the images \n",
    "    # based on known means for different color channels\n",
    "    im[:,:,0] -= 103.939\n",
    "    im[:,:,1] -= 116.779\n",
    "    im[:,:,2] -= 123.68\n",
    "    \n",
    "    im = im.transpose((2,0,1)) # adjust from (224, 224, 3) to (3, 224, 224) for keras (for keras you have to tell them aobut how you put in the data)\n",
    "    im = np.expand_dims(im, axis=0) # adjust to (1, 3, 224, 224) for generating keras prediction\n",
    "    return im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](img/dog.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to do this - we're using an explicitly defined model structure and loaded pre-trained weights into it. We can then run the pre-trained model any any image we want, and see which label it predicts. Of course, the predicted label is drawn from the set of labels that the model was trained on (ImageNet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "img = prepare_image('img/dog.jpg')  # this line is to modify image into right size and format\n",
    "# the output of this will give you a matrix\n",
    "\n",
    "out = model.predict(img) \n",
    "# this gives you a lot of numbers\n",
    "\n",
    "\n",
    "y_pred = np.argmax(out) \n",
    "# out of all the numbers above, there is going to be the one that is the largest -- and that is the prediction\n",
    "\n",
    "print(y_pred)\n",
    "print(synset.loc[y_pred].synset) # synset is the ground truth. the label that was used to train the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synset   \n",
    "# # if you type out this code, you will see all the images and the \n",
    "# # categories of which the model is trained on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instead, the preferred and stable way to do this!!\n",
    "\n",
    "A more stable way to do this - using built in keras applications to load the network we want and tell it to download / use pretrained weights. **This is highly recommended over manually defining architectures and loading weights!**\n",
    "\n",
    "Here are [more examples of keras transfer learning](https://keras.io/applications/) with modern pretrained CNNs. Check out the documentation specific to the model(s) you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "\n",
    "#instead of doing evertyhing ourselves, just import VGG16\n",
    "\n",
    "# prepare the image yourself\n",
    "img = prepare_image('img/dog.jpg')\n",
    "\n",
    "# create a model with the VGG16\n",
    "model = VGG16(weights='imagenet')\n",
    "\n",
    "# then do a prediction \n",
    "out = model.predict(img)\n",
    "y_pred = np.argmax(out)\n",
    "\n",
    "\n",
    "# decode_predictions is gives you other predictions in descending order\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Close enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](img/sloth.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "img = prepare_image('img/sloth.jpg')\n",
    "out = model.predict(img)\n",
    "\n",
    "\n",
    "print('Predicted:', decode_predictions(out))\n",
    "# this preidciton actually predicted this as a cup as opposed to a sloth.\n",
    "# This is probably because in the 1000 images that was trained it wasn't trained on a sloth!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It took a huge amount of gpu time/power and data to train this model. More that you will have access to over the next 3 weeks. So what is one to do if you want to do an image related project?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we did above was transfer learning! We didn't do any training by ourselves\n",
    "## We can use transfer learning to just pull out images that we want!! (Transfer learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Cheat!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Transfer Learning\n",
    "\n",
    "it turns out that the lower level featured learned by VGG16 on imagenet are still applicable to other problems with natural images. If we can preserve the lower-level features, we can just train a new model on those features. (In fact, in the case of 'softmax', we can think of this as just training a new multinomial logistic regression, on those convolution features)\n",
    "\n",
    "Lets just snip off last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All we need to do is to snip off the last layer and apply it to our own!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A Caveat\n",
    "\n",
    "if we just add a new layer with default weights, it is going to be very wrong the first iteration. Since it is so wrong, the gradient will be huge, and because we are using back propagation those errors will be sent down stream into the lower level features. This can quickly destroy the rest of the network.\n",
    "\n",
    "In order to retrain this model we must protect the lower-level features, until our new layers have reached more stability. We can do this by freezing those layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then we'll add our new layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "# note we exclude the final dense layers and add one back below, \n",
    "# we would retrain it ourselves\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(3,224,224)) \n",
    " \n",
    "# Freeze convolutional layers  (WE DON'T WANT TO RETRAIN THE ENTIRE MODEL!!)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False    \n",
    "    \n",
    "# Think of this part as the feature extraction\n",
    "# Then when we want to do our own customized prediction (maybe.. corgi or not corgi)\n",
    "## Then we add our own 2 layers, and freeze the feature extractors\n",
    "### Then we do training on our own last 2 layers with our own images of corgis\n",
    "    \n",
    "    \n",
    "x = base_model.output\n",
    "# We are adding the following 2 layers to the output\n",
    "x = Flatten()(x) # flatten from convolution tensor output (remove the last layer) \n",
    "predictions = Dense(2, activation='softmax')(x) # should match # of classes predicted\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9),\n",
    "            loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then you would just train like normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "# i.e. if we had training images and our own labels, we could run\n",
    "model.fit(X_train,y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How much data do you need?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "More!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Actually with this bottleneck approach, you don't need as much. 200-1000 representitive images of each class will give good results. Because\n",
    "* Google has already done most of the hard work\n",
    "* We can use image augmentation to increase our number of training samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Image Augmentation\n",
    "\n",
    "![](img/DataAugmentation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Possible Augmentations:\n",
    "* Scale\n",
    "* Rotation\n",
    "* Skew\n",
    "* Flips\n",
    "* color tinting\n",
    "* blur\n",
    "* crops\n",
    "* ETC\n",
    "\n",
    "As long as you do not destroy the info you are trying to represent.\n",
    "\n",
    "[Check out this](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) for more with keras!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other Deep Learning Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auto-Encoders\n",
    "![](img/Autoencoder.png)\n",
    "\n",
    "Trying to learn an approximation of our input data after compression step. Autoencoders are super similar to Matrix Factorization, and are completely unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Variational Auto-Encoder\n",
    "![](http://kvfrans.com/content/images/2016/08/vae.jpg)\n",
    "If you build in an understanding of noise, you have a Variational Autoencoder (VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Siamese Network\n",
    "![](img/siamese.png)\n",
    "\n",
    "This tends to be used to compare the inputs. \n",
    "\n",
    "- Answer and question matching\n",
    "- standardization\n",
    "- facial recognition\n",
    "- picture captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generative Adversarial Networks\n",
    "![](img/GAN.jpg)\n",
    "Create a Generative model, and a Discrimative model. Pit them against eachother to improve both\n",
    "\n",
    "[Really cool example of Adversarial Networks in action](http://carpedm20.github.io/faces/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Stack GANs\n",
    "Gans can be stacked with awesome results\n",
    "![](http://i.imgur.com/SGzE7vI.jpg)\n",
    "\n",
    "https://arxiv.org/pdf/1612.03242v1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### image analogy\n",
    "\n",
    "![](https://raw.githubusercontent.com/awentzonline/image-analogies/master/examples/images/sugarskull-analogy.jpg)\n",
    "![](https://raw.githubusercontent.com/awentzonline/image-analogies/master/examples/images/image-analogy-explanation.jpg)\n",
    "![](https://raw.githubusercontent.com/awentzonline/image-analogies/master/examples/images/trump-image-analogy.jpg)\n",
    "\n",
    "https://github.com/awentzonline/image-analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "New Architectures are being published every day. So much to read!\n",
    "\n",
    "* [Curated List of Deep Learning papers](https://github.com/ChristosChristofidis/awesome-deep-learning)\n",
    "* [Good reddit post for keeping up with the latest research](https://www.reddit.com/r/MachineLearning/comments/6d7nb1/d_machine_learning_wayr_what_are_you_reading_week/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
