{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Deep-Learning-for-NLP\" data-toc-modified-id=\"Deep-Learning-for-NLP-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Deep Learning for NLP</a></span></li><li><span><a href=\"#Embeddings-Example:-Skipgrams!\" data-toc-modified-id=\"Embeddings-Example:-Skipgrams!-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Embeddings Example: Skipgrams!</a></span><ul class=\"toc-item\"><li><span><a href=\"#Loading-in-Data\" data-toc-modified-id=\"Loading-in-Data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Loading in Data</a></span></li><li><span><a href=\"#Keras-Tools-for-Preprocessing-Text-Data\" data-toc-modified-id=\"Keras-Tools-for-Preprocessing-Text-Data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Keras Tools for Preprocessing Text Data</a></span></li><li><span><a href=\"#Generating-Input-and-Output-Labels\" data-toc-modified-id=\"Generating-Input-and-Output-Labels-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Generating Input and Output Labels</a></span></li><li><span><a href=\"#Creating-the-Model-Architecture\" data-toc-modified-id=\"Creating-the-Model-Architecture-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Creating the Model Architecture</a></span></li><li><span><a href=\"#Compiling-and-Training-the-Model\" data-toc-modified-id=\"Compiling-and-Training-the-Model-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Compiling and Training the Model</a></span></li><li><span><a href=\"#Saving-the-Word-Vectors\" data-toc-modified-id=\"Saving-the-Word-Vectors-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Saving the Word Vectors</a></span></li><li><span><a href=\"#Examining-the-Vectors\" data-toc-modified-id=\"Examining-the-Vectors-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Examining the Vectors</a></span></li></ul></li><li><span><a href=\"#RNN-Example:-Text-Classification\" data-toc-modified-id=\"RNN-Example:-Text-Classification-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>RNN Example: Text Classification</a></span></li><li><span><a href=\"#LSTM-Example:-Sentiment-Analysis\" data-toc-modified-id=\"LSTM-Example:-Sentiment-Analysis-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>LSTM Example: Sentiment Analysis</a></span></li><li><span><a href=\"#CNN-Example:-Sentiment-Analysis\" data-toc-modified-id=\"CNN-Example:-Sentiment-Analysis-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>CNN Example: Sentiment Analysis</a></span></li><li><span><a href=\"#BRNN-Example:-Sentiment-Analysis\" data-toc-modified-id=\"BRNN-Example:-Sentiment-Analysis-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>BRNN Example: Sentiment Analysis</a></span></li><li><span><a href=\"#LSTM-Example:-Text-Generation\" data-toc-modified-id=\"LSTM-Example:-Text-Generation-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>LSTM Example: Text Generation</a></span></li><li><span><a href=\"#Save-Trained-Models\" data-toc-modified-id=\"Save-Trained-Models-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Save Trained Models</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T22:58:43.712784Z",
     "start_time": "2019-05-31T22:58:37.673211Z"
    },
    "init_cell": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "# Basics\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# gensim\n",
    "import gensim\n",
    "\n",
    "# keras\n",
    "np.random.seed(13)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (Dense, Embedding, Reshape, Activation, \n",
    "                          SimpleRNN, LSTM, Convolution1D, \n",
    "                          MaxPooling1D, Dropout, Bidirectional)\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.datasets import imdb, reuters\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "\n",
    "\n",
    "# logging for gensim (set to INFO)\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning for NLP\n",
    "\n",
    "We took a quick look at an NLP task in the previous notebook's Dropout example. In this notebook we'll look at some more complete NLP pipelines and learn how to work with several new types of keras Layers.\n",
    "\n",
    "We'll start with a detailed look at building a network with an Embeddings model, and then we'll run through several examples using more advanced layer types: RNNs, LSTMs, CNNs, and BRNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Embeddings Example: Skipgrams!\n",
    "\n",
    "Let's use keras to train a word embeddings model. Along the way, we'll use some special keras tools to create a complete NLP pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loading in Data\n",
    "\n",
    "We'll work with text data from H.P. Lovecraft's story, [The Nameless City](https://raw.githubusercontent.com/urschrei/lovecraft/master/lovecraft.txt). Each item in our corpus will be a paragraph from the story. Note that there are line breaks in the text that will lead to empty elements, and poems in which each line gets its own item in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T22:58:50.548941Z",
     "start_time": "2019-05-31T22:58:49.949895Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Load in Corpus using Keras utility\n",
    "# We'll use some Lovecraft\n",
    "!curl -o lovecraft.txt https://raw.githubusercontent.com/urschrei/lovecraft/master/lovecraft.txt\n",
    "\n",
    "corpus = open(\"lovecraft.txt\").readlines()[0:200]\n",
    "\n",
    "corpus[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Keras Tools for Preprocessing Text Data\n",
    "\n",
    "`Keras` has some nice text preprocessing functions too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T22:58:52.455081Z",
     "start_time": "2019-05-31T22:58:52.403855Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# For simplicity, one \"sentence\" per line \n",
    "corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2]\n",
    "\n",
    "# Tokenize using Keras\n",
    "tokenizer = Tokenizer(filters=\"\"\"!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\"\"\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "# Convert tokenized sentences to sequence format\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "\n",
    "print(corpus[3])\n",
    "print(sequences[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generating Input and Output Labels\n",
    "Now we need to generate our `X_train` and `y_train` so we can train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T22:58:53.715890Z",
     "start_time": "2019-05-31T22:58:53.703684Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Setting parameters for our model:\n",
    "\n",
    "# Vocab size\n",
    "V = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Dimension to reduce to (length of word embedding vectors)\n",
    "dim = 100\n",
    "window_size = 2\n",
    "\n",
    "print(\"vocabulary size: \", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T22:58:54.362689Z",
     "start_time": "2019-05-31T22:58:54.351577Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Function to generate the inputs and outputs for all windows\n",
    "def generate_data(sequences, window_size, V):\n",
    "    maxlen = window_size*2\n",
    "    # For each line (sentence)\n",
    "    for words in sequences:\n",
    "        L = len(words)\n",
    "        # Choose the target word\n",
    "        for index, word in enumerate(words):\n",
    "            # Create the window\n",
    "            s = index-window_size\n",
    "            e = index+window_size+1\n",
    "                    \n",
    "            in_words = []\n",
    "            labels = []\n",
    "            # Create the input/outputs for skipgrams\n",
    "            for i in range(s, e):\n",
    "                if i != index and 0 <= i < L:\n",
    "                    in_words.append([word] )\n",
    "                    labels.append(words[i])\n",
    "\n",
    "            x = np.array(in_words,dtype=np.int32)\n",
    "            y = np_utils.to_categorical(labels, V)\n",
    "            yield (x, y)\n",
    "            \n",
    "# We'll call this later on, from within our train_skipgram_model function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating the Model Architecture\n",
    "- Lastly, we create the (shallow) network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T22:58:55.659078Z",
     "start_time": "2019-05-31T22:58:55.473742Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create the Keras model and view it \n",
    "skipgram = Sequential()\n",
    "skipgram.add(Embedding(input_dim=V, input_length=1, embeddings_initializer=\"glorot_uniform\", output_dim=dim))\n",
    "skipgram.add(Reshape((dim, )))\n",
    "skipgram.add(Dense(input_dim=dim, units=V, activation='softmax'))\n",
    "\n",
    "skipgram.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Compiling and Training the Model\n",
    "- Time to compile and train\n",
    "- We use crossentropy, common loss for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T22:58:57.098670Z",
     "start_time": "2019-05-31T22:58:57.088302Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def train_skipgram_model(skipgram, sequences, window_size, dimension_size=V):\n",
    "    \"\"\"\n",
    "    skipgram: Keras model to train\n",
    "    sequences: list of lists of integers. \n",
    "               sequences[i][j] is the encoding of word j in document i\n",
    "    window_size: number of words in the window\n",
    "    dimension_size: integer. Size of the vector space for the word vectors.\n",
    "    \n",
    "    Note: this is slow to train. Took an 1hr 40min (no GPU) on 2016 Macbook Pro.\n",
    "    \"\"\"\n",
    "    # Note this cell took 1hr 40min on my machine (no GPU)\n",
    "    # Compile the Keras Model\n",
    "    skipgram.compile(loss='categorical_crossentropy', optimizer=\"adadelta\")\n",
    "\n",
    "    # Fit the Skipgrams\n",
    "    for iteration in range(10):\n",
    "        loss = 0.\n",
    "        for sequence, label in generate_data(sequences, window_size, dimension_size):\n",
    "            loss += skipgram.train_on_batch(sequence, label)\n",
    "\n",
    "        print(iteration, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Saving the Word Vectors\n",
    "- Let's save the vectors to a file\n",
    "- So we can load them into word2vec and test them out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T22:58:58.378949Z",
     "start_time": "2019-05-31T22:58:58.370228Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def write_model_weights(skipgram, tokenizer, filename='vectors.txt'):\n",
    "    # Write the resulting vectors to a text file\n",
    "    with open(filename ,'w') as f:\n",
    "        f.write(f\"{V-1} {dim}\\n\")\n",
    "        vectors = skipgram.get_weights()[0]\n",
    "        for word, i in tokenizer.word_index.items():\n",
    "            line = f\"{word} \" + \" \".join([str(num) for num in vectors[i,:]]) + \"\\n\"\n",
    "            f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T22:58:59.043001Z",
     "start_time": "2019-05-31T22:58:59.037421Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('vectors.txt'):\n",
    "    train_skipgram_model(skipgram, sequences, window_size, V)\n",
    "    write_model_weights(skipgram, token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Examining the Vectors\n",
    "- Let's load the vectors in to query them with `gensim`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T22:59:00.298533Z",
     "start_time": "2019-05-31T22:58:59.838955Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load the vectors into word2vec and see how we did!\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('./vectors.txt', binary=False)\n",
    "w2v.most_similar(positive=['white', 'rabbit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RNN Example: Text Classification\n",
    "- Let's try a Recurrent Neural Net for the same Reuters classification task from the Intro to Keras notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T22:59:02.705232Z",
     "start_time": "2019-05-31T22:59:01.743280Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Same data loading as before\n",
    "max_features = 2000\n",
    "(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=max_features)\n",
    "maxlen = 10\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "nb_epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T22:59:03.639023Z",
     "start_time": "2019-05-31T22:59:03.244177Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "rnn_model = Sequential()\n",
    "# The Embedding layer allows us to map words into dense vectors as inputs, common first layer\n",
    "rnn_model.add(Embedding(input_dim=max_features, output_dim=100, embeddings_initializer='glorot_uniform', input_length=maxlen))\n",
    "# This is the most basic kind of RNN!  We're using 20 units, \n",
    "#which somewhat reflects our \"memory\" of past events in a sequence\n",
    "# For the purposes of keras, it's just another type of \"unit\" you can try!\n",
    "rnn_model.add(SimpleRNN(20, return_sequences=False))\n",
    "rnn_model.add(Dense(46))\n",
    "rnn_model.add(Activation('softmax'))\n",
    "\n",
    "rnn_model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adadelta',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T22:59:50.369772Z",
     "start_time": "2019-05-31T22:59:04.300031Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "rnn_model.fit(X_train, y_train, batch_size=256, epochs=nb_epoch, \n",
    "              validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**WHOA!** Over 100% improvement on ANN!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LSTM Example: Sentiment Analysis\n",
    "\n",
    "Here is some code to train sentiment analysis on IMDB reviews.\n",
    "\n",
    "*(Note that this network takes about 20 minutes to train over 15 epochs.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T23:02:23.986454Z",
     "start_time": "2019-05-31T22:59:50.373702Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "max_features = 20000\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "# Load data (Keras utility)\n",
    "print('Loading data...')\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "# Pad Short sentences\n",
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "# Build our model!\n",
    "print('Build model...')\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(max_features, 128, dropout=0.2))\n",
    "# Here's the LSTM magic!\n",
    "lstm_model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))  # try using a GRU instead, for fun\n",
    "lstm_model.add(Dense(1))\n",
    "# Sigmoid for binary classification\n",
    "lstm_model.add(Activation('sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "lstm_model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "lstm_model.fit(X_train, y_train, batch_size=batch_size, epochs=15,\n",
    "               validation_data=(X_test, y_test))\n",
    "score, acc = lstm_model.evaluate(X_test, y_test,\n",
    "                                 batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CNN Example: Sentiment Analysis\n",
    "- Here's the same Sentiment Analysis task with a CNN + LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T23:03:58.763082Z",
     "start_time": "2019-05-31T23:02:27.126521Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Embedding\n",
    "max_features = 20000\n",
    "maxlen = 100\n",
    "embedding_size = 128\n",
    "\n",
    "# Convolution\n",
    "filter_length = 5\n",
    "nb_filter = 64\n",
    "pool_length = 4\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "# Training\n",
    "batch_size = 30\n",
    "nb_epoch = 2\n",
    "\n",
    "'''\n",
    "Note:\n",
    "batch_size is highly sensitive.\n",
    "Only 2 epochs are needed as the dataset is very small.\n",
    "'''\n",
    "\n",
    "# Load data\n",
    "print('Loading data...')\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "# Pad sentences\n",
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "cnn_model.add(Dropout(0.25))\n",
    "\n",
    "# Convolution!\n",
    "cnn_model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                            filter_length=filter_length,\n",
    "                            border_mode='valid',\n",
    "                            activation='relu',\n",
    "                            subsample_length=1))\n",
    "cnn_model.add(MaxPooling1D(pool_length=pool_length))\n",
    "\n",
    "# LSTM!\n",
    "cnn_model.add(LSTM(lstm_output_size))\n",
    "cnn_model.add(Dense(1))\n",
    "cnn_model.add(Activation('sigmoid'))\n",
    "\n",
    "cnn_model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "cnn_model.fit(X_train, y_train, batch_size=batch_size, epochs=nb_epoch,\n",
    "          validation_data=(X_test, y_test))\n",
    "score, acc = cnn_model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# BRNN Example: Sentiment Analysis\n",
    "- One more time on the sentiment, now with a Bidirectional Recurrent Neural Net!\n",
    "\n",
    "*(This takes about 2 minutes per epoch to train)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T23:04:24.831955Z",
     "start_time": "2019-05-31T23:04:02.132821Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "maxlen = 100  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "brnn_model = Sequential()\n",
    "brnn_model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "\n",
    "# Bidirectional LSTM!!!\n",
    "brnn_model.add(Bidirectional(LSTM(64)))\n",
    "brnn_model.add(Dropout(0.5))\n",
    "brnn_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "brnn_model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "brnn_model.fit(X_train, y_train,\n",
    "               batch_size=batch_size,\n",
    "               epochs=4,\n",
    "               validation_data=[X_test, y_test])\n",
    "\n",
    "score, acc = brnn_model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T23:04:24.836104Z",
     "start_time": "2019-05-31T23:04:03.579Z"
    }
   },
   "outputs": [],
   "source": [
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T23:04:24.838249Z",
     "start_time": "2019-05-31T23:04:04.466Z"
    }
   },
   "outputs": [],
   "source": [
    "brnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Example: Text Generation\n",
    "- Finally, a different example!\n",
    "- We're going to do **Text Generation** with **LSTM**\n",
    "- We'll watch our model start spitting out words of Nils's favorite philosopher in real time!\n",
    "  - ps, it's Friedrich Nietzsche\n",
    "- As we go through epoch's, the ability to generate Nietzcheian sentences will get better and better!\n",
    "- Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T23:02:24.004683Z",
     "start_time": "2019-05-31T22:59:23.560Z"
    }
   },
   "outputs": [],
   "source": [
    "path = get_file('nietzsche.txt', origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
    "text = open(path).read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "\n",
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "textgen_model = Sequential()\n",
    "textgen_model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "textgen_model.add(Dense(len(chars)))\n",
    "textgen_model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "textgen_model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "# train the model, output generated text after each iteration\n",
    "for iteration in range(1, 60):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    textgen_model.fit(X, y, batch_size=128, epochs=1)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print()\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = textgen_model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('models/skipgram.pkl', 'wb') as pkl:\n",
    "    pickle.dump(skipgram, pkl)\n",
    "with open('models/rnn_model.pkl', 'wb') as pkl:\n",
    "    pickle.dump(rnn_model, pkl)\n",
    "with open('models/lstm_model.pkl', 'wb') as pkl:\n",
    "    pickle.dump(lstm_model, pkl)\n",
    "with open('models/cnn_model.pkl', 'wb') as pkl:\n",
    "    pickle.dump(cnn_model, pkl)\n",
    "with open('models/brnn_model.pkl', 'wb') as pkl:\n",
    "    pickle.dump(brnn_model, pkl)\n",
    "with open('models/textgen_model.pkl', 'wb') as pkl:\n",
    "    pickle.dump(brnn_model, pkl)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "metis",
   "language": "python",
   "name": "metis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "livereveal": {
   "height": "100%",
   "margin": 0,
   "maxScale": 1,
   "minScale": 1,
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "sky",
   "transition": "zoom",
   "width": "100%"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "369px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "457px",
    "left": "0px",
    "right": "968px",
    "top": "130px",
    "width": "265.141px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
