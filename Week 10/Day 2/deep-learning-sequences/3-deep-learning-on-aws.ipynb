{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Onward to AWS!\n",
    "- Now that you saw how long those CPUs can take...\n",
    "- Let's go to GPUs!\n",
    "- That means AWS...\n",
    "\n",
    "## Setting up a Deep Learning AWS GPU Instance\n",
    "- You'll typically want to use a **Deep Learning AMI** that comes with all the relevant packages pre-installed and nice default configurations.\n",
    "- Now follow along with me as we venture Beyond the Wall!\n",
    "- **Caution**: This will cost $.  If you don't have your AWS credits, be wary.\n",
    "\n",
    "## Launching the Instance\n",
    "1. Proceed to the AWS EC2 Console\n",
    "2. Click \"Launch Instance\"\n",
    "3. Click Community AMIs\n",
    "4. Search for / find the Deep Learning AMI\n",
    "5. Select it\n",
    "6. Select a GPU instance (e.g. g2.2xlarge) and click \"Next: Configure Instance Details\" - the rest is the usual AWS steps!\n",
    "\n",
    "## Configuration\n",
    "1. Log onto your server via:\n",
    "  - `ssh -i <your_keypair.pem> icarus@<your_public_ip>`\n",
    "  - Password: `changetheworld`\n",
    "2. Log out\n",
    "3. In EC2 Console, reboot your instance (for init scripts to run)\n",
    "4. Log back in again (#1 above) and let's go!\n",
    "\n",
    "## Start a Jupyter Notebook (on your server)\n",
    "- You know how to do this!:\n",
    "  - `jupyter notebook`\n",
    "- If you want it to be open to the internet, you'll need to follow these instructions:\n",
    "  - [Running Jupyter Notebook Server in AWS](http://jupyter-notebook.readthedocs.io/en/latest/public_server.html)\n",
    "- In your local browser, browse to:\n",
    "  - `https://<your_public_ip>:8888`\n",
    "  - Note, you'll probably have to `pip install gensim` on the server\n",
    "- And let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS LSTM + CNN Example: Sentiment 1 more time!\n",
    "- Let's marvel at how blazing fast the same example runs on your GPU instance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from keras.datasets import imdb, reuters\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (Embedding, Convolution1D, Dropout, MaxPooling1D, \n",
    "                          LSTM, Dense, Activation)\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "\n",
    "# Embedding\n",
    "max_features = 20000\n",
    "maxlen = 100\n",
    "embedding_size = 128\n",
    "\n",
    "# Convolution\n",
    "filter_length = 5\n",
    "nb_filter = 64\n",
    "pool_length = 4\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "# Training\n",
    "batch_size = 30\n",
    "nb_epoch = 2\n",
    "\n",
    "'''\n",
    "Note:\n",
    "batch_size is highly sensitive.\n",
    "Only 2 epochs are needed as the dataset is very small.\n",
    "'''\n",
    "\n",
    "print('Loading data...')\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "aws_cnn_model = Sequential()\n",
    "aws_cnn_model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "aws_cnn_model.add(Dropout(0.25))\n",
    "aws_cnn_model.add(Convolution1D(filters=nb_filter,\n",
    "                                kernel_size=filter_length,\n",
    "                                padding='valid',\n",
    "                                activation='relu',\n",
    "                                strides=1))\n",
    "aws_cnn_model.add(MaxPooling1D(pool_size=pool_length))\n",
    "aws_cnn_model.add(LSTM(lstm_output_size))\n",
    "aws_cnn_model.add(Dense(1))\n",
    "aws_cnn_model.add(Activation('sigmoid'))\n",
    "\n",
    "aws_cnn_model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "filname = \"models/aws_cnn_model.pkl\"\n",
    "aws_cnn_model.fit(X_train, y_train, batch_size=batch_size, epochs=nb_epoch,\n",
    "                  validation_data=(X_test, y_test))\n",
    "score, acc = aws_cnn_model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metis",
   "language": "python",
   "name": "metis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
