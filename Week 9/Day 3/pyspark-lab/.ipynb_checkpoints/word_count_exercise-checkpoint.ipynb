{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with Spark API: Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create pyspark and get it ready to do things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = ['This is a document',\n",
    "             'This is another document',\n",
    "             'This is yet a third document',\n",
    "             'When will this list of document end',\n",
    "             'This is the last document']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df = spark.createDataFrame([(d,) for d in documents], ['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|word                               |\n",
      "+-----------------------------------+\n",
      "|This is a document                 |\n",
      "|This is another document           |\n",
      "|This is yet a third document       |\n",
      "|When will this list of document end|\n",
      "|This is the last document          |\n",
      "+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a few useful functions ready to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+\n",
      "|word                                       |\n",
      "+-------------------------------------------+\n",
      "|[this, is, a, document]                    |\n",
      "|[this, is, another, document]              |\n",
      "|[this, is, yet, a, third, document]        |\n",
      "|[when, will, this, list, of, document, end]|\n",
      "|[this, is, the, last, document]            |\n",
      "+-------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode, col, lower, sort_array\n",
    "\n",
    "doc_df.withColumn('word', split(lower(col('word')), \"\\s\")).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|word    |\n",
      "+--------+\n",
      "|this    |\n",
      "|is      |\n",
      "|a       |\n",
      "|document|\n",
      "|this    |\n",
      "|is      |\n",
      "|another |\n",
      "|document|\n",
      "|this    |\n",
      "|is      |\n",
      "+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_df.withColumn('word', explode(split(lower(col('word')), \"\\s\"))).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|    word|count|\n",
      "+--------+-----+\n",
      "|document|    5|\n",
      "|    this|    5|\n",
      "|      is|    4|\n",
      "|       a|    2|\n",
      "|    when|    1|\n",
      "|     end|    1|\n",
      "|    will|    1|\n",
      "|      of|    1|\n",
      "|     the|    1|\n",
      "|   third|    1|\n",
      "|    list|    1|\n",
      "|     yet|    1|\n",
      "| another|    1|\n",
      "|    last|    1|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_df.withColumn('word', explode(split(lower(col('word')), \"\\s\")))\\\n",
    "      .where('word != \"\"')\\\n",
    "      .groupBy('word')\\\n",
    "      .count()\\\n",
    "      .orderBy('count', ascending=False)\\\n",
    "      .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words with friends - finding anagrams\n",
    "\n",
    "In the file \"data/words.txt\", there is a list of words. Our goal is to group together words that are anagrams of each other (e.g. ACT and CAT).\n",
    "\n",
    "This will show us how to load from a file, and a cool \"canonical representation\" trick.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|   value|\n",
      "+--------+\n",
      "|      AA|\n",
      "|     AAH|\n",
      "|   AAHED|\n",
      "|  AAHING|\n",
      "|    AAHS|\n",
      "|     AAL|\n",
      "|   AALII|\n",
      "|  AALIIS|\n",
      "|    AALS|\n",
      "|AARDVARK|\n",
      "+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_df = spark.read.text('data/words.txt')\n",
    "word_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step, let's take every word and split it out into a list of characters and store that as a new column. So we want to go from:\n",
    "\n",
    "```\n",
    "| value |\n",
    "---------\n",
    "| AA    |\n",
    "| AAH   |\n",
    "| ...   |\n",
    "```\n",
    "\n",
    "Will become:\n",
    "\n",
    "```\n",
    "| value |     key     |\n",
    "-----------------------\n",
    "| AA    | [, A, A]    |\n",
    "| AAH   | [, A, A, H] |\n",
    "| ...   | ...         |\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|   value|                 key|\n",
      "+--------+--------------------+\n",
      "|      AA|            [, A, A]|\n",
      "|     AAH|         [, A, A, H]|\n",
      "|   AAHED|   [, A, A, D, E, H]|\n",
      "|  AAHING|[, A, A, G, H, I, N]|\n",
      "|    AAHS|      [, A, A, H, S]|\n",
      "|     AAL|         [, A, A, L]|\n",
      "|   AALII|   [, A, A, I, I, L]|\n",
      "|  AALIIS|[, A, A, I, I, L, S]|\n",
      "|    AALS|      [, A, A, L, S]|\n",
      "|AARDVARK|[, A, A, A, D, K,...|\n",
      "|AARDWOLF|[, A, A, D, F, L,...|\n",
      "|   AARGH|   [, A, A, G, H, R]|\n",
      "|  AARRGH|[, A, A, G, H, R, R]|\n",
      "| AARRGHH|[, A, A, G, H, H,...|\n",
      "|     AAS|         [, A, A, S]|\n",
      "|AASVOGEL|[, A, A, E, G, L,...|\n",
      "|      AB|            [, A, B]|\n",
      "|     ABA|         [, A, A, B]|\n",
      "|   ABACA|   [, A, A, A, B, C]|\n",
      "|  ABACAS|[, A, A, A, B, C, S]|\n",
      "+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_df_key = word_df.withColumn('key', sort_array(split(col('value'), \"\")))\n",
    "word_df_key.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take that new list of characters you created and treat that as a key and group on that and see how many times those keys occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                 key|count|\n",
      "+--------------------+-----+\n",
      "|   [, A, E, P, R, S]|   11|\n",
      "|[, A, E, L, R, S, T]|   11|\n",
      "|   [, A, E, L, S, T]|   10|\n",
      "+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count  of the keys\n",
    "word_df_key.groupBy('key').count().orderBy('count', ascending=False).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to actually see all the anagrams? Hint: Check out the `collect_list` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------------------------------------------------------------------------------------+----+\n",
      "|key                    |words                                                                                   |freq|\n",
      "+-----------------------+----------------------------------------------------------------------------------------+----+\n",
      "|[, A, E, L, R, S, T]   |[ALERTS, ALTERS, ARTELS, ESTRAL, LASTER, RATELS, SALTER, SLATER, STALER, STELAR, TALERS]|11  |\n",
      "|[, A, E, P, R, S]      |[APERS, APRES, ASPER, PARES, PARSE, PEARS, PRASE, PRESA, REAPS, SPARE, SPEAR]           |11  |\n",
      "|[, A, E, L, S, T]      |[LEAST, SETAL, SLATE, STALE, STEAL, STELA, TAELS, TALES, TEALS, TESLA]                  |10  |\n",
      "|[, A, C, E, P, R, S]   |[CAPERS, CRAPES, ESCARP, PACERS, PARSEC, RECAPS, SCRAPE, SECPAR, SPACER]                |9   |\n",
      "|[, A, E, I, N, R, S, T]|[ANESTRI, ANTSIER, NASTIER, RATINES, RETAINS, RETINAS, RETSINA, STAINER, STEARIN]       |9   |\n",
      "|[, A, E, L, P, S, T]   |[PALEST, PALETS, PASTEL, PETALS, PLATES, PLEATS, SEPTAL, STAPLE, TEPALS]                |9   |\n",
      "|[, E, I, N, R, S, T]   |[ESTRIN, INERTS, INSERT, INTERS, NITERS, NITRES, SINTER, TRIENS, TRINES]                |9   |\n",
      "|[, A, C, E, R, S, T]   |[CARETS, CARTES, CASTER, CATERS, CRATES, REACTS, RECAST, TRACES]                        |8   |\n",
      "|[, A, E, P, R, S, S]   |[ASPERS, PARSES, PASSER, PRASES, REPASS, SPARES, SPARSE, SPEARS]                        |8   |\n",
      "|[, A, E, G, I, N, R, S]|[EARINGS, ERASING, GAINERS, REAGINS, REGAINS, REGINAS, SEARING, SERINGA]                |8   |\n",
      "|[, A, E, L, P, S]      |[LAPSE, LEAPS, PALES, PEALS, PLEAS, SALEP, SEPAL, SPALE]                                |8   |\n",
      "|[, A, E, S, T]         |[ATES, EAST, EATS, ETAS, SATE, SEAT, SETA, TEAS]                                        |8   |\n",
      "|[, E, E, N, R, S, T]   |[ENTERS, NESTER, RENEST, RENTES, RESENT, TENSER, TERNES, TREENS]                        |8   |\n",
      "|[, A, E, L, R, S]      |[ARLES, EARLS, LARES, LASER, LEARS, RALES, REALS, SERAL]                                |8   |\n",
      "|[, E, I, P, R, S]      |[PERIS, PIERS, PRIES, PRISE, RIPES, SPEIR, SPIER, SPIRE]                                |8   |\n",
      "+-----------------------+----------------------------------------------------------------------------------------+----+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If we want the actual anagrams?\n",
    "from pyspark.sql.functions import collect_list, struct, count\n",
    "\n",
    "# Bracketing on the outside enables multi-line splitting of one long code\n",
    "(word_df_key.groupBy('key')\n",
    "            .agg(collect_list('value').alias('words'), count('key').alias('freq'))\n",
    "            .orderBy('freq', ascending=False)\n",
    "            .show(15, truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
