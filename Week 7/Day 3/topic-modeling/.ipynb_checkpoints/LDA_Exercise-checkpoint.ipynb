{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling with gensim\n",
    "We'll try out [Latent Dirichlet Allocation (LDA)](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) in [gensim](http://radimrehurek.com/gensim/index.html) on the [20 Newsgroups dataset](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html) with some simple preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install gensim -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "\n",
    "# sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# logging for gensim (set to INFO)\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retain only a subset of the 20 categories in the original 20 Newsgroups Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set categories\n",
    "categories = ['alt.atheism', 'comp.graphics', 'rec.sport.baseball', \n",
    "              'rec.motorcycles', 'sci.space', 'talk.politics.mideast']\n",
    "\n",
    "# Download the training subset of the 20 NG dataset, with headers, footers, quotes removed\n",
    "# Only keep docs from the 6 categories above\n",
    "ng_train = datasets.fetch_20newsgroups(subset='train', categories=categories, \n",
    "                                      remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Well, the Red Sox have apparenly resigned Herm Winningham to a AAA contract.\\nTed \"Larry\" Simmons signed him to a AAA contract then released him from\\nBuffalo, allowing Lou \"Curly\" Gorman to circumvent the rule about not\\nresigning free agents until May 1. Clearly, neither of these guys is bright\\nenough to be Moe.\\n\\n Mike Jones | AIX High-End Development | mjones@donald.aix.kingston.ibm.com'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the first doc\n",
    "ng_train.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Preprocessing\n",
    "We'll need to generate a term-document matrix of word (token) counts for use in LDA.\n",
    "\n",
    "We'll use `sklearn`'s `CountVectorizer` to generate our term-document matrix of counts. We'll make use of a few parameters to accomplish the following preprocessing of the text documents all within the `CountVectorizer`:\n",
    "* `analyzer=word`: Tokenize by word\n",
    "* `ngram_range=(1,2)`: Keep all 1 and 2-word grams\n",
    "* `stop_words=english`: Remove all English stop words\n",
    "* `token_pattern=\\\\b[a-z][a-z]+\\\\b`: Match all tokens with 2 or more (strictly) alphabet characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
       "                strip_accents=None, token_pattern='\\\\b[a-z][a-z]+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a CountVectorizer for parsing/counting words\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 2),  \n",
    "                                   stop_words='english', token_pattern=\"\\\\b[a-z][a-z]+\\\\b\")\n",
    "\n",
    "count_vectorizer.fit(ng_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the term-document matrix\n",
    "# Transpose it so the terms are the rows\n",
    "doc_word = count_vectorizer.transform(ng_train.data).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3406</th>\n",
       "      <th>3407</th>\n",
       "      <th>3408</th>\n",
       "      <th>3409</th>\n",
       "      <th>3410</th>\n",
       "      <th>3411</th>\n",
       "      <th>3412</th>\n",
       "      <th>3413</th>\n",
       "      <th>3414</th>\n",
       "      <th>3415</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>aa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>aa aaa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>aa albany</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>aa atlanta</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>aa does</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3416 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0     1     2     3     4     5     6     7     8     9     ...  \\\n",
       "aa             0     0     0     0     0     0     0     0     0     0  ...   \n",
       "aa aaa         0     0     0     0     0     0     0     0     0     0  ...   \n",
       "aa albany      0     0     0     0     0     0     0     0     0     0  ...   \n",
       "aa atlanta     0     0     0     0     0     0     0     0     0     0  ...   \n",
       "aa does        0     0     0     0     0     0     0     0     0     0  ...   \n",
       "\n",
       "            3406  3407  3408  3409  3410  3411  3412  3413  3414  3415  \n",
       "aa             0     0     0     0     0     0     0     0     0     0  \n",
       "aa aaa         0     0     0     0     0     0     0     0     0     0  \n",
       "aa albany      0     0     0     0     0     0     0     0     0     0  \n",
       "aa atlanta     0     0     0     0     0     0     0     0     0     0  \n",
       "aa does        0     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 3416 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(doc_word.toarray(), count_vectorizer.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(272502, 3416)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_word.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert to gensim\n",
    "We need to convert our sparse `scipy` matrix to a `gensim`-friendly object called a Corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "corpus = matutils.Sparse2Corpus(doc_word)  # converting data into another format that gensim can understand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Map matrix rows to words (tokens)\n",
    "We need to save a mapping (dict) of row id to word (token) for later use by gensim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = dict((v, k) for k, v in count_vectorizer.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "272502"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA\n",
    "At this point we can simply plow ahead in creating an LDA model.  It requires our corpus of word counts, mapping of row ids to words, and the number of topics (3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-24 17:22:42,421 : INFO : using symmetric alpha at 0.3333333333333333\n",
      "2019-11-24 17:22:42,431 : INFO : using symmetric eta at 0.3333333333333333\n",
      "2019-11-24 17:22:42,505 : INFO : using serial LDA version on this node\n",
      "2019-11-24 17:22:42,661 : INFO : running online (multi-pass) LDA training, 3 topics, 5 passes over the supplied corpus of 3416 documents, updating model once every 2000 documents, evaluating perplexity every 3416 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2019-11-24 17:22:42,662 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2019-11-24 17:22:42,807 : INFO : PROGRESS: pass 0, at document #2000/3416\n",
      "2019-11-24 17:22:44,976 : INFO : merging changes from 2000 documents into a model of 3416 documents\n",
      "2019-11-24 17:22:45,093 : INFO : topic #0 (0.333): 0.002*\"people\" + 0.001*\"just\" + 0.001*\"know\" + 0.001*\"time\" + 0.001*\"like\" + 0.001*\"space\" + 0.001*\"don\" + 0.001*\"edu\" + 0.001*\"think\" + 0.001*\"does\"\n",
      "2019-11-24 17:22:45,102 : INFO : topic #1 (0.333): 0.002*\"like\" + 0.001*\"space\" + 0.001*\"don\" + 0.001*\"think\" + 0.001*\"people\" + 0.001*\"know\" + 0.001*\"just\" + 0.001*\"time\" + 0.001*\"jews\" + 0.001*\"good\"\n",
      "2019-11-24 17:22:45,113 : INFO : topic #2 (0.333): 0.001*\"just\" + 0.001*\"people\" + 0.001*\"don\" + 0.001*\"know\" + 0.001*\"like\" + 0.001*\"space\" + 0.001*\"time\" + 0.001*\"ve\" + 0.001*\"did\" + 0.001*\"good\"\n",
      "2019-11-24 17:22:45,122 : INFO : topic diff=1.436028, rho=1.000000\n",
      "2019-11-24 17:22:48,847 : INFO : -13.130 per-word bound, 8961.8 perplexity estimate based on a held-out corpus of 1416 documents with 254128 words\n",
      "2019-11-24 17:22:48,848 : INFO : PROGRESS: pass 0, at document #3416/3416\n",
      "2019-11-24 17:22:50,343 : INFO : merging changes from 1416 documents into a model of 3416 documents\n",
      "2019-11-24 17:22:50,438 : INFO : topic #0 (0.333): 0.002*\"people\" + 0.002*\"just\" + 0.002*\"don\" + 0.002*\"like\" + 0.002*\"know\" + 0.002*\"does\" + 0.001*\"think\" + 0.001*\"time\" + 0.001*\"space\" + 0.001*\"god\"\n",
      "2019-11-24 17:22:50,445 : INFO : topic #1 (0.333): 0.002*\"people\" + 0.002*\"like\" + 0.002*\"space\" + 0.002*\"don\" + 0.001*\"armenian\" + 0.001*\"think\" + 0.001*\"said\" + 0.001*\"know\" + 0.001*\"armenians\" + 0.001*\"just\"\n",
      "2019-11-24 17:22:50,454 : INFO : topic #2 (0.333): 0.002*\"don\" + 0.002*\"just\" + 0.002*\"people\" + 0.001*\"jpeg\" + 0.001*\"like\" + 0.001*\"image\" + 0.001*\"know\" + 0.001*\"space\" + 0.001*\"ve\" + 0.001*\"time\"\n",
      "2019-11-24 17:22:50,460 : INFO : topic diff=1.254431, rho=0.707107\n",
      "2019-11-24 17:22:50,587 : INFO : PROGRESS: pass 1, at document #2000/3416\n",
      "2019-11-24 17:22:52,610 : INFO : merging changes from 2000 documents into a model of 3416 documents\n",
      "2019-11-24 17:22:52,701 : INFO : topic #0 (0.333): 0.002*\"people\" + 0.002*\"just\" + 0.002*\"like\" + 0.002*\"don\" + 0.002*\"space\" + 0.002*\"know\" + 0.001*\"think\" + 0.001*\"time\" + 0.001*\"does\" + 0.001*\"edu\"\n",
      "2019-11-24 17:22:52,710 : INFO : topic #1 (0.333): 0.002*\"people\" + 0.002*\"like\" + 0.002*\"don\" + 0.002*\"armenian\" + 0.001*\"turkish\" + 0.001*\"space\" + 0.001*\"said\" + 0.001*\"armenians\" + 0.001*\"know\" + 0.001*\"think\"\n",
      "2019-11-24 17:22:52,721 : INFO : topic #2 (0.333): 0.001*\"just\" + 0.001*\"don\" + 0.001*\"people\" + 0.001*\"like\" + 0.001*\"know\" + 0.001*\"year\" + 0.001*\"space\" + 0.001*\"image\" + 0.001*\"time\" + 0.001*\"good\"\n",
      "2019-11-24 17:22:52,727 : INFO : topic diff=0.618606, rho=0.519314\n",
      "2019-11-24 17:22:56,369 : INFO : -12.048 per-word bound, 4234.8 perplexity estimate based on a held-out corpus of 1416 documents with 254128 words\n",
      "2019-11-24 17:22:56,370 : INFO : PROGRESS: pass 1, at document #3416/3416\n",
      "2019-11-24 17:22:57,417 : INFO : merging changes from 1416 documents into a model of 3416 documents\n",
      "2019-11-24 17:22:57,509 : INFO : topic #0 (0.333): 0.002*\"people\" + 0.002*\"just\" + 0.002*\"don\" + 0.002*\"like\" + 0.001*\"know\" + 0.001*\"does\" + 0.001*\"space\" + 0.001*\"god\" + 0.001*\"think\" + 0.001*\"time\"\n",
      "2019-11-24 17:22:57,521 : INFO : topic #1 (0.333): 0.002*\"people\" + 0.002*\"armenian\" + 0.002*\"said\" + 0.002*\"like\" + 0.002*\"don\" + 0.002*\"armenians\" + 0.001*\"know\" + 0.001*\"turkish\" + 0.001*\"just\" + 0.001*\"think\"\n",
      "2019-11-24 17:22:57,532 : INFO : topic #2 (0.333): 0.002*\"image\" + 0.001*\"jpeg\" + 0.001*\"don\" + 0.001*\"like\" + 0.001*\"just\" + 0.001*\"space\" + 0.001*\"people\" + 0.001*\"use\" + 0.001*\"year\" + 0.001*\"time\"\n",
      "2019-11-24 17:22:57,540 : INFO : topic diff=0.584277, rho=0.519314\n",
      "2019-11-24 17:22:57,651 : INFO : PROGRESS: pass 2, at document #2000/3416\n",
      "2019-11-24 17:22:58,946 : INFO : merging changes from 2000 documents into a model of 3416 documents\n",
      "2019-11-24 17:22:59,031 : INFO : topic #0 (0.333): 0.002*\"people\" + 0.002*\"just\" + 0.002*\"like\" + 0.002*\"space\" + 0.002*\"don\" + 0.001*\"know\" + 0.001*\"think\" + 0.001*\"edu\" + 0.001*\"does\" + 0.001*\"time\"\n",
      "2019-11-24 17:22:59,039 : INFO : topic #1 (0.333): 0.002*\"people\" + 0.002*\"armenian\" + 0.002*\"said\" + 0.002*\"like\" + 0.002*\"don\" + 0.002*\"turkish\" + 0.002*\"armenians\" + 0.001*\"know\" + 0.001*\"just\" + 0.001*\"jews\"\n",
      "2019-11-24 17:22:59,047 : INFO : topic #2 (0.333): 0.001*\"just\" + 0.001*\"don\" + 0.001*\"image\" + 0.001*\"like\" + 0.001*\"space\" + 0.001*\"year\" + 0.001*\"people\" + 0.001*\"know\" + 0.001*\"jpeg\" + 0.001*\"time\"\n",
      "2019-11-24 17:22:59,054 : INFO : topic diff=0.384427, rho=0.460874\n",
      "2019-11-24 17:23:02,352 : INFO : -11.705 per-word bound, 3338.4 perplexity estimate based on a held-out corpus of 1416 documents with 254128 words\n",
      "2019-11-24 17:23:02,353 : INFO : PROGRESS: pass 2, at document #3416/3416\n",
      "2019-11-24 17:23:03,121 : INFO : merging changes from 1416 documents into a model of 3416 documents\n",
      "2019-11-24 17:23:03,199 : INFO : topic #0 (0.333): 0.002*\"people\" + 0.002*\"just\" + 0.002*\"don\" + 0.002*\"like\" + 0.001*\"space\" + 0.001*\"god\" + 0.001*\"know\" + 0.001*\"does\" + 0.001*\"think\" + 0.001*\"time\"\n",
      "2019-11-24 17:23:03,206 : INFO : topic #1 (0.333): 0.002*\"people\" + 0.002*\"armenian\" + 0.002*\"said\" + 0.002*\"armenians\" + 0.002*\"like\" + 0.002*\"don\" + 0.001*\"turkish\" + 0.001*\"know\" + 0.001*\"just\" + 0.001*\"think\"\n",
      "2019-11-24 17:23:03,214 : INFO : topic #2 (0.333): 0.002*\"image\" + 0.001*\"jpeg\" + 0.001*\"like\" + 0.001*\"don\" + 0.001*\"just\" + 0.001*\"space\" + 0.001*\"use\" + 0.001*\"year\" + 0.001*\"people\" + 0.001*\"time\"\n",
      "2019-11-24 17:23:03,222 : INFO : topic diff=0.351534, rho=0.460874\n",
      "2019-11-24 17:23:03,330 : INFO : PROGRESS: pass 3, at document #2000/3416\n",
      "2019-11-24 17:23:04,433 : INFO : merging changes from 2000 documents into a model of 3416 documents\n",
      "2019-11-24 17:23:04,512 : INFO : topic #0 (0.333): 0.002*\"just\" + 0.002*\"people\" + 0.002*\"space\" + 0.002*\"don\" + 0.002*\"like\" + 0.001*\"think\" + 0.001*\"know\" + 0.001*\"edu\" + 0.001*\"does\" + 0.001*\"time\"\n",
      "2019-11-24 17:23:04,521 : INFO : topic #1 (0.333): 0.002*\"people\" + 0.002*\"armenian\" + 0.002*\"said\" + 0.002*\"armenians\" + 0.002*\"turkish\" + 0.001*\"don\" + 0.001*\"like\" + 0.001*\"know\" + 0.001*\"just\" + 0.001*\"jews\"\n",
      "2019-11-24 17:23:04,528 : INFO : topic #2 (0.333): 0.001*\"image\" + 0.001*\"just\" + 0.001*\"don\" + 0.001*\"like\" + 0.001*\"space\" + 0.001*\"year\" + 0.001*\"jpeg\" + 0.001*\"people\" + 0.001*\"time\" + 0.001*\"good\"\n",
      "2019-11-24 17:23:04,534 : INFO : topic diff=0.276582, rho=0.418560\n",
      "2019-11-24 17:23:07,124 : INFO : -11.614 per-word bound, 3135.0 perplexity estimate based on a held-out corpus of 1416 documents with 254128 words\n",
      "2019-11-24 17:23:07,126 : INFO : PROGRESS: pass 3, at document #3416/3416\n",
      "2019-11-24 17:23:07,806 : INFO : merging changes from 1416 documents into a model of 3416 documents\n",
      "2019-11-24 17:23:07,885 : INFO : topic #0 (0.333): 0.002*\"people\" + 0.002*\"just\" + 0.002*\"don\" + 0.002*\"like\" + 0.001*\"space\" + 0.001*\"god\" + 0.001*\"know\" + 0.001*\"think\" + 0.001*\"does\" + 0.001*\"time\"\n",
      "2019-11-24 17:23:07,892 : INFO : topic #1 (0.333): 0.002*\"people\" + 0.002*\"armenian\" + 0.002*\"said\" + 0.002*\"armenians\" + 0.002*\"don\" + 0.001*\"like\" + 0.001*\"turkish\" + 0.001*\"know\" + 0.001*\"just\" + 0.001*\"think\"\n",
      "2019-11-24 17:23:07,901 : INFO : topic #2 (0.333): 0.002*\"image\" + 0.001*\"jpeg\" + 0.001*\"like\" + 0.001*\"don\" + 0.001*\"just\" + 0.001*\"space\" + 0.001*\"use\" + 0.001*\"time\" + 0.001*\"people\" + 0.001*\"year\"\n",
      "2019-11-24 17:23:07,907 : INFO : topic diff=0.267092, rho=0.418560\n",
      "2019-11-24 17:23:08,013 : INFO : PROGRESS: pass 4, at document #2000/3416\n",
      "2019-11-24 17:23:09,024 : INFO : merging changes from 2000 documents into a model of 3416 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-24 17:23:09,105 : INFO : topic #0 (0.333): 0.002*\"just\" + 0.002*\"people\" + 0.002*\"don\" + 0.002*\"like\" + 0.002*\"space\" + 0.001*\"think\" + 0.001*\"know\" + 0.001*\"edu\" + 0.001*\"does\" + 0.001*\"time\"\n",
      "2019-11-24 17:23:09,113 : INFO : topic #1 (0.333): 0.002*\"people\" + 0.002*\"armenian\" + 0.002*\"armenians\" + 0.002*\"said\" + 0.002*\"turkish\" + 0.001*\"don\" + 0.001*\"like\" + 0.001*\"know\" + 0.001*\"just\" + 0.001*\"jews\"\n",
      "2019-11-24 17:23:09,121 : INFO : topic #2 (0.333): 0.001*\"image\" + 0.001*\"just\" + 0.001*\"don\" + 0.001*\"like\" + 0.001*\"space\" + 0.001*\"year\" + 0.001*\"jpeg\" + 0.001*\"people\" + 0.001*\"time\" + 0.001*\"good\"\n",
      "2019-11-24 17:23:09,127 : INFO : topic diff=0.235596, rho=0.386103\n",
      "2019-11-24 17:23:11,696 : INFO : -11.577 per-word bound, 3054.3 perplexity estimate based on a held-out corpus of 1416 documents with 254128 words\n",
      "2019-11-24 17:23:11,698 : INFO : PROGRESS: pass 4, at document #3416/3416\n",
      "2019-11-24 17:23:12,368 : INFO : merging changes from 1416 documents into a model of 3416 documents\n",
      "2019-11-24 17:23:12,448 : INFO : topic #0 (0.333): 0.002*\"people\" + 0.002*\"just\" + 0.002*\"don\" + 0.002*\"like\" + 0.001*\"space\" + 0.001*\"god\" + 0.001*\"know\" + 0.001*\"think\" + 0.001*\"does\" + 0.001*\"time\"\n",
      "2019-11-24 17:23:12,456 : INFO : topic #1 (0.333): 0.002*\"people\" + 0.002*\"armenian\" + 0.002*\"said\" + 0.002*\"armenians\" + 0.001*\"don\" + 0.001*\"like\" + 0.001*\"turkish\" + 0.001*\"know\" + 0.001*\"just\" + 0.001*\"space\"\n",
      "2019-11-24 17:23:12,464 : INFO : topic #2 (0.333): 0.002*\"image\" + 0.001*\"jpeg\" + 0.001*\"like\" + 0.001*\"don\" + 0.001*\"just\" + 0.001*\"space\" + 0.001*\"use\" + 0.001*\"time\" + 0.001*\"people\" + 0.001*\"year\"\n",
      "2019-11-24 17:23:12,471 : INFO : topic diff=0.227881, rho=0.386103\n"
     ]
    }
   ],
   "source": [
    "# Create lda model (equivalent to \"fit\" in sklearn)\n",
    "lda = models.LdaModel(corpus=corpus,  # we converted our data into a way gensim can understadn above\n",
    "                      num_topics=3,   # how many topics are there\n",
    "                      id2word=id2word, \n",
    "                      passes=5)   # telling gensim to run trough how many iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what happened.  Here are the 5 most important words for each of the 3 topics we found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-24 17:23:12,534 : INFO : topic #0 (0.333): 0.002*\"people\" + 0.002*\"just\" + 0.002*\"don\" + 0.002*\"like\" + 0.001*\"space\" + 0.001*\"god\" + 0.001*\"know\" + 0.001*\"think\" + 0.001*\"does\" + 0.001*\"time\"\n",
      "2019-11-24 17:23:12,541 : INFO : topic #1 (0.333): 0.002*\"people\" + 0.002*\"armenian\" + 0.002*\"said\" + 0.002*\"armenians\" + 0.001*\"don\" + 0.001*\"like\" + 0.001*\"turkish\" + 0.001*\"know\" + 0.001*\"just\" + 0.001*\"space\"\n",
      "2019-11-24 17:23:12,550 : INFO : topic #2 (0.333): 0.002*\"image\" + 0.001*\"jpeg\" + 0.001*\"like\" + 0.001*\"don\" + 0.001*\"just\" + 0.001*\"space\" + 0.001*\"use\" + 0.001*\"time\" + 0.001*\"people\" + 0.001*\"year\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.002*\"people\" + 0.002*\"just\" + 0.002*\"don\" + 0.002*\"like\" + 0.001*\"space\" + 0.001*\"god\" + 0.001*\"know\" + 0.001*\"think\" + 0.001*\"does\" + 0.001*\"time\"'),\n",
       " (1,\n",
       "  '0.002*\"people\" + 0.002*\"armenian\" + 0.002*\"said\" + 0.002*\"armenians\" + 0.001*\"don\" + 0.001*\"like\" + 0.001*\"turkish\" + 0.001*\"know\" + 0.001*\"just\" + 0.001*\"space\"'),\n",
       " (2,\n",
       "  '0.002*\"image\" + 0.001*\"jpeg\" + 0.001*\"like\" + 0.001*\"don\" + 0.001*\"just\" + 0.001*\"space\" + 0.001*\"use\" + 0.001*\"time\" + 0.001*\"people\" + 0.001*\"year\"')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics()   # it will give you the topics\n",
    "                     # will return you the number of topics you specified above\n",
    "    \n",
    "    # HOWEVER, it is your job to infer what the topics are!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Space\n",
    "If we want to map our documents to the topic space we need to actually use the LdaModel transformer that we created above, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.interfaces.TransformedCorpus at 0x279f0bc8208>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the docs from the word space to the topic space (like \"transform\" in sklearn)\n",
    "lda_corpus = lda[corpus]\n",
    "lda_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the documents' topic vectors in a list so we can take a peak\n",
    "lda_docs = [doc for doc in lda_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 0.99065596)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take a look at the document vectors in the topic space, which are measures of the component of each document along each topic.  Thus, at most a document vector can have num_topics=3 nonzero components in the topic space, and most have far fewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(2, 0.99065596)],\n",
       " [(0, 0.98118085)],\n",
       " [(0, 0.011481263), (1, 0.97667), (2, 0.011848766)],\n",
       " [(2, 0.98503464)],\n",
       " [(0, 0.99290806)]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out the document vectors in the topic space for the first 5 documents\n",
    "lda_docs[0:5]  # individual document vectors in the topic space\n",
    "\n",
    "# For document 1 (first row), it is mostly made out of topic 2\n",
    "# in document 2, it is mostly made out of topic 1 and topic 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Well, the Red Sox have apparenly resigned Herm Winningham to a AAA contract.\\nTed \"Larry\" Simmons signed him to a AAA contract then released him from\\nBuffalo, allowing Lou \"Curly\" Gorman to circumvent the rule about not\\nresigning free agents until May 1. Clearly, neither of these guys is bright\\nenough to be Moe.\\n\\n Mike Jones | AIX High-End Development | mjones@donald.aix.kingston.ibm.com'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ng_train.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On your own...\n",
    "- Pick a few subsets of the 20newsgroups dataset  \n",
    "- Try performing LDA on this data with gensim\n",
    "- Play with some of the preprocessing options and parameters for LDA, observe what happens\n",
    "- See if you can use the resulting topic space to extract topic vectors\n",
    "- How do your results look?\n",
    "- Can you think of how you could cluster this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-24 17:23:14,321 : INFO : using symmetric alpha at 0.16666666666666666\n",
      "2019-11-24 17:23:14,323 : INFO : using symmetric eta at 0.16666666666666666\n",
      "2019-11-24 17:23:14,372 : INFO : using serial LDA version on this node\n",
      "2019-11-24 17:23:14,607 : INFO : running online (multi-pass) LDA training, 6 topics, 10 passes over the supplied corpus of 3416 documents, updating model once every 2000 documents, evaluating perplexity every 3416 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2019-11-24 17:23:14,672 : INFO : PROGRESS: pass 0, at document #2000/3416\n",
      "2019-11-24 17:23:16,905 : INFO : merging changes from 2000 documents into a model of 3416 documents\n",
      "2019-11-24 17:23:17,060 : INFO : topic #1 (0.167): 0.001*\"people\" + 0.001*\"just\" + 0.001*\"space\" + 0.001*\"like\" + 0.001*\"know\" + 0.001*\"time\" + 0.001*\"don\" + 0.001*\"edu\" + 0.001*\"think\" + 0.001*\"good\"\n",
      "2019-11-24 17:23:17,069 : INFO : topic #4 (0.167): 0.002*\"people\" + 0.001*\"like\" + 0.001*\"just\" + 0.001*\"don\" + 0.001*\"time\" + 0.001*\"does\" + 0.001*\"edu\" + 0.001*\"know\" + 0.001*\"good\" + 0.001*\"think\"\n",
      "2019-11-24 17:23:17,076 : INFO : topic #5 (0.167): 0.002*\"people\" + 0.001*\"don\" + 0.001*\"know\" + 0.001*\"like\" + 0.001*\"just\" + 0.001*\"time\" + 0.001*\"space\" + 0.001*\"good\" + 0.001*\"turkish\" + 0.001*\"does\"\n",
      "2019-11-24 17:23:17,086 : INFO : topic #0 (0.167): 0.001*\"think\" + 0.001*\"space\" + 0.001*\"people\" + 0.001*\"like\" + 0.001*\"know\" + 0.001*\"don\" + 0.001*\"edu\" + 0.001*\"just\" + 0.001*\"time\" + 0.001*\"good\"\n",
      "2019-11-24 17:23:17,096 : INFO : topic #2 (0.167): 0.002*\"just\" + 0.002*\"don\" + 0.002*\"know\" + 0.002*\"people\" + 0.001*\"think\" + 0.001*\"space\" + 0.001*\"time\" + 0.001*\"said\" + 0.001*\"armenian\" + 0.001*\"say\"\n",
      "2019-11-24 17:23:17,107 : INFO : topic diff=2.967916, rho=1.000000\n",
      "2019-11-24 17:23:20,469 : INFO : -14.656 per-word bound, 25810.4 perplexity estimate based on a held-out corpus of 1416 documents with 254128 words\n",
      "2019-11-24 17:23:20,470 : INFO : PROGRESS: pass 0, at document #3416/3416\n",
      "2019-11-24 17:23:21,901 : INFO : merging changes from 1416 documents into a model of 3416 documents\n",
      "2019-11-24 17:23:22,050 : INFO : topic #0 (0.167): 0.002*\"space\" + 0.002*\"think\" + 0.002*\"people\" + 0.001*\"like\" + 0.001*\"don\" + 0.001*\"just\" + 0.001*\"time\" + 0.001*\"new\" + 0.001*\"good\" + 0.001*\"does\"\n",
      "2019-11-24 17:23:22,057 : INFO : topic #4 (0.167): 0.002*\"god\" + 0.001*\"people\" + 0.001*\"like\" + 0.001*\"does\" + 0.001*\"don\" + 0.001*\"use\" + 0.001*\"new\" + 0.001*\"image\" + 0.001*\"just\" + 0.001*\"time\"\n",
      "2019-11-24 17:23:22,066 : INFO : topic #5 (0.167): 0.004*\"people\" + 0.002*\"don\" + 0.002*\"like\" + 0.002*\"know\" + 0.002*\"just\" + 0.002*\"armenian\" + 0.002*\"said\" + 0.002*\"armenians\" + 0.002*\"image\" + 0.002*\"time\"\n",
      "2019-11-24 17:23:22,074 : INFO : topic #2 (0.167): 0.003*\"don\" + 0.003*\"people\" + 0.003*\"just\" + 0.002*\"think\" + 0.002*\"know\" + 0.002*\"said\" + 0.002*\"time\" + 0.002*\"like\" + 0.002*\"armenian\" + 0.002*\"say\"\n",
      "2019-11-24 17:23:22,084 : INFO : topic #1 (0.167): 0.002*\"jpeg\" + 0.002*\"like\" + 0.001*\"just\" + 0.001*\"space\" + 0.001*\"people\" + 0.001*\"good\" + 0.001*\"year\" + 0.001*\"time\" + 0.001*\"better\" + 0.001*\"don\"\n",
      "2019-11-24 17:23:22,096 : INFO : topic diff=1.269042, rho=0.707107\n",
      "2019-11-24 17:23:22,202 : INFO : PROGRESS: pass 1, at document #2000/3416\n",
      "2019-11-24 17:23:23,716 : INFO : merging changes from 2000 documents into a model of 3416 documents\n",
      "2019-11-24 17:23:23,865 : INFO : topic #4 (0.167): 0.001*\"god\" + 0.001*\"like\" + 0.001*\"data\" + 0.001*\"people\" + 0.001*\"new\" + 0.001*\"does\" + 0.001*\"use\" + 0.001*\"don\" + 0.001*\"just\" + 0.001*\"edu\"\n",
      "2019-11-24 17:23:23,873 : INFO : topic #2 (0.167): 0.003*\"don\" + 0.003*\"people\" + 0.003*\"just\" + 0.002*\"think\" + 0.002*\"said\" + 0.002*\"know\" + 0.002*\"time\" + 0.002*\"like\" + 0.002*\"say\" + 0.001*\"israel\"\n",
      "2019-11-24 17:23:23,882 : INFO : topic #0 (0.167): 0.002*\"space\" + 0.001*\"think\" + 0.001*\"like\" + 0.001*\"people\" + 0.001*\"don\" + 0.001*\"just\" + 0.001*\"time\" + 0.001*\"good\" + 0.001*\"years\" + 0.001*\"know\"\n",
      "2019-11-24 17:23:23,889 : INFO : topic #5 (0.167): 0.003*\"people\" + 0.002*\"don\" + 0.002*\"like\" + 0.002*\"know\" + 0.002*\"armenian\" + 0.002*\"just\" + 0.002*\"armenians\" + 0.002*\"turkish\" + 0.002*\"time\" + 0.001*\"said\"\n",
      "2019-11-24 17:23:23,896 : INFO : topic #3 (0.167): 0.002*\"space\" + 0.002*\"like\" + 0.001*\"just\" + 0.001*\"don\" + 0.001*\"think\" + 0.001*\"use\" + 0.001*\"did\" + 0.001*\"know\" + 0.001*\"time\" + 0.001*\"israel\"\n",
      "2019-11-24 17:23:23,908 : INFO : topic diff=0.713888, rho=0.519314\n",
      "2019-11-24 17:23:28,035 : INFO : -13.493 per-word bound, 11529.2 perplexity estimate based on a held-out corpus of 1416 documents with 254128 words\n",
      "2019-11-24 17:23:28,037 : INFO : PROGRESS: pass 1, at document #3416/3416\n",
      "2019-11-24 17:23:29,171 : INFO : merging changes from 1416 documents into a model of 3416 documents\n",
      "2019-11-24 17:23:29,317 : INFO : topic #3 (0.167): 0.004*\"space\" + 0.002*\"launch\" + 0.002*\"like\" + 0.001*\"just\" + 0.001*\"satellite\" + 0.001*\"don\" + 0.001*\"new\" + 0.001*\"use\" + 0.001*\"year\" + 0.001*\"think\"\n",
      "2019-11-24 17:23:29,325 : INFO : topic #4 (0.167): 0.001*\"god\" + 0.001*\"data\" + 0.001*\"new\" + 0.001*\"like\" + 0.001*\"image\" + 0.001*\"use\" + 0.001*\"does\" + 0.001*\"don\" + 0.001*\"graphics\" + 0.001*\"people\"\n",
      "2019-11-24 17:23:29,334 : INFO : topic #1 (0.167): 0.002*\"jpeg\" + 0.001*\"like\" + 0.001*\"year\" + 0.001*\"just\" + 0.001*\"good\" + 0.001*\"better\" + 0.001*\"edu\" + 0.001*\"time\" + 0.001*\"years\" + 0.001*\"think\"\n",
      "2019-11-24 17:23:29,342 : INFO : topic #0 (0.167): 0.002*\"space\" + 0.001*\"think\" + 0.001*\"like\" + 0.001*\"don\" + 0.001*\"people\" + 0.001*\"just\" + 0.001*\"years\" + 0.001*\"time\" + 0.001*\"new\" + 0.001*\"good\"\n",
      "2019-11-24 17:23:29,351 : INFO : topic #5 (0.167): 0.004*\"people\" + 0.003*\"don\" + 0.002*\"like\" + 0.002*\"know\" + 0.002*\"armenian\" + 0.002*\"just\" + 0.002*\"image\" + 0.002*\"armenians\" + 0.002*\"said\" + 0.002*\"time\"\n",
      "2019-11-24 17:23:29,363 : INFO : topic diff=0.624505, rho=0.519314\n",
      "2019-11-24 17:23:29,470 : INFO : PROGRESS: pass 2, at document #2000/3416\n",
      "2019-11-24 17:23:30,741 : INFO : merging changes from 2000 documents into a model of 3416 documents\n",
      "2019-11-24 17:23:30,887 : INFO : topic #4 (0.167): 0.001*\"data\" + 0.001*\"god\" + 0.001*\"new\" + 0.001*\"like\" + 0.001*\"does\" + 0.001*\"use\" + 0.001*\"edu\" + 0.001*\"image\" + 0.001*\"graphics\" + 0.001*\"won\"\n",
      "2019-11-24 17:23:30,897 : INFO : topic #5 (0.167): 0.004*\"people\" + 0.002*\"don\" + 0.002*\"armenian\" + 0.002*\"know\" + 0.002*\"like\" + 0.002*\"just\" + 0.002*\"armenians\" + 0.002*\"turkish\" + 0.002*\"said\" + 0.002*\"time\"\n",
      "2019-11-24 17:23:30,905 : INFO : topic #1 (0.167): 0.001*\"like\" + 0.001*\"just\" + 0.001*\"edu\" + 0.001*\"year\" + 0.001*\"jpeg\" + 0.001*\"good\" + 0.001*\"time\" + 0.001*\"better\" + 0.001*\"years\" + 0.001*\"think\"\n",
      "2019-11-24 17:23:30,915 : INFO : topic #0 (0.167): 0.002*\"space\" + 0.001*\"think\" + 0.001*\"just\" + 0.001*\"like\" + 0.001*\"don\" + 0.001*\"years\" + 0.001*\"people\" + 0.001*\"good\" + 0.001*\"time\" + 0.001*\"know\"\n",
      "2019-11-24 17:23:30,922 : INFO : topic #2 (0.167): 0.003*\"don\" + 0.002*\"people\" + 0.002*\"just\" + 0.002*\"think\" + 0.002*\"said\" + 0.002*\"know\" + 0.002*\"like\" + 0.002*\"time\" + 0.002*\"israel\" + 0.002*\"say\"\n",
      "2019-11-24 17:23:30,934 : INFO : topic diff=0.472968, rho=0.460874\n",
      "2019-11-24 17:23:33,780 : INFO : -12.662 per-word bound, 6479.2 perplexity estimate based on a held-out corpus of 1416 documents with 254128 words\n",
      "2019-11-24 17:23:33,782 : INFO : PROGRESS: pass 2, at document #3416/3416\n",
      "2019-11-24 17:23:34,711 : INFO : merging changes from 1416 documents into a model of 3416 documents\n",
      "2019-11-24 17:23:34,857 : INFO : topic #4 (0.167): 0.002*\"image\" + 0.002*\"data\" + 0.001*\"new\" + 0.001*\"god\" + 0.001*\"use\" + 0.001*\"like\" + 0.001*\"graphics\" + 0.001*\"does\" + 0.001*\"edu\" + 0.001*\"software\"\n",
      "2019-11-24 17:23:34,864 : INFO : topic #3 (0.167): 0.004*\"space\" + 0.002*\"launch\" + 0.001*\"like\" + 0.001*\"satellite\" + 0.001*\"just\" + 0.001*\"don\" + 0.001*\"new\" + 0.001*\"year\" + 0.001*\"use\" + 0.001*\"time\"\n",
      "2019-11-24 17:23:34,873 : INFO : topic #0 (0.167): 0.002*\"space\" + 0.001*\"think\" + 0.001*\"just\" + 0.001*\"don\" + 0.001*\"like\" + 0.001*\"years\" + 0.001*\"people\" + 0.001*\"good\" + 0.001*\"time\" + 0.001*\"new\"\n",
      "2019-11-24 17:23:34,882 : INFO : topic #1 (0.167): 0.001*\"like\" + 0.001*\"year\" + 0.001*\"good\" + 0.001*\"just\" + 0.001*\"time\" + 0.001*\"better\" + 0.001*\"edu\" + 0.001*\"jpeg\" + 0.001*\"years\" + 0.001*\"think\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-24 17:23:34,891 : INFO : topic #5 (0.167): 0.004*\"people\" + 0.002*\"don\" + 0.002*\"know\" + 0.002*\"armenian\" + 0.002*\"like\" + 0.002*\"just\" + 0.002*\"said\" + 0.002*\"armenians\" + 0.002*\"image\" + 0.002*\"time\"\n",
      "2019-11-24 17:23:34,903 : INFO : topic diff=0.400402, rho=0.460874\n",
      "2019-11-24 17:23:35,008 : INFO : PROGRESS: pass 3, at document #2000/3416\n",
      "2019-11-24 17:23:36,226 : INFO : merging changes from 2000 documents into a model of 3416 documents\n",
      "2019-11-24 17:23:36,372 : INFO : topic #5 (0.167): 0.004*\"people\" + 0.002*\"don\" + 0.002*\"know\" + 0.002*\"armenian\" + 0.002*\"like\" + 0.002*\"just\" + 0.002*\"armenians\" + 0.002*\"said\" + 0.001*\"turkish\" + 0.001*\"time\"\n",
      "2019-11-24 17:23:36,380 : INFO : topic #2 (0.167): 0.002*\"don\" + 0.002*\"people\" + 0.002*\"just\" + 0.002*\"think\" + 0.002*\"said\" + 0.002*\"like\" + 0.002*\"israel\" + 0.002*\"know\" + 0.002*\"time\" + 0.001*\"say\"\n",
      "2019-11-24 17:23:36,388 : INFO : topic #1 (0.167): 0.001*\"like\" + 0.001*\"just\" + 0.001*\"year\" + 0.001*\"edu\" + 0.001*\"good\" + 0.001*\"time\" + 0.001*\"years\" + 0.001*\"better\" + 0.001*\"think\" + 0.001*\"team\"\n",
      "2019-11-24 17:23:36,396 : INFO : topic #0 (0.167): 0.002*\"space\" + 0.001*\"think\" + 0.001*\"just\" + 0.001*\"years\" + 0.001*\"don\" + 0.001*\"like\" + 0.001*\"good\" + 0.001*\"people\" + 0.001*\"time\" + 0.001*\"know\"\n",
      "2019-11-24 17:23:36,402 : INFO : topic #3 (0.167): 0.003*\"space\" + 0.001*\"like\" + 0.001*\"launch\" + 0.001*\"just\" + 0.001*\"don\" + 0.001*\"dod\" + 0.001*\"think\" + 0.001*\"time\" + 0.001*\"use\" + 0.001*\"satellite\"\n",
      "2019-11-24 17:23:36,413 : INFO : topic diff=0.295392, rho=0.418560\n",
      "2019-11-24 17:23:39,299 : INFO : -12.015 per-word bound, 4138.4 perplexity estimate based on a held-out corpus of 1416 documents with 254128 words\n",
      "2019-11-24 17:23:39,300 : INFO : PROGRESS: pass 3, at document #3416/3416\n",
      "2019-11-24 17:23:40,098 : INFO : merging changes from 1416 documents into a model of 3416 documents\n",
      "2019-11-24 17:23:40,250 : INFO : topic #0 (0.167): 0.002*\"space\" + 0.001*\"think\" + 0.001*\"just\" + 0.001*\"don\" + 0.001*\"like\" + 0.001*\"years\" + 0.001*\"people\" + 0.001*\"good\" + 0.001*\"time\" + 0.001*\"new\"\n",
      "2019-11-24 17:23:40,259 : INFO : topic #5 (0.167): 0.004*\"people\" + 0.002*\"don\" + 0.002*\"know\" + 0.002*\"armenian\" + 0.002*\"like\" + 0.002*\"just\" + 0.002*\"said\" + 0.002*\"armenians\" + 0.001*\"jpeg\" + 0.001*\"god\"\n",
      "2019-11-24 17:23:40,265 : INFO : topic #3 (0.167): 0.004*\"space\" + 0.002*\"launch\" + 0.001*\"satellite\" + 0.001*\"like\" + 0.001*\"just\" + 0.001*\"don\" + 0.001*\"new\" + 0.001*\"year\" + 0.001*\"time\" + 0.001*\"think\"\n",
      "2019-11-24 17:23:40,273 : INFO : topic #4 (0.167): 0.002*\"image\" + 0.002*\"data\" + 0.001*\"new\" + 0.001*\"edu\" + 0.001*\"graphics\" + 0.001*\"use\" + 0.001*\"like\" + 0.001*\"software\" + 0.001*\"god\" + 0.001*\"does\"\n",
      "2019-11-24 17:23:40,281 : INFO : topic #1 (0.167): 0.001*\"like\" + 0.001*\"year\" + 0.001*\"good\" + 0.001*\"just\" + 0.001*\"time\" + 0.001*\"edu\" + 0.001*\"better\" + 0.001*\"years\" + 0.001*\"think\" + 0.001*\"team\"\n",
      "2019-11-24 17:23:40,292 : INFO : topic diff=0.255456, rho=0.418560\n",
      "2019-11-24 17:23:40,401 : INFO : PROGRESS: pass 4, at document #2000/3416\n",
      "2019-11-24 17:23:41,613 : INFO : merging changes from 2000 documents into a model of 3416 documents\n",
      "2019-11-24 17:23:41,758 : INFO : topic #3 (0.167): 0.003*\"space\" + 0.001*\"launch\" + 0.001*\"like\" + 0.001*\"just\" + 0.001*\"don\" + 0.001*\"dod\" + 0.001*\"satellite\" + 0.001*\"think\" + 0.001*\"time\" + 0.001*\"use\"\n",
      "2019-11-24 17:23:41,766 : INFO : topic #2 (0.167): 0.002*\"people\" + 0.002*\"don\" + 0.002*\"just\" + 0.002*\"think\" + 0.002*\"israel\" + 0.002*\"said\" + 0.002*\"like\" + 0.002*\"know\" + 0.001*\"time\" + 0.001*\"say\"\n",
      "2019-11-24 17:23:41,777 : INFO : topic #0 (0.167): 0.002*\"space\" + 0.001*\"think\" + 0.001*\"just\" + 0.001*\"years\" + 0.001*\"don\" + 0.001*\"like\" + 0.001*\"good\" + 0.001*\"people\" + 0.001*\"time\" + 0.001*\"know\"\n",
      "2019-11-24 17:23:41,786 : INFO : topic #1 (0.167): 0.001*\"like\" + 0.001*\"year\" + 0.001*\"just\" + 0.001*\"good\" + 0.001*\"edu\" + 0.001*\"time\" + 0.001*\"years\" + 0.001*\"think\" + 0.001*\"better\" + 0.001*\"team\"\n",
      "2019-11-24 17:23:41,795 : INFO : topic #4 (0.167): 0.002*\"image\" + 0.002*\"data\" + 0.001*\"new\" + 0.001*\"edu\" + 0.001*\"graphics\" + 0.001*\"use\" + 0.001*\"like\" + 0.001*\"god\" + 0.001*\"does\" + 0.001*\"software\"\n",
      "2019-11-24 17:23:41,806 : INFO : topic diff=0.204918, rho=0.386103\n",
      "2019-11-24 17:23:44,464 : INFO : -11.748 per-word bound, 3440.5 perplexity estimate based on a held-out corpus of 1416 documents with 254128 words\n",
      "2019-11-24 17:23:44,465 : INFO : PROGRESS: pass 4, at document #3416/3416\n",
      "2019-11-24 17:23:45,227 : INFO : merging changes from 1416 documents into a model of 3416 documents\n",
      "2019-11-24 17:23:45,373 : INFO : topic #3 (0.167): 0.003*\"space\" + 0.002*\"launch\" + 0.001*\"satellite\" + 0.001*\"like\" + 0.001*\"just\" + 0.001*\"don\" + 0.001*\"new\" + 0.001*\"year\" + 0.001*\"time\" + 0.001*\"think\"\n",
      "2019-11-24 17:23:45,381 : INFO : topic #4 (0.167): 0.002*\"image\" + 0.002*\"data\" + 0.001*\"new\" + 0.001*\"edu\" + 0.001*\"graphics\" + 0.001*\"use\" + 0.001*\"software\" + 0.001*\"like\" + 0.001*\"does\" + 0.001*\"images\"\n",
      "2019-11-24 17:23:45,389 : INFO : topic #2 (0.167): 0.002*\"people\" + 0.002*\"don\" + 0.002*\"just\" + 0.002*\"think\" + 0.002*\"israel\" + 0.002*\"like\" + 0.002*\"said\" + 0.001*\"know\" + 0.001*\"time\" + 0.001*\"say\"\n",
      "2019-11-24 17:23:45,399 : INFO : topic #1 (0.167): 0.001*\"like\" + 0.001*\"year\" + 0.001*\"good\" + 0.001*\"just\" + 0.001*\"time\" + 0.001*\"edu\" + 0.001*\"better\" + 0.001*\"think\" + 0.001*\"years\" + 0.001*\"new\"\n",
      "2019-11-24 17:23:45,409 : INFO : topic #0 (0.167): 0.002*\"space\" + 0.001*\"think\" + 0.001*\"don\" + 0.001*\"just\" + 0.001*\"like\" + 0.001*\"years\" + 0.001*\"people\" + 0.001*\"good\" + 0.001*\"time\" + 0.001*\"know\"\n",
      "2019-11-24 17:23:45,420 : INFO : topic diff=0.182481, rho=0.386103\n",
      "2019-11-24 17:23:45,527 : INFO : PROGRESS: pass 5, at document #2000/3416\n",
      "2019-11-24 17:23:46,661 : INFO : merging changes from 2000 documents into a model of 3416 documents\n",
      "2019-11-24 17:23:46,806 : INFO : topic #3 (0.167): 0.003*\"space\" + 0.001*\"launch\" + 0.001*\"like\" + 0.001*\"just\" + 0.001*\"don\" + 0.001*\"dod\" + 0.001*\"satellite\" + 0.001*\"think\" + 0.001*\"time\" + 0.001*\"know\"\n",
      "2019-11-24 17:23:46,815 : INFO : topic #5 (0.167): 0.004*\"people\" + 0.002*\"don\" + 0.002*\"know\" + 0.002*\"armenian\" + 0.002*\"like\" + 0.002*\"just\" + 0.002*\"armenians\" + 0.002*\"said\" + 0.001*\"time\" + 0.001*\"turkish\"\n",
      "2019-11-24 17:23:46,825 : INFO : topic #1 (0.167): 0.001*\"like\" + 0.001*\"year\" + 0.001*\"just\" + 0.001*\"good\" + 0.001*\"edu\" + 0.001*\"time\" + 0.001*\"think\" + 0.001*\"years\" + 0.001*\"better\" + 0.001*\"team\"\n",
      "2019-11-24 17:23:46,834 : INFO : topic #0 (0.167): 0.002*\"space\" + 0.001*\"think\" + 0.001*\"just\" + 0.001*\"years\" + 0.001*\"don\" + 0.001*\"like\" + 0.001*\"people\" + 0.001*\"good\" + 0.001*\"time\" + 0.001*\"know\"\n",
      "2019-11-24 17:23:46,843 : INFO : topic #4 (0.167): 0.002*\"image\" + 0.002*\"data\" + 0.001*\"new\" + 0.001*\"edu\" + 0.001*\"graphics\" + 0.001*\"use\" + 0.001*\"like\" + 0.001*\"does\" + 0.001*\"available\" + 0.001*\"software\"\n",
      "2019-11-24 17:23:46,855 : INFO : topic diff=0.162940, rho=0.360188\n",
      "2019-11-24 17:23:49,504 : INFO : -11.656 per-word bound, 3226.8 perplexity estimate based on a held-out corpus of 1416 documents with 254128 words\n",
      "2019-11-24 17:23:49,506 : INFO : PROGRESS: pass 5, at document #3416/3416\n",
      "2019-11-24 17:23:50,239 : INFO : merging changes from 1416 documents into a model of 3416 documents\n",
      "2019-11-24 17:23:50,388 : INFO : topic #5 (0.167): 0.004*\"people\" + 0.002*\"don\" + 0.002*\"know\" + 0.002*\"armenian\" + 0.002*\"just\" + 0.002*\"like\" + 0.002*\"said\" + 0.002*\"armenians\" + 0.001*\"jpeg\" + 0.001*\"god\"\n",
      "2019-11-24 17:23:50,396 : INFO : topic #4 (0.167): 0.002*\"image\" + 0.002*\"data\" + 0.001*\"new\" + 0.001*\"edu\" + 0.001*\"graphics\" + 0.001*\"software\" + 0.001*\"use\" + 0.001*\"like\" + 0.001*\"images\" + 0.001*\"does\"\n",
      "2019-11-24 17:23:50,404 : INFO : topic #2 (0.167): 0.002*\"people\" + 0.002*\"don\" + 0.002*\"just\" + 0.002*\"israel\" + 0.002*\"think\" + 0.002*\"like\" + 0.002*\"said\" + 0.001*\"know\" + 0.001*\"time\" + 0.001*\"year\"\n",
      "2019-11-24 17:23:50,411 : INFO : topic #3 (0.167): 0.003*\"space\" + 0.002*\"launch\" + 0.001*\"satellite\" + 0.001*\"like\" + 0.001*\"just\" + 0.001*\"don\" + 0.001*\"new\" + 0.001*\"year\" + 0.001*\"time\" + 0.001*\"think\"\n",
      "2019-11-24 17:23:50,419 : INFO : topic #0 (0.167): 0.002*\"space\" + 0.001*\"think\" + 0.001*\"don\" + 0.001*\"just\" + 0.001*\"like\" + 0.001*\"years\" + 0.001*\"people\" + 0.001*\"good\" + 0.001*\"time\" + 0.001*\"know\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-24 17:23:50,431 : INFO : topic diff=0.148887, rho=0.360188\n",
      "2019-11-24 17:23:50,533 : INFO : PROGRESS: pass 6, at document #2000/3416\n",
      "2019-11-24 17:23:51,644 : INFO : merging changes from 2000 documents into a model of 3416 documents\n",
      "2019-11-24 17:23:51,791 : INFO : topic #4 (0.167): 0.002*\"image\" + 0.002*\"data\" + 0.001*\"new\" + 0.001*\"edu\" + 0.001*\"graphics\" + 0.001*\"use\" + 0.001*\"like\" + 0.001*\"available\" + 0.001*\"software\" + 0.001*\"does\"\n",
      "2019-11-24 17:23:51,798 : INFO : topic #3 (0.167): 0.003*\"space\" + 0.002*\"launch\" + 0.001*\"like\" + 0.001*\"just\" + 0.001*\"don\" + 0.001*\"dod\" + 0.001*\"satellite\" + 0.001*\"think\" + 0.001*\"time\" + 0.001*\"know\"\n",
      "2019-11-24 17:23:51,805 : INFO : topic #2 (0.167): 0.002*\"people\" + 0.002*\"don\" + 0.002*\"just\" + 0.002*\"israel\" + 0.002*\"think\" + 0.002*\"like\" + 0.002*\"said\" + 0.001*\"know\" + 0.001*\"time\" + 0.001*\"say\"\n",
      "2019-11-24 17:23:51,815 : INFO : topic #0 (0.167): 0.002*\"space\" + 0.001*\"think\" + 0.001*\"just\" + 0.001*\"years\" + 0.001*\"don\" + 0.001*\"like\" + 0.001*\"people\" + 0.001*\"good\" + 0.001*\"time\" + 0.001*\"know\"\n",
      "2019-11-24 17:23:51,824 : INFO : topic #5 (0.167): 0.004*\"people\" + 0.002*\"don\" + 0.002*\"know\" + 0.002*\"armenian\" + 0.002*\"like\" + 0.002*\"just\" + 0.002*\"armenians\" + 0.002*\"said\" + 0.001*\"time\" + 0.001*\"turkish\"\n",
      "2019-11-24 17:23:51,836 : INFO : topic diff=0.140294, rho=0.338876\n",
      "2019-11-24 17:23:54,932 : INFO : -11.616 per-word bound, 3139.4 perplexity estimate based on a held-out corpus of 1416 documents with 254128 words\n",
      "2019-11-24 17:23:54,933 : INFO : PROGRESS: pass 6, at document #3416/3416\n",
      "2019-11-24 17:23:55,761 : INFO : merging changes from 1416 documents into a model of 3416 documents\n",
      "2019-11-24 17:23:55,925 : INFO : topic #0 (0.167): 0.002*\"space\" + 0.001*\"think\" + 0.001*\"don\" + 0.001*\"just\" + 0.001*\"years\" + 0.001*\"like\" + 0.001*\"people\" + 0.001*\"good\" + 0.001*\"time\" + 0.001*\"know\"\n",
      "2019-11-24 17:23:55,935 : INFO : topic #4 (0.167): 0.002*\"image\" + 0.002*\"data\" + 0.001*\"new\" + 0.001*\"edu\" + 0.001*\"graphics\" + 0.001*\"software\" + 0.001*\"use\" + 0.001*\"like\" + 0.001*\"images\" + 0.001*\"available\"\n",
      "2019-11-24 17:23:55,944 : INFO : topic #2 (0.167): 0.002*\"people\" + 0.002*\"don\" + 0.002*\"just\" + 0.002*\"israel\" + 0.002*\"think\" + 0.002*\"like\" + 0.002*\"said\" + 0.001*\"know\" + 0.001*\"time\" + 0.001*\"year\"\n",
      "2019-11-24 17:23:55,955 : INFO : topic #5 (0.167): 0.004*\"people\" + 0.002*\"don\" + 0.002*\"know\" + 0.002*\"armenian\" + 0.002*\"just\" + 0.002*\"like\" + 0.002*\"said\" + 0.002*\"armenians\" + 0.001*\"god\" + 0.001*\"jpeg\"\n",
      "2019-11-24 17:23:55,968 : INFO : topic #1 (0.167): 0.001*\"like\" + 0.001*\"year\" + 0.001*\"good\" + 0.001*\"just\" + 0.001*\"time\" + 0.001*\"edu\" + 0.001*\"better\" + 0.001*\"think\" + 0.001*\"years\" + 0.001*\"new\"\n",
      "2019-11-24 17:23:55,981 : INFO : topic diff=0.131137, rho=0.338876\n",
      "2019-11-24 17:23:56,110 : INFO : PROGRESS: pass 7, at document #2000/3416\n",
      "2019-11-24 17:23:57,322 : INFO : merging changes from 2000 documents into a model of 3416 documents\n",
      "2019-11-24 17:23:57,509 : INFO : topic #5 (0.167): 0.004*\"people\" + 0.002*\"don\" + 0.002*\"armenian\" + 0.002*\"know\" + 0.002*\"like\" + 0.002*\"just\" + 0.002*\"armenians\" + 0.002*\"said\" + 0.001*\"time\" + 0.001*\"turkish\"\n",
      "2019-11-24 17:23:57,520 : INFO : topic #0 (0.167): 0.002*\"space\" + 0.001*\"think\" + 0.001*\"just\" + 0.001*\"don\" + 0.001*\"years\" + 0.001*\"people\" + 0.001*\"like\" + 0.001*\"good\" + 0.001*\"time\" + 0.001*\"know\"\n",
      "2019-11-24 17:23:57,542 : INFO : topic #4 (0.167): 0.002*\"image\" + 0.002*\"data\" + 0.001*\"new\" + 0.001*\"graphics\" + 0.001*\"edu\" + 0.001*\"use\" + 0.001*\"like\" + 0.001*\"available\" + 0.001*\"software\" + 0.001*\"does\"\n",
      "2019-11-24 17:23:57,552 : INFO : topic #3 (0.167): 0.003*\"space\" + 0.002*\"launch\" + 0.001*\"like\" + 0.001*\"just\" + 0.001*\"don\" + 0.001*\"satellite\" + 0.001*\"dod\" + 0.001*\"think\" + 0.001*\"time\" + 0.001*\"know\"\n",
      "2019-11-24 17:23:57,562 : INFO : topic #1 (0.167): 0.001*\"like\" + 0.001*\"year\" + 0.001*\"just\" + 0.001*\"good\" + 0.001*\"edu\" + 0.001*\"time\" + 0.001*\"think\" + 0.001*\"years\" + 0.001*\"better\" + 0.001*\"team\"\n",
      "2019-11-24 17:23:57,576 : INFO : topic diff=0.126618, rho=0.320948\n",
      "2019-11-24 17:24:00,940 : INFO : -11.594 per-word bound, 3092.1 perplexity estimate based on a held-out corpus of 1416 documents with 254128 words\n",
      "2019-11-24 17:24:00,942 : INFO : PROGRESS: pass 7, at document #3416/3416\n",
      "2019-11-24 17:24:01,691 : INFO : merging changes from 1416 documents into a model of 3416 documents\n",
      "2019-11-24 17:24:01,846 : INFO : topic #5 (0.167): 0.004*\"people\" + 0.002*\"don\" + 0.002*\"know\" + 0.002*\"armenian\" + 0.002*\"just\" + 0.002*\"like\" + 0.002*\"said\" + 0.002*\"armenians\" + 0.001*\"god\" + 0.001*\"jpeg\"\n",
      "2019-11-24 17:24:01,854 : INFO : topic #3 (0.167): 0.003*\"space\" + 0.002*\"launch\" + 0.001*\"satellite\" + 0.001*\"like\" + 0.001*\"just\" + 0.001*\"don\" + 0.001*\"new\" + 0.001*\"time\" + 0.001*\"think\" + 0.001*\"year\"\n",
      "2019-11-24 17:24:01,865 : INFO : topic #0 (0.167): 0.002*\"space\" + 0.001*\"think\" + 0.001*\"don\" + 0.001*\"just\" + 0.001*\"years\" + 0.001*\"like\" + 0.001*\"people\" + 0.001*\"good\" + 0.001*\"time\" + 0.001*\"know\"\n",
      "2019-11-24 17:24:01,876 : INFO : topic #4 (0.167): 0.002*\"image\" + 0.002*\"data\" + 0.001*\"new\" + 0.001*\"graphics\" + 0.001*\"edu\" + 0.001*\"software\" + 0.001*\"use\" + 0.001*\"like\" + 0.001*\"images\" + 0.001*\"available\"\n",
      "2019-11-24 17:24:01,892 : INFO : topic #1 (0.167): 0.001*\"like\" + 0.001*\"year\" + 0.001*\"good\" + 0.001*\"just\" + 0.001*\"time\" + 0.001*\"edu\" + 0.001*\"better\" + 0.001*\"think\" + 0.001*\"years\" + 0.001*\"new\"\n",
      "2019-11-24 17:24:01,907 : INFO : topic diff=0.119896, rho=0.320948\n",
      "2019-11-24 17:24:02,041 : INFO : PROGRESS: pass 8, at document #2000/3416\n",
      "2019-11-24 17:24:03,224 : INFO : merging changes from 2000 documents into a model of 3416 documents\n",
      "2019-11-24 17:24:03,377 : INFO : topic #3 (0.167): 0.003*\"space\" + 0.002*\"launch\" + 0.001*\"like\" + 0.001*\"just\" + 0.001*\"don\" + 0.001*\"satellite\" + 0.001*\"dod\" + 0.001*\"think\" + 0.001*\"time\" + 0.001*\"know\"\n",
      "2019-11-24 17:24:03,385 : INFO : topic #2 (0.167): 0.002*\"people\" + 0.002*\"don\" + 0.002*\"just\" + 0.002*\"israel\" + 0.002*\"think\" + 0.002*\"like\" + 0.002*\"said\" + 0.001*\"know\" + 0.001*\"time\" + 0.001*\"year\"\n",
      "2019-11-24 17:24:03,397 : INFO : topic #1 (0.167): 0.001*\"like\" + 0.001*\"year\" + 0.001*\"just\" + 0.001*\"good\" + 0.001*\"edu\" + 0.001*\"time\" + 0.001*\"think\" + 0.001*\"better\" + 0.001*\"years\" + 0.001*\"team\"\n",
      "2019-11-24 17:24:03,411 : INFO : topic #0 (0.167): 0.002*\"space\" + 0.001*\"think\" + 0.001*\"just\" + 0.001*\"don\" + 0.001*\"years\" + 0.001*\"people\" + 0.001*\"like\" + 0.001*\"good\" + 0.001*\"time\" + 0.001*\"know\"\n",
      "2019-11-24 17:24:03,421 : INFO : topic #4 (0.167): 0.002*\"image\" + 0.002*\"data\" + 0.001*\"graphics\" + 0.001*\"new\" + 0.001*\"edu\" + 0.001*\"use\" + 0.001*\"like\" + 0.001*\"available\" + 0.001*\"software\" + 0.001*\"does\"\n",
      "2019-11-24 17:24:03,435 : INFO : topic diff=0.117032, rho=0.305595\n",
      "2019-11-24 17:24:06,478 : INFO : -11.580 per-word bound, 3062.0 perplexity estimate based on a held-out corpus of 1416 documents with 254128 words\n",
      "2019-11-24 17:24:06,479 : INFO : PROGRESS: pass 8, at document #3416/3416\n",
      "2019-11-24 17:24:07,206 : INFO : merging changes from 1416 documents into a model of 3416 documents\n",
      "2019-11-24 17:24:07,362 : INFO : topic #4 (0.167): 0.002*\"image\" + 0.002*\"data\" + 0.001*\"new\" + 0.001*\"graphics\" + 0.001*\"edu\" + 0.001*\"software\" + 0.001*\"use\" + 0.001*\"like\" + 0.001*\"images\" + 0.001*\"available\"\n",
      "2019-11-24 17:24:07,370 : INFO : topic #1 (0.167): 0.001*\"like\" + 0.001*\"year\" + 0.001*\"good\" + 0.001*\"just\" + 0.001*\"time\" + 0.001*\"edu\" + 0.001*\"better\" + 0.001*\"think\" + 0.001*\"years\" + 0.001*\"new\"\n",
      "2019-11-24 17:24:07,379 : INFO : topic #5 (0.167): 0.004*\"people\" + 0.002*\"don\" + 0.002*\"know\" + 0.002*\"armenian\" + 0.002*\"just\" + 0.002*\"like\" + 0.002*\"said\" + 0.002*\"armenians\" + 0.001*\"god\" + 0.001*\"jpeg\"\n",
      "2019-11-24 17:24:07,390 : INFO : topic #0 (0.167): 0.002*\"space\" + 0.001*\"think\" + 0.001*\"don\" + 0.001*\"just\" + 0.001*\"people\" + 0.001*\"years\" + 0.001*\"like\" + 0.001*\"good\" + 0.001*\"time\" + 0.001*\"know\"\n",
      "2019-11-24 17:24:07,399 : INFO : topic #3 (0.167): 0.004*\"space\" + 0.002*\"launch\" + 0.001*\"satellite\" + 0.001*\"like\" + 0.001*\"just\" + 0.001*\"don\" + 0.001*\"new\" + 0.001*\"time\" + 0.001*\"think\" + 0.001*\"year\"\n",
      "2019-11-24 17:24:07,412 : INFO : topic diff=0.112317, rho=0.305595\n",
      "2019-11-24 17:24:07,593 : INFO : PROGRESS: pass 9, at document #2000/3416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-24 17:24:08,722 : INFO : merging changes from 2000 documents into a model of 3416 documents\n",
      "2019-11-24 17:24:08,879 : INFO : topic #4 (0.167): 0.002*\"image\" + 0.002*\"data\" + 0.001*\"graphics\" + 0.001*\"new\" + 0.001*\"edu\" + 0.001*\"use\" + 0.001*\"like\" + 0.001*\"available\" + 0.001*\"software\" + 0.001*\"does\"\n",
      "2019-11-24 17:24:08,890 : INFO : topic #1 (0.167): 0.001*\"like\" + 0.001*\"year\" + 0.001*\"just\" + 0.001*\"good\" + 0.001*\"edu\" + 0.001*\"time\" + 0.001*\"think\" + 0.001*\"better\" + 0.001*\"years\" + 0.001*\"team\"\n",
      "2019-11-24 17:24:08,898 : INFO : topic #5 (0.167): 0.003*\"people\" + 0.002*\"don\" + 0.002*\"armenian\" + 0.002*\"know\" + 0.002*\"like\" + 0.002*\"just\" + 0.002*\"armenians\" + 0.002*\"said\" + 0.001*\"time\" + 0.001*\"turkish\"\n",
      "2019-11-24 17:24:08,911 : INFO : topic #0 (0.167): 0.002*\"space\" + 0.001*\"think\" + 0.001*\"just\" + 0.001*\"don\" + 0.001*\"years\" + 0.001*\"people\" + 0.001*\"like\" + 0.001*\"good\" + 0.001*\"time\" + 0.001*\"know\"\n",
      "2019-11-24 17:24:08,923 : INFO : topic #2 (0.167): 0.002*\"people\" + 0.002*\"don\" + 0.002*\"just\" + 0.002*\"israel\" + 0.002*\"think\" + 0.002*\"like\" + 0.002*\"said\" + 0.001*\"know\" + 0.001*\"time\" + 0.001*\"year\"\n",
      "2019-11-24 17:24:08,940 : INFO : topic diff=0.109203, rho=0.292253\n",
      "2019-11-24 17:24:11,619 : INFO : -11.570 per-word bound, 3040.7 perplexity estimate based on a held-out corpus of 1416 documents with 254128 words\n",
      "2019-11-24 17:24:11,621 : INFO : PROGRESS: pass 9, at document #3416/3416\n",
      "2019-11-24 17:24:12,339 : INFO : merging changes from 1416 documents into a model of 3416 documents\n",
      "2019-11-24 17:24:12,490 : INFO : topic #0 (0.167): 0.002*\"space\" + 0.001*\"think\" + 0.001*\"don\" + 0.001*\"just\" + 0.001*\"people\" + 0.001*\"years\" + 0.001*\"like\" + 0.001*\"good\" + 0.001*\"time\" + 0.001*\"know\"\n",
      "2019-11-24 17:24:12,498 : INFO : topic #1 (0.167): 0.001*\"like\" + 0.001*\"year\" + 0.001*\"good\" + 0.001*\"just\" + 0.001*\"time\" + 0.001*\"edu\" + 0.001*\"think\" + 0.001*\"better\" + 0.001*\"years\" + 0.001*\"new\"\n",
      "2019-11-24 17:24:12,508 : INFO : topic #2 (0.167): 0.002*\"people\" + 0.002*\"don\" + 0.002*\"just\" + 0.002*\"israel\" + 0.002*\"think\" + 0.002*\"like\" + 0.001*\"said\" + 0.001*\"know\" + 0.001*\"time\" + 0.001*\"year\"\n",
      "2019-11-24 17:24:12,519 : INFO : topic #5 (0.167): 0.004*\"people\" + 0.002*\"don\" + 0.002*\"know\" + 0.002*\"armenian\" + 0.002*\"just\" + 0.002*\"like\" + 0.002*\"said\" + 0.002*\"armenians\" + 0.001*\"god\" + 0.001*\"jpeg\"\n",
      "2019-11-24 17:24:12,528 : INFO : topic #4 (0.167): 0.002*\"image\" + 0.002*\"data\" + 0.001*\"graphics\" + 0.001*\"new\" + 0.001*\"edu\" + 0.001*\"software\" + 0.001*\"use\" + 0.001*\"like\" + 0.001*\"images\" + 0.001*\"available\"\n",
      "2019-11-24 17:24:12,541 : INFO : topic diff=0.105074, rho=0.292253\n"
     ]
    }
   ],
   "source": [
    "# Create lda model (equivalent to \"fit\" in sklearn)\n",
    "lda = models.LdaModel(corpus=corpus,  # we converted our data into a way gensim can understadn above\n",
    "                      num_topics=6,   # how many topics are there\n",
    "                      id2word=id2word, \n",
    "                      passes=10)   # telling gensim to run trough how many iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-24 17:24:12,621 : INFO : topic #0 (0.167): 0.002*\"space\" + 0.001*\"think\" + 0.001*\"don\" + 0.001*\"just\" + 0.001*\"people\" + 0.001*\"years\" + 0.001*\"like\" + 0.001*\"good\" + 0.001*\"time\" + 0.001*\"know\"\n",
      "2019-11-24 17:24:12,632 : INFO : topic #1 (0.167): 0.001*\"like\" + 0.001*\"year\" + 0.001*\"good\" + 0.001*\"just\" + 0.001*\"time\" + 0.001*\"edu\" + 0.001*\"think\" + 0.001*\"better\" + 0.001*\"years\" + 0.001*\"new\"\n",
      "2019-11-24 17:24:12,642 : INFO : topic #2 (0.167): 0.002*\"people\" + 0.002*\"don\" + 0.002*\"just\" + 0.002*\"israel\" + 0.002*\"think\" + 0.002*\"like\" + 0.001*\"said\" + 0.001*\"know\" + 0.001*\"time\" + 0.001*\"year\"\n",
      "2019-11-24 17:24:12,649 : INFO : topic #3 (0.167): 0.004*\"space\" + 0.002*\"launch\" + 0.001*\"satellite\" + 0.001*\"like\" + 0.001*\"just\" + 0.001*\"don\" + 0.001*\"new\" + 0.001*\"time\" + 0.001*\"think\" + 0.001*\"nasa\"\n",
      "2019-11-24 17:24:12,659 : INFO : topic #4 (0.167): 0.002*\"image\" + 0.002*\"data\" + 0.001*\"graphics\" + 0.001*\"new\" + 0.001*\"edu\" + 0.001*\"software\" + 0.001*\"use\" + 0.001*\"like\" + 0.001*\"images\" + 0.001*\"available\"\n",
      "2019-11-24 17:24:12,673 : INFO : topic #5 (0.167): 0.004*\"people\" + 0.002*\"don\" + 0.002*\"know\" + 0.002*\"armenian\" + 0.002*\"just\" + 0.002*\"like\" + 0.002*\"said\" + 0.002*\"armenians\" + 0.001*\"god\" + 0.001*\"jpeg\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.002*\"space\" + 0.001*\"think\" + 0.001*\"don\" + 0.001*\"just\" + 0.001*\"people\" + 0.001*\"years\" + 0.001*\"like\" + 0.001*\"good\" + 0.001*\"time\" + 0.001*\"know\"'),\n",
       " (1,\n",
       "  '0.001*\"like\" + 0.001*\"year\" + 0.001*\"good\" + 0.001*\"just\" + 0.001*\"time\" + 0.001*\"edu\" + 0.001*\"think\" + 0.001*\"better\" + 0.001*\"years\" + 0.001*\"new\"'),\n",
       " (2,\n",
       "  '0.002*\"people\" + 0.002*\"don\" + 0.002*\"just\" + 0.002*\"israel\" + 0.002*\"think\" + 0.002*\"like\" + 0.001*\"said\" + 0.001*\"know\" + 0.001*\"time\" + 0.001*\"year\"'),\n",
       " (3,\n",
       "  '0.004*\"space\" + 0.002*\"launch\" + 0.001*\"satellite\" + 0.001*\"like\" + 0.001*\"just\" + 0.001*\"don\" + 0.001*\"new\" + 0.001*\"time\" + 0.001*\"think\" + 0.001*\"nasa\"'),\n",
       " (4,\n",
       "  '0.002*\"image\" + 0.002*\"data\" + 0.001*\"graphics\" + 0.001*\"new\" + 0.001*\"edu\" + 0.001*\"software\" + 0.001*\"use\" + 0.001*\"like\" + 0.001*\"images\" + 0.001*\"available\"'),\n",
       " (5,\n",
       "  '0.004*\"people\" + 0.002*\"don\" + 0.002*\"know\" + 0.002*\"armenian\" + 0.002*\"just\" + 0.002*\"like\" + 0.002*\"said\" + 0.002*\"armenians\" + 0.001*\"god\" + 0.001*\"jpeg\"')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics()   # it will give you the topics\n",
    "                     # will return you the number of topics you specified above\n",
    "    \n",
    "    # HOWEVER, it is your job to infer what the topics are!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
