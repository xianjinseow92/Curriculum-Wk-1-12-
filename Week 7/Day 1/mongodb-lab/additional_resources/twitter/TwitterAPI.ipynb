{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter and Python\n",
    "\n",
    "We will be showing how to access Twitter using Python. We will use\n",
    "1. the relatively low-level `requests` library, which will deal with any API\n",
    "2. the easier to use `tweety` and `twitter` and  libraries, but which only works with Twitter\n",
    "\n",
    "We will need to install some packages. You should run\n",
    "```bash\n",
    "conda install -c conda-forga --file requirements.txt\n",
    "```\n",
    "or \n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The requests library\n",
    "\n",
    "We will be doing a lot of work with Twitter, but I don't want you to leave with the impresssion that requests only works with Twitter. It works with any API, and we have already used it with our Flask apps. A request is a \"low-level\" network call, and duplicates what the command line tool curl does, for example.\n",
    "\n",
    "Before diving into connecting to Twitter (where we will need to provide a username and password), I wanted to give an example using the [Star Wars API](http://swapi.co) which doesn't require authentication. \n",
    "\n",
    "Let's start with curl from the command line (the `json_pp` just makes the output JSON pretty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://swapi.co/api/people/1/ | json_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big picture idea:\n",
    "\n",
    "We pass an URL (or endpoint) to curl, and it returns data. Requests gives us a way of doing this in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://swapi.co/api/people/1/'\n",
    "response = requests.get(url)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass anything that is supported by the API. For example, did you know we can get Wookiee translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://swapi.co/api/people/1/?format=wookiee'\n",
    "requests.get(url).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last piece of SW trivia -- we can search!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where is Vader?\n",
    "url = 'https://swapi.co/api/people/?search=vader'\n",
    "requests.get(url).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic takeway\n",
    "- `requests.get(url)`: make a (GET) request to a URL. Can get a webpage or a JSON object back. Returns a `response` object\n",
    "- `response.json()`: access the JSON object returned (if there was one)\n",
    "\n",
    "To get this to work with Twitter, we will need to authenticate ourselves. This is the job of OAuth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions to connect to Twitter using requests\n",
    "\n",
    "We need to identify ourselves to Twitter using \n",
    "- a public and private key, which doesn't expire\n",
    "- as well as a public and private token (which does expire). \n",
    "If you are re-running this notebook tomorrow, you will need to get a token from the Twitter page (but your keys will remain the same).\n",
    "\n",
    "Follow the instructions [here](setup_twitter_instructions.md) to get your keys and tokens, and place them in `twitter_credenitials.py`.\n",
    "\n",
    "**The cell below won't work until you follow the instructions!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed to authenticate us to Twitter\n",
    "\n",
    "try:\n",
    "    from requests_oauthlib import OAuth1\n",
    "except ModuleNotFoundError:\n",
    "    import sys\n",
    "    import os\n",
    "\n",
    "    # I need this because requests_oauth gets installed in a weird place on my system\n",
    "    sys.path.append('/usr/local/lib/python3.6/site-packages')\n",
    "    from requests_oauthlib import OAuth1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load Twitter credentials from the .env file (this is the trick we saw earlier -- our .env file is in .gitignore, so this prevents our auth keys going on GitHub).\n",
    "\n",
    "You should have it already installed from earlier, but if you don't you can install it with \n",
    "```\n",
    "pip install -U python-dotenv\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "import os\n",
    "\n",
    "oauth = OAuth1(os.getenv(\"TWITTER_CONSUMER_KEY\"),\n",
    "               os.getenv(\"TWITTER_CONSUMER_KEY_SECRET\"),\n",
    "               os.getenv(\"TWITTER_ACCESS_TOKEN\"),\n",
    "               os.getenv(\"TWITTER_ACCESS_TOKEN_SECRET\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This \"user_timeline\" url tells the API to get the tweets of the user associated with the app.\n",
    "response = requests.get(\"https://api.twitter.com/1.1/statuses/user_timeline.json\",\n",
    "                        auth=oauth)\n",
    "\n",
    "# Note that we're using the \"requests\" library\n",
    "tweets = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter search API (free version scrapes last week's tweets)\n",
    "\n",
    "A detailed description of the twitter search API can be found [here](https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets.html). Some of the key parameters\n",
    "\n",
    "| Parameter | Notes | Example |\n",
    "|---|---|---|\n",
    "| q | (required) query string to search for | `@metis` |\n",
    "| geocode | (optional) Uses tweet geolocation, or user's profile location if tweet geolocation disabled. Should be of the format `latitude longitude radius[unit]` where unit is either \"km\" or \"mi\" | `41.8781, -87.6298, 5mi` |\n",
    "| lang | (optional) Only return tweets in language given. Languages are coded by the two character code used in [ISO 639-1](http://en.wikipedia.org/wiki/List_of_ISO_639-1_codes). | `en` `cn` |\n",
    "| count | (optional) Number of results to return. Defaults to 15, max value is 100 | `20` |\n",
    "\n",
    "The API returns a JSON object with two keys:\n",
    "- search_metadata: Information about how long the search took, what was searched for, etc\n",
    "- statuses: the actual queries that you wanted\n",
    "\n",
    "Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"q\": \"wait wait don't tell me\", \"count\":20, \n",
    "              \"geocode\": \"41.8781,-87.6298,100mi\",\n",
    "             \"lang\":\"en\"}\n",
    "\n",
    "response = requests.get(\"https://api.twitter.com/1.1/search/tweets.json\",\n",
    "                        params = parameters,\n",
    "                        auth=oauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just look at the first tweet:\n",
    "response.json()['statuses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, can we extract some of the info from this text?\n",
    "tweets = response.json()['statuses']\n",
    "\n",
    "def tweet_to_string(tweet):\n",
    "    s = \"\"\"\n",
    "        Text: {text}\n",
    "        Hashtags: {hashtags}\n",
    "        Username: {screenname} -- ({description})\n",
    "        Social status: {friends} friends, {followers} followers\n",
    "        Location: {location}\n",
    "    \"\"\".format(text=tweet['text'], hashtags=tweet['entities']['hashtags'],\n",
    "               screenname=tweet['user']['screen_name'], \n",
    "               description=tweet['user']['description'],\n",
    "               friends=tweet['user']['friends_count'],\n",
    "               followers=tweet['user']['followers_count'],\n",
    "               location=tweet['user']['location'])\n",
    "    return s\n",
    "\n",
    "print(tweet_to_string(tweets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweets[:5]:\n",
    "    print(tweet_to_string(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did we pull all 20 tweets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of tweets = \", len(tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pull the next set of tweets if we want (i.e. the \"next\" 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_page_url = \"https://api.twitter.com/1.1/search/tweets.json\" + response.json()['search_metadata']['next_results']\n",
    "\n",
    "response = requests.get(next_page_url, auth=oauth)\n",
    "\n",
    "more_tweets = response.json()['statuses']\n",
    "\n",
    "for tweet in more_tweets[:4]:\n",
    "    print(tweet['text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Twitter package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter\n",
    "import pandas as pd\n",
    "import time\n",
    "consumer_key = os.getenv(\"TWITTER_CONSUMER_KEY\")\n",
    "consumer_secret = os.getenv(\"TWITTER_CONSUMER_KEY_SECRET\")\n",
    "access_token_key = os.getenv(\"TWITTER_ACCESS_TOKEN\")\n",
    "access_token_secret = os.getenv(\"TWITTER_ACCESS_TOKEN_SECRET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = twitter.Api(consumer_key=consumer_key,\n",
    "                  consumer_secret=consumer_secret,\n",
    "                  access_token_key=access_token_key,\n",
    "                  access_token_secret=access_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.PostUpdate(\"Hello from SF!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = api.GetSearch(raw_query=\"l=&q=%23machinelearning%2C%20OR%20%23ml%2C%20OR%20%23AI&\", count=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = api.GetSearch(raw_query='l=&q=\"Deep%20learning\"%20\"machine%20learning\"%20\"AI\"%20%23machinelearning%20OR%20%23deeplearning%20OR%20%23DL%20OR%20%23AI%20OR%20%23ML', count=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweets:\n",
    "    print(tweet)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweets:\n",
    "    \n",
    "    for key, val in tweet.AsDict().items():\n",
    "        print(f'{key}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet.AsDict()[\"retweeted_status\"]['user']['favourites_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.VerifyCredentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = api.GetFriends()\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweets_to_df(tweets):\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    df['tweet_created_at'] = [tweet.AsDict()['created_at'] for tweet in tweets]\n",
    "    df['hashtags']= [[inner_dict['text'] for inner_dict in tweet.AsDict()['hashtags']] for tweet in tweets]\n",
    "    df['id'] = [tweet.AsDict()['id'] for tweet in tweets]\n",
    "    df['lang'] = [tweet.AsDict()['lang'] for tweet in tweets]\n",
    "    df['user_screen_name'] = [tweet.AsDict()['user']['screen_name'] for tweet in tweets]\n",
    "    df['user_created_at'] = [tweet.AsDict()['user']['created_at'] for tweet in tweets]\n",
    "    df['user_description'] = [tweet.AsDict()['user']['description'] for tweet in tweets]\n",
    "    df['user_statuses_count'] = [tweet.AsDict()['user']['statuses_count'] for tweet in tweets]\n",
    "    df['tweet_text'] = [tweet.AsDict()['text'].encode('utf-8') for tweet in tweets]\n",
    "    df['user_friends_count'] = [tweet.AsDict()['user']['friends_count'] for tweet in tweets]\n",
    "    df['retweet_count'] = [tweet.AsDict()['retweet_count'] if 'retweet_count' in tweet.AsDict().keys() \\\n",
    "                           else 0 for tweet in tweets]\n",
    "    df['favorites_count'] = [tweet.AsDict()[\"retweeted_status\"]['user']['favourites_count'] \\\n",
    "                             if 'retweeted_status' in tweet.AsDict().keys() else 0 for tweet in tweets]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_to_df(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.GetStatus(1027454336429895680)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.GetStatuses([1027440068980613120, 1027439980740669440])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming tweets\n",
    "\n",
    "Instead of looking at the tweets that have already been made, we can look at the tweets in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key=consumer_key,\n",
    "                           consumer_secret=consumer_secret)\n",
    "auth.set_access_token(access_token_key,\n",
    "                      access_token_secret)\n",
    "\n",
    "api=tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tweets = 3\n",
    "\n",
    "for index, tweet in enumerate(tweepy.Cursor(api.search ,q=\"machinelearning\").items(max_tweets)):\n",
    "    # You can see all the methods available on tweet using .<tab> or \n",
    "    # dir(tweet). You can access the raw JSON using tweet._json\n",
    "    print(str(index) + '. ' + tweet.text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also duplicate our original query\n",
    "for index, tweet in enumerate(tweepy.Cursor(api.search, **parameters).items(max_tweets)):\n",
    "    print(str(index) + '. ' + tweet.text + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting tweets into dataframes\n",
    "\n",
    "The demo below shows how to do this for the tweepy API using cursors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this creates the iterable cursor -- it doesn't pull the tweets yet!\n",
    "cursor = tweepy.Cursor(api.search, **parameters).items(max_tweets)\n",
    "\n",
    "# This command, when we pull from the cursor, actually starts going to twitter and returns\n",
    "# a list of the \"json\" objects (Python dictionaries)\n",
    "the_tweets = [tweet._json for tweet in cursor]\n",
    "\n",
    "# which we then can convert to a dataframe:\n",
    "tweet_frame = pd.DataFrame(the_tweets)\n",
    "tweet_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also do this with the response method we looked at earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = [t for t in tweepy.Cursor(api.search, q='congress').items(4)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = cursor[0]\n",
    "\n",
    "c.entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data into Mongo\n",
    "\n",
    "We can also insert tweets into MongoDB. We will use the cursor method, but you could also use the requests or streaming method (defined in the next section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# This connects us to the \"legistlation\" database, and the \"collection\" [think table] news \n",
    "# in that database\n",
    "client = MongoClient()\n",
    "db = client.legislation\n",
    "news_collection = db.news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTweet(tweet):\n",
    "    tweet_dict = {\n",
    "        'datetime': tweet.created_at,\n",
    "        'tweet': tweet.text,\n",
    "        'entities': tweet.entities\n",
    "    }\n",
    "    \n",
    "    if tweet.coordinates:\n",
    "        tweet_dict['coordinates'] = tweet.coordinates\n",
    "    if tweet.geo:\n",
    "        tweet_dict['geo'] = tweet.geo\n",
    "    \n",
    "    return tweet_dict\n",
    "\n",
    "cursor = tweepy.Cursor(api.search, q='congress').items(400)\n",
    "\n",
    "# get a list of dictionarier\n",
    "tweets = [ processTweet(tweet) for tweet in cursor ]\n",
    "\n",
    "# insert them into Mongo\n",
    "db.news.insert_many(tweets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the PyMongo client to get some information back from the database!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many documents do we have\n",
    "news_collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many mention Trump?\n",
    "news_collection.find({'tweet': {'$regex': 'Trump'}}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More complicated query: 20 most popular hashtags\n",
    "\n",
    "What are the most popular hashtags in the dataset? \n",
    "    \n",
    "To start, let's find a document with at least one hashtag. Then we will build a pipeline using `aggregate`, which goes through a series of filtering sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_collection.find_one({'entities.hashtags.0': {'$exists': 1}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregation steps used below. Note that `$` signs get used for operators or for existing column names (to distinguish them from normal strings):\n",
    "- `$match`: The standard query used in `find` that we have already seen\n",
    "- `$project`: allows us to rename fields, or only include those fields that you want\n",
    "- `$unwind`: Take a field that contains an array, and create a new record for each element in that array (see example)\n",
    "- `$group`: This is like a SQL `GROUP BY`. Take a mandatory `_id` (which is what it groups by). Create new fields with aggegate function\n",
    "- `$sort: { sort_field : +1 ascending or -1 descending }`\n",
    "- `$limit`: the number of records to return. This can also be called on the resulting cursor.\n",
    "\n",
    "\n",
    "#### Unwind example\n",
    "\n",
    "If we have a document\n",
    "```javascript\n",
    "{\n",
    "   '_id': 123456789,\n",
    "   'field1': [1,2,3],\n",
    "   'field2': [5,6,7],\n",
    "   'field3': 'abba'\n",
    "}\n",
    "```\n",
    "after doing an `$unwind` on field 2 you would get three new documents:\n",
    "```javascript\n",
    "{\n",
    "   '_id': 123456789,\n",
    "   'field1': [1,2,3],\n",
    "   'field2': 5,\n",
    "   'field3': 'abba'\n",
    "},\n",
    "{\n",
    "   '_id': 123456789,\n",
    "   'field1': [1,2,3],\n",
    "   'field2': 6,\n",
    "   'field3': 'abba'\n",
    "},\n",
    "{\n",
    "   '_id': 123456789,\n",
    "   'field1': [1,2,3],\n",
    "   'field2': 7,\n",
    "   'field3': 'abba'\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate is a pipeline, order matters\n",
    "cursor = news_collection.aggregate([\n",
    "    {'$match': {'entities.hashtags.0': {'$exists': 1}}},\n",
    "    {'$project': {'_id': 0, 'hashtags': '$entities.hashtags'}},\n",
    "    {'$unwind': '$hashtags'},\n",
    "    {'$group': {'_id': '$hashtags.text', 'count': {'$sum': 1}}},\n",
    "    {'$sort': {'count': -1}},\n",
    "    {'$limit': 20}\n",
    "])\n",
    "\n",
    "list(cursor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real streaming: the tweepy API\n",
    "\n",
    "Okay, but how about real streaming? That is, an object that sits there are \"listens\" for new tweets, and then processes them as they arrive?\n",
    "\n",
    "This uses a slightly different API. There are a couple of things to pay attention to\n",
    "- The tweets that we get are *strings* of JSON objects, not the JSON objects themselves. We also don't have the nice ways of accessing the attributes directly (e.g. tweet.text above). Instead we convert the string to JSON, which gives us a dictionary, and then go from there.\n",
    "- A twitter stream takes a `StreamListener` class. We should write member functions `on_data` and `on_error` that are called when a new tweet arrives, or we encounter an error, respectively.\n",
    "\n",
    "In this example, we implement a `deque` of length 5, so that we are retaining the 5 most recent tweets. In the `on_data` call, we are adding the tweet to our collection, then printing out the currently stored tweets. \n",
    "\n",
    "If we wanted to store the data for all time, then `on_data` method would be where we would load them into Mongo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "from IPython import display\n",
    "from collections import deque\n",
    "import json\n",
    "\n",
    "class MyListener(StreamListener):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.list_of_tweets = deque([], maxlen=5)\n",
    "        \n",
    "    def on_data(self, data):\n",
    "        tweet_text = json.loads(data)['text']\n",
    "        self.list_of_tweets.append(tweet_text)\n",
    "        self.print_list_of_tweets()\n",
    "        \n",
    "    \n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "\n",
    "    def print_list_of_tweets(self):\n",
    "        display.clear_output(wait=True)\n",
    "        for index, tweet_text in enumerate(self.list_of_tweets):\n",
    "            m='{}. {}\\n\\n'.format(index, tweet_text)\n",
    "            print(m)\n",
    "            \n",
    "twitter_stream = Stream(auth, MyListener())\n",
    "twitter_stream.filter(track=['#SpaceForce'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metis",
   "language": "python",
   "name": "metis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
