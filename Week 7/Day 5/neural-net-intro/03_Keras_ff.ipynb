{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T13:13:34.444760Z",
     "start_time": "2019-05-20T13:13:31.927263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# standard code block #\n",
    "#######################\n",
    "\n",
    "# see https://ipython.readthedocs.io/en/stable/interactive/magics.html\n",
    "%pylab inline\n",
    "\n",
    "# sets backend to render higher res images\n",
    "%config InlineBackend.figure_formats = ['retina']\n",
    "\n",
    "#######################\n",
    "#       imports       #\n",
    "#######################\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "# import sklearn\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T13:13:38.277389Z",
     "start_time": "2019-05-20T13:13:35.024031Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for tabular data\n",
    "\n",
    "## Summary of Deep Learning Methods\n",
    "\n",
    "Today we will cover the most fundamental application of deep learning:\n",
    "- **Feedforward** Neural Networks maximize flexibility. They are appropriate in cases where we shouldn't make assumptions about relationships between our input features. They do especially well on tabular data, like the dataframes we've seen so far. \n",
    "\n",
    "Other important techniques of deep learning:\n",
    "- **Embeddings** are a technique for learning efficient relationships between categories.\n",
    "- **Convolutional** Neural Networks are use to model spatial relationships. They are particularly useful in image and audio tasks but have many more applications.\n",
    "- **Recurrent** Neural Networks are used to model sequences of data, like sentences or time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose of this notebook\n",
    "\n",
    "- Demonstrate best practices for deep learning on tabular data.\n",
    "- Discuss common techniques for applying and improving deep learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We will load in a the Ames Housing Data, split into train and test sets, and build some models. (www.amstat.org/publications/jse/v19n3/decock.pdf). \n",
    "\n",
    "In the following cells I will perform the basic data cleaning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order</th>\n",
       "      <th>PID</th>\n",
       "      <th>MS SubClass</th>\n",
       "      <th>MS Zoning</th>\n",
       "      <th>Lot Frontage</th>\n",
       "      <th>Lot Area</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>Lot Shape</th>\n",
       "      <th>Land Contour</th>\n",
       "      <th>...</th>\n",
       "      <th>Pool Area</th>\n",
       "      <th>Pool QC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>Misc Feature</th>\n",
       "      <th>Misc Val</th>\n",
       "      <th>Mo Sold</th>\n",
       "      <th>Yr Sold</th>\n",
       "      <th>Sale Type</th>\n",
       "      <th>Sale Condition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>526301100</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>141.0</td>\n",
       "      <td>31770</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>215000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>526350040</td>\n",
       "      <td>20</td>\n",
       "      <td>RH</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11622</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>105000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>526351010</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>81.0</td>\n",
       "      <td>14267</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gar2</td>\n",
       "      <td>12500</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>172000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>526353030</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>93.0</td>\n",
       "      <td>11160</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>244000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>527105010</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13830</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>189900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Order        PID  MS SubClass MS Zoning  Lot Frontage  Lot Area Street  \\\n",
       "0      1  526301100           20        RL         141.0     31770   Pave   \n",
       "1      2  526350040           20        RH          80.0     11622   Pave   \n",
       "2      3  526351010           20        RL          81.0     14267   Pave   \n",
       "3      4  526353030           20        RL          93.0     11160   Pave   \n",
       "4      5  527105010           60        RL          74.0     13830   Pave   \n",
       "\n",
       "  Alley Lot Shape Land Contour  ... Pool Area Pool QC  Fence Misc Feature  \\\n",
       "0   NaN       IR1          Lvl  ...         0     NaN    NaN          NaN   \n",
       "1   NaN       Reg          Lvl  ...         0     NaN  MnPrv          NaN   \n",
       "2   NaN       IR1          Lvl  ...         0     NaN    NaN         Gar2   \n",
       "3   NaN       Reg          Lvl  ...         0     NaN    NaN          NaN   \n",
       "4   NaN       IR1          Lvl  ...         0     NaN  MnPrv          NaN   \n",
       "\n",
       "  Misc Val Mo Sold Yr Sold Sale Type  Sale Condition  SalePrice  \n",
       "0        0       5    2010       WD           Normal     215000  \n",
       "1        0       6    2010       WD           Normal     105000  \n",
       "2    12500       6    2010       WD           Normal     172000  \n",
       "3        0       4    2010       WD           Normal     244000  \n",
       "4        0       3    2010       WD           Normal     189900  \n",
       "\n",
       "[5 rows x 82 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ames_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T13:16:04.231954Z",
     "start_time": "2019-05-20T13:16:03.593264Z"
    }
   },
   "outputs": [],
   "source": [
    "# I'm going to load in the data and take care of the data cleaning here.\n",
    "\n",
    "ames_df=pd.read_csv(\"http://www.amstat.org/publications/jse/v19n3/decock/AmesHousing.txt\", sep='\\t')\n",
    "\n",
    "\n",
    "\n",
    "y_orig = ames_df[\"SalePrice\"]\n",
    "x_orig = ames_df.drop(columns=\"SalePrice\")\n",
    "\n",
    "ames_dummies = pd.get_dummies(x_orig, dummy_na=True)\n",
    "ames_dummies = ames_dummies.fillna(0)\n",
    "\n",
    "x_orig = x_orig.select_dtypes('number')\n",
    "x_orig = x_orig.fillna(0)\n",
    "\n",
    "\n",
    "x_orig = x_orig.merge(ames_dummies, left_index=True, right_index=True)\n",
    "x_orig = x_orig.astype(float)\n",
    "\n",
    "# At the end of the day we are looking for the sales price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>**Important**: we previously said that neural nets require minimal pre-processing. One of those processing steps is scaling our input data. It's recommended that you scale (normalize) all input data when using deep learning/neural nets.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T13:16:08.607048Z",
     "start_time": "2019-05-20T13:16:08.515624Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing, model_selection\n",
    "\n",
    "# Split into train/test\n",
    "x, x_test, y, y_test = model_selection.train_test_split(x_orig,y_orig)\n",
    "\n",
    "\n",
    "# Scale our data \n",
    "\n",
    "# Why do we need to scale our data?\n",
    "# Either scale to center\n",
    "# Or scale from 0 to 1\n",
    "\n",
    "# If you scale it down, it's easier for your computer to generalize your data\n",
    "scaler = preprocessing.StandardScaler()\n",
    "x = scaler.fit_transform(x)\n",
    "x_test = scaler.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make a test set on which we can compare our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T13:16:13.625790Z",
     "start_time": "2019-05-20T13:16:13.617873Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def benchmark(model):\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    print(f\"mae: {metrics.mean_absolute_error(y_test, y_pred):,.2f}\")  \n",
    "    # We are goin going to use MAE / MSE to quantify our results since this is a regression problem\n",
    "    print(f\"mse: {metrics.mean_squared_error(y_test, y_pred):,.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model\n",
    "\n",
    "When we care about performance, random forests are a great off-the-shelf model for tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T13:16:22.114317Z",
     "start_time": "2019-05-20T13:16:14.941935Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "                      max_features='auto', max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                      n_jobs=None, oob_score=False, random_state=None,\n",
       "                      verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "\n",
    "rf = ensemble.RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "rf.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T13:16:23.794041Z",
     "start_time": "2019-05-20T13:16:23.748790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae: 16,216.61\n",
      "mse: 640,458,231.53\n"
     ]
    }
   ],
   "source": [
    "benchmark(rf)\n",
    "\n",
    "# We don't know if this is good because we don't have another model to compare with\n",
    "# But now we have RF as our baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use this RF as our baseline. Let's train a plain neural net for to do the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple neural net\n",
    "\n",
    "\n",
    "Let's build a two-layer network just as we did in the Neural Net Theory notebook. The difference here is that we will not use an activation function on the output.\n",
    "\n",
    "\n",
    "<mark>For regression problems, you will typically use a linear (no) activation function on your final layer.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T13:16:28.468015Z",
     "start_time": "2019-05-20T13:16:28.272703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\xianj\\Anaconda3\\envs\\metis\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 5)                 1940      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 1,946\n",
      "Trainable params: 1,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Training our Neural Net\n",
    "\n",
    "ff_model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=x.shape[1:]),\n",
    "    keras.layers.Dense(units=5, activation=\"relu\"),\n",
    "    keras.layers.Dense(units=1),\n",
    "])\n",
    "ff_model.compile(\"sgd\", loss=\"mean_absolute_error\", metrics=[\"mean_squared_error\"])\n",
    "ff_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T13:16:39.748086Z",
     "start_time": "2019-05-20T13:16:36.682371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2197/2197 [==============================] - 0s 46us/step - loss: 17342.0272 - mean_squared_error: 734146465.5294\n",
      "Epoch 2/20\n",
      "2197/2197 [==============================] - 0s 46us/step - loss: 17016.9249 - mean_squared_error: 708021141.4401\n",
      "Epoch 3/20\n",
      "2197/2197 [==============================] - 0s 49us/step - loss: 16761.0676 - mean_squared_error: 697887697.8425\n",
      "Epoch 4/20\n",
      "2197/2197 [==============================] - 0s 50us/step - loss: 16715.5155 - mean_squared_error: 671869299.3136\n",
      "Epoch 5/20\n",
      "2197/2197 [==============================] - 0s 49us/step - loss: 16579.6066 - mean_squared_error: 681734317.0214\n",
      "Epoch 6/20\n",
      "2197/2197 [==============================] - 0s 45us/step - loss: 16358.2452 - mean_squared_error: 669981548.1620\n",
      "Epoch 7/20\n",
      "2197/2197 [==============================] - 0s 47us/step - loss: 16176.0899 - mean_squared_error: 636043577.6787\n",
      "Epoch 8/20\n",
      "2197/2197 [==============================] - 0s 46us/step - loss: 16086.7675 - mean_squared_error: 638534279.6905\n",
      "Epoch 9/20\n",
      "2197/2197 [==============================] - 0s 47us/step - loss: 15784.2329 - mean_squared_error: 623309027.5394\n",
      "Epoch 10/20\n",
      "2197/2197 [==============================] - 0s 47us/step - loss: 15705.9578 - mean_squared_error: 620304391.5740\n",
      "Epoch 11/20\n",
      "2197/2197 [==============================] - 0s 48us/step - loss: 15741.3915 - mean_squared_error: 623704716.2349\n",
      "Epoch 12/20\n",
      "2197/2197 [==============================] - 0s 44us/step - loss: 15518.4457 - mean_squared_error: 600261521.9372\n",
      "Epoch 13/20\n",
      "2197/2197 [==============================] - 0s 83us/step - loss: 15428.8454 - mean_squared_error: 596284220.6208\n",
      "Epoch 14/20\n",
      "2197/2197 [==============================] - 0s 66us/step - loss: 15274.1182 - mean_squared_error: 600944328.9431\n",
      "Epoch 15/20\n",
      "2197/2197 [==============================] - 0s 65us/step - loss: 15218.3546 - mean_squared_error: 575351248.5025\n",
      "Epoch 16/20\n",
      "2197/2197 [==============================] - 0s 55us/step - loss: 15153.5147 - mean_squared_error: 582105782.5398\n",
      "Epoch 17/20\n",
      "2197/2197 [==============================] - 0s 63us/step - loss: 15056.7160 - mean_squared_error: 576012945.1579\n",
      "Epoch 18/20\n",
      "2197/2197 [==============================] - 0s 65us/step - loss: 14929.0365 - mean_squared_error: 571134458.6545\n",
      "Epoch 19/20\n",
      "2197/2197 [==============================] - 0s 64us/step - loss: 14812.2995 - mean_squared_error: 569102146.8257\n",
      "Epoch 20/20\n",
      "2197/2197 [==============================] - 0s 79us/step - loss: 14685.9977 - mean_squared_error: 565295054.3905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x155c820dc88>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff_model.fit(x, y, epochs=20)\n",
    "\n",
    "# epoch is a hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:09:44.694350Z",
     "start_time": "2019-05-09T21:09:44.634584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae: 23,258.98\n",
      "mse: 1,062,695,263.32\n"
     ]
    }
   ],
   "source": [
    "benchmark(ff_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.dropbox.com/s/46x057it18kuhh3/2019-03-01_09-16-00.png?raw=1)\n",
    "Here's a look at some new things used in the above cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f59f374922c498bf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Check for understanding\n",
    "\n",
    "**Question**: the output layer of the model above has 6 trainable parameters. Where does that number come from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-dfc330ac454af5e7",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Each dense layer includes the linear transformation and possibly an activation function\n",
    "\n",
    "$$f_a(\\mathbf{w}^\\top\\mathbf{x} + \\mathbf{b})$$\n",
    "\n",
    "We know that the input to the second layer has a dimensionality of $[5 \\times n]$ where $n$ is the number of examples in our minibatch. We also know that the output of the second layer is $[1 \\times n]$. These both come from setting `units=5` in the first layer and `units=1` in the second.\n",
    "\n",
    "what do $\\mathbf{w}$ and $\\mathbf{b}$ need to look like in order to scale the units from 5 to 1? $\\mathbf{w}$ will need to be $[5 \\times 1]$ and $\\mathbf{b}$ will need to be $[1 \\times 1]$. That leaves 5 trainable parameters in $\\mathbf{w}$ and just one in $\\mathbf{b}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful tools 🛠\n",
    "\n",
    "Okay, so that's our basic, vanilla neural network. It's a good starting point. Let's look at a few additions that might help us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "```python\n",
    "keras.layers.Dropout(0.05)\n",
    "```\n",
    "\n",
    "Dropout is a layer that randomly selects a percentage of the weights and sets them to zero. The percentage above is 5%. Dropout only applies in the training phase of the model and is turned of when we use `model.predict(...)`.\n",
    "\n",
    "Dropout is part of a broad view of regularization that has become more common with Deep Learning techniques. Before you might have defined regularization as something like \"adding weights to your loss function\". From now on, we'll use regularization to refer to any technique that penalizes training performance in order to improve test performance.\n",
    "\n",
    "\n",
    "> <mark>\"Regularization is any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error.\"</mark> [Deep Learning](https://www.deeplearningbook.org/) by Goodfellow, Bengio and Courville"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight regularization\n",
    "\n",
    "```python\n",
    "keras.layers.Dense(units=300, activation=\"relu\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "```\n",
    "\n",
    "You can still use regularization on the weights of your neural network with the `kernel_regularizer=` parameter. This works just like in ridge/lasso regression that you learned about before. Regularization can also be applied to the bias (less common) and the activation for each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced optimizers\n",
    "\n",
    "```python\n",
    "test_model.compile(\n",
    "    keras.optimizers.adam(lr=0.001), loss=\"mean_absolute_error\",\n",
    "    metrics=[\"mean_squared_error\"])\n",
    "```\n",
    "\n",
    "We previously used the default optimizer (\"sgd\" for stochastic gradient descent) but we have a lot of options. You can use any of the optimizers from the keras package and set their parameters yourself. \n",
    "\n",
    "[See the list of keras optimizers.](https://keras.io/optimizers/)\n",
    "\n",
    "**Setting the learning rate**: If you set `lr` too high, the model won't learn because the . Too low and it won't learn because it isn't moving far enough. The default values are often good places to start. <mark>Tip: it often works best to set the learning rate to the highest rate where the model still improves with each epoch.</mark>\n",
    "\n",
    "**Good default**: a good default is to start with `keras.optimizers.adam`. This is a strong-performing advanced optimizer that incorporates momentum and adapts the learning rate for each parameter.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CALL BACKS ARE REALLY IMPORTANT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "\n",
    "Callbacks are helpful functions that are run after an epoch or batch. There are callbacks in keras and they are very, very helpful. \n",
    "\n",
    "[See the list of keras callbacks.](https://keras.io/callbacks/)\n",
    "\n",
    "```python\n",
    "test_model.fit(\n",
    "    x, y, epochs=100, batch_size=100, validation_split=.25, verbose=1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=8, verbose=1, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=.5, patience=3, verbose=1),\n",
    "    ])\n",
    "```\n",
    "\n",
    "### `ReduceLROnPlateau`\n",
    "\n",
    "![](https://www.dropbox.com/s/bplh2nbnyaus682/2019-03-01_10-26-06.png?raw=1)\n",
    "\n",
    "`ReduceLROnPlateau` reduces the learning rate after the model stops improving. You set how aggressively this happens with the `patience` parameter.\n",
    " \n",
    "Why would you want to do this? The learning rate balances how quickly the model descends the loss gradient vs how precisely it does so. A high learning rate moves more quickly but cannot precisely find minima.  \n",
    "  \n",
    "**A low learning rate can carefully fall into a minima but it does so very slowly. In practice, modelers have found that using learning rate schedules is a helpful way to allow your model to quickly find minima at the beginning of training and then later more precisely target those minima to improve performance. **\n",
    "  \n",
    "  In the figure here (reproduced from [Clevert, Unterthiner & Hochreiter](https://arxiv.org/abs/1511.07289)) you can see a common pattern where learning plateaus at one learning rate and then accelerates again as soon as the learning rate is lowered.\n",
    "\n",
    "`ReduceLROnPlateau` is convenient because it allows you to perform this automatically without designing a learning rate schedule by hand. \n",
    "\n",
    "### `EarlyStopping`\n",
    "\n",
    "How many epochs should you tell your model to use? There's no good answer to this question but **thankfully we can use `EarlyStopping` to tell the model to stop whenever performance no longer improving**. <mark>Set `patience` here to be higher than `ReduceLROnPlateau` if you're using both.</mark> With `restore_best_weights=True` the model will restore the weights of the best epoch after it's finished. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:09:45.045660Z",
     "start_time": "2019-05-09T21:09:44.700244Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\xianj\\Anaconda3\\envs\\metis\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 300)               116400    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 200)               60200     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 201,801\n",
      "Trainable params: 201,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "test_model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=x.shape[1:]),\n",
    "    keras.layers.Dense(units=300, activation=\"relu\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "    keras.layers.Dropout(0.01),\n",
    "    keras.layers.Dense(units=200, activation=\"relu\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "    keras.layers.Dense(units=100, activation=\"relu\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "    keras.layers.Dense(units=50, activation=\"relu\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "    keras.layers.Dense(units=1),\n",
    "])\n",
    "test_model.compile(\n",
    "    keras.optimizers.adam(lr=0.001), loss=\"mean_absolute_error\",\n",
    "    metrics=[\"mean_squared_error\"])\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:10:07.239607Z",
     "start_time": "2019-05-09T21:09:45.049598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1647 samples, validate on 550 samples\n",
      "Epoch 1/100\n",
      "1647/1647 [==============================] - 1s 499us/step - loss: 178884.4267 - mean_squared_error: 38266642146.0012 - val_loss: 175885.7863 - val_mean_squared_error: 37052687885.0327\n",
      "Epoch 2/100\n",
      "1647/1647 [==============================] - 0s 168us/step - loss: 117881.3534 - mean_squared_error: 20590417441.8069 - val_loss: 50847.4887 - val_mean_squared_error: 4259555686.4000\n",
      "Epoch 3/100\n",
      "1647/1647 [==============================] - 0s 167us/step - loss: 33903.5526 - mean_squared_error: 2472410760.7043 - val_loss: 30372.3182 - val_mean_squared_error: 1941148663.2727\n",
      "Epoch 4/100\n",
      "1647/1647 [==============================] - 0s 170us/step - loss: 24517.9533 - mean_squared_error: 1475636140.6485 - val_loss: 26057.1353 - val_mean_squared_error: 1596916023.3891\n",
      "Epoch 5/100\n",
      "1647/1647 [==============================] - 0s 160us/step - loss: 20716.4717 - mean_squared_error: 1138464480.6023 - val_loss: 23898.7530 - val_mean_squared_error: 1406352850.6182\n",
      "Epoch 6/100\n",
      "1647/1647 [==============================] - 0s 167us/step - loss: 18377.7945 - mean_squared_error: 938021817.1609 - val_loss: 22466.2713 - val_mean_squared_error: 1267653625.3091\n",
      "Epoch 7/100\n",
      "1647/1647 [==============================] - 0s 185us/step - loss: 16525.6136 - mean_squared_error: 790517054.3291 - val_loss: 21406.3867 - val_mean_squared_error: 1206078188.3636\n",
      "Epoch 8/100\n",
      "1647/1647 [==============================] - 0s 179us/step - loss: 14984.1867 - mean_squared_error: 677358990.3777 - val_loss: 21031.5597 - val_mean_squared_error: 1162197957.7018\n",
      "Epoch 9/100\n",
      "1647/1647 [==============================] - 0s 171us/step - loss: 14143.3687 - mean_squared_error: 614294913.4766 - val_loss: 20572.8704 - val_mean_squared_error: 1124178021.9636\n",
      "Epoch 10/100\n",
      "1647/1647 [==============================] - 0s 159us/step - loss: 13114.3713 - mean_squared_error: 546926761.4815 - val_loss: 20147.3457 - val_mean_squared_error: 1096291542.8364\n",
      "Epoch 11/100\n",
      "1647/1647 [==============================] - 0s 168us/step - loss: 12542.4450 - mean_squared_error: 489102992.7675 - val_loss: 20053.3026 - val_mean_squared_error: 1106816527.4182\n",
      "Epoch 12/100\n",
      "1647/1647 [==============================] - 0s 164us/step - loss: 11823.7153 - mean_squared_error: 458297740.0073 - val_loss: 19631.2112 - val_mean_squared_error: 1084662334.8655\n",
      "Epoch 13/100\n",
      "1647/1647 [==============================] - 0s 163us/step - loss: 11177.9646 - mean_squared_error: 427624492.4930 - val_loss: 19420.7994 - val_mean_squared_error: 1064930864.7564\n",
      "Epoch 14/100\n",
      "1647/1647 [==============================] - 0s 170us/step - loss: 10804.8945 - mean_squared_error: 405261905.9526 - val_loss: 19739.3778 - val_mean_squared_error: 1073534613.8909\n",
      "Epoch 15/100\n",
      "1647/1647 [==============================] - 0s 165us/step - loss: 10308.3148 - mean_squared_error: 373108066.7007 - val_loss: 19215.7570 - val_mean_squared_error: 1034770178.3564\n",
      "Epoch 16/100\n",
      "1647/1647 [==============================] - 0s 179us/step - loss: 9925.5398 - mean_squared_error: 356797659.2787 - val_loss: 19415.3668 - val_mean_squared_error: 1082208860.3345\n",
      "Epoch 17/100\n",
      "1647/1647 [==============================] - 0s 180us/step - loss: 9622.6886 - mean_squared_error: 330113733.6539 - val_loss: 18828.0288 - val_mean_squared_error: 1027251247.7964\n",
      "Epoch 18/100\n",
      "1647/1647 [==============================] - 0s 168us/step - loss: 9272.3464 - mean_squared_error: 322301604.2647 - val_loss: 18812.6517 - val_mean_squared_error: 1036546464.6691\n",
      "Epoch 19/100\n",
      "1647/1647 [==============================] - 0s 174us/step - loss: 8959.2425 - mean_squared_error: 326367853.1293 - val_loss: 18685.9210 - val_mean_squared_error: 1038822560.8727\n",
      "Epoch 20/100\n",
      "1647/1647 [==============================] - 0s 165us/step - loss: 8719.2938 - mean_squared_error: 297891384.0971 - val_loss: 19051.3470 - val_mean_squared_error: 1051866866.3273\n",
      "Epoch 21/100\n",
      "1647/1647 [==============================] - 0s 173us/step - loss: 8710.3421 - mean_squared_error: 290009771.8276 - val_loss: 18885.1019 - val_mean_squared_error: 1041639628.4364\n",
      "Epoch 22/100\n",
      "1647/1647 [==============================] - 0s 166us/step - loss: 8158.6179 - mean_squared_error: 279515947.3515 - val_loss: 18888.6101 - val_mean_squared_error: 1042067430.1527\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 23/100\n",
      "1647/1647 [==============================] - 0s 191us/step - loss: 7540.1203 - mean_squared_error: 256399266.5282 - val_loss: 18751.8676 - val_mean_squared_error: 1043367797.7600\n",
      "Epoch 24/100\n",
      "1647/1647 [==============================] - 0s 174us/step - loss: 7362.1964 - mean_squared_error: 260342565.6466 - val_loss: 18601.8344 - val_mean_squared_error: 1037746618.3127\n",
      "Epoch 25/100\n",
      "1647/1647 [==============================] - 0s 166us/step - loss: 7013.5181 - mean_squared_error: 249807375.6697 - val_loss: 18545.2923 - val_mean_squared_error: 1032516416.4945\n",
      "Epoch 26/100\n",
      "1647/1647 [==============================] - 0s 163us/step - loss: 6908.3521 - mean_squared_error: 245383746.7201 - val_loss: 18586.2903 - val_mean_squared_error: 1042671366.4291\n",
      "Epoch 27/100\n",
      "1647/1647 [==============================] - 0s 171us/step - loss: 6880.7738 - mean_squared_error: 254048270.3437 - val_loss: 18642.4757 - val_mean_squared_error: 1045450203.0400\n",
      "Epoch 28/100\n",
      "1647/1647 [==============================] - 0s 169us/step - loss: 6718.6025 - mean_squared_error: 239133122.4918 - val_loss: 18515.4857 - val_mean_squared_error: 1053546392.7127\n",
      "Epoch 29/100\n",
      "1647/1647 [==============================] - 0s 162us/step - loss: 6655.4370 - mean_squared_error: 244652387.7511 - val_loss: 18465.2519 - val_mean_squared_error: 1045636995.0982\n",
      "Epoch 30/100\n",
      "1647/1647 [==============================] - 0s 163us/step - loss: 6466.6997 - mean_squared_error: 235006242.6910 - val_loss: 18575.0950 - val_mean_squared_error: 1044548076.8291\n",
      "Epoch 31/100\n",
      "1647/1647 [==============================] - 0s 163us/step - loss: 6356.9795 - mean_squared_error: 231492518.0279 - val_loss: 18420.7510 - val_mean_squared_error: 1046614640.8436\n",
      "Epoch 32/100\n",
      "1647/1647 [==============================] - 0s 161us/step - loss: 6373.2637 - mean_squared_error: 240664006.3570 - val_loss: 18572.6505 - val_mean_squared_error: 1049718625.8764\n",
      "Epoch 33/100\n",
      "1647/1647 [==============================] - 0s 163us/step - loss: 6274.8856 - mean_squared_error: 232349671.3977 - val_loss: 18414.5934 - val_mean_squared_error: 1049628613.6727\n",
      "Epoch 34/100\n",
      "1647/1647 [==============================] - 0s 160us/step - loss: 6293.7597 - mean_squared_error: 224295694.5622 - val_loss: 18507.7215 - val_mean_squared_error: 1034003242.0945\n",
      "Epoch 35/100\n",
      "1647/1647 [==============================] - 0s 159us/step - loss: 6177.8802 - mean_squared_error: 222734018.7444 - val_loss: 18478.8981 - val_mean_squared_error: 1044039317.4109\n",
      "Epoch 36/100\n",
      "1647/1647 [==============================] - 0s 171us/step - loss: 6094.3193 - mean_squared_error: 230158189.8919 - val_loss: 18673.1657 - val_mean_squared_error: 1054309581.9927\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 37/100\n",
      "1647/1647 [==============================] - 0s 174us/step - loss: 5850.2621 - mean_squared_error: 219710747.2665 - val_loss: 18509.9589 - val_mean_squared_error: 1050656153.1636\n",
      "Epoch 38/100\n",
      "1647/1647 [==============================] - 0s 196us/step - loss: 5678.3351 - mean_squared_error: 218863060.6800 - val_loss: 18497.6850 - val_mean_squared_error: 1055722711.9127\n",
      "Epoch 39/100\n",
      "1647/1647 [==============================] - 0s 176us/step - loss: 5716.6140 - mean_squared_error: 220789883.5883 - val_loss: 18471.2147 - val_mean_squared_error: 1058893598.0073\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 40/100\n",
      "1647/1647 [==============================] - 0s 174us/step - loss: 5471.0784 - mean_squared_error: 210865538.3388 - val_loss: 18499.0925 - val_mean_squared_error: 1055766688.4655\n",
      "Epoch 41/100\n",
      "1647/1647 [==============================] - 0s 168us/step - loss: 5450.8262 - mean_squared_error: 219495652.6047 - val_loss: 18464.0520 - val_mean_squared_error: 1051091089.6873\n",
      "Restoring model weights from the end of the best epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00041: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x155c1174828>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model.fit(\n",
    "    x, y, epochs=100, validation_split=.25, verbose=1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=8, verbose=1, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=.5, patience=3, verbose=1),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:10:07.465186Z",
     "start_time": "2019-05-09T21:10:07.257356Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae: 21,198.57\n",
      "mse: 848,061,545.17\n"
     ]
    }
   ],
   "source": [
    "benchmark(test_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-450d7395be5ba3eb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# In-class exercise (you code)\n",
    "\n",
    "The model above is overfitting. How can you tell?\n",
    "\n",
    "\n",
    "In the cell below, build a similar model but address the problem of overfitting. See if this improves performance on the test set.\n",
    "\n",
    "You should name your model `student_model`. Feel free to start with the boilerplate below\n",
    "\n",
    "```python\n",
    "student_model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=x.shape[1:]),\n",
    "    ...\n",
    "])\n",
    "student_model.compile(\n",
    "    keras.optimizers.adam(lr=0.001), loss=\"mean_absolute_error\",\n",
    "    metrics=[\"mean_squared_error\"])\n",
    "student_model.summary()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-556284169006d15f",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "<small>[Note from Sophie Searcy]</small>  \n",
    "I often find that L2 regularization is effective for tabular regression tasks. The trick is to make sure you are scaling your L2 multiplier correctly. You want to set the multiplier so that your model pays attention to both your regression loss and regularization loss. \n",
    "\n",
    "Also note that when we use regularization, we can no longer assume that the model loss (which is your total loss for your model) and regression loss (the loss for your regression task) are the same. In the case below, the loss reported after each epoch is now `mean_absolute_error+regularization`. To get a measure of the regression loss, I add `mean_absolute_error` back into the metrics. This way I know what each component of the loss is after each epoch. \n",
    "\n",
    "The last epoch for the solution below should log something like `loss: 20266.6511 - mean_absolute_error: 8115.6820`. With that information I know that my regularization loss is `20266.6511 - 8115.6820=12150.9690`. Even though the regularization loss is most of the total loss at the end, the model is still overfitting! That's okay, because we still are getting validation and test performance that's beating our random forest above 😉.\n",
    "\n",
    "Also note that we are using both dropout and L2 regularization. I find that the combination tends to work better that using one of them alone but your mileage may vary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:10:08.151337Z",
     "start_time": "2019-05-09T21:10:07.470501Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-50f90991a8757e6c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 140)               54320     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 140)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 120)               16920     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               12100     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 80)                8080      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 60)                4860      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 40)                2440      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 20)                820       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 99,561\n",
      "Trainable params: 99,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "L2 = 50\n",
    "DROP = 0.05\n",
    "  \n",
    "student_model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=x.shape[1:]),\n",
    "    keras.layers.Dense(units=140, activation=\"relu\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(L2)),\n",
    "    keras.layers.Dropout(DROP),\n",
    "    keras.layers.Dense(units=120, activation=\"relu\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(L2)),\n",
    "    keras.layers.Dropout(DROP),\n",
    "    keras.layers.Dense(units=100, activation=\"relu\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(L2)),\n",
    "    keras.layers.Dropout(DROP),\n",
    "    keras.layers.Dense(units=80, activation=\"relu\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(L2)),\n",
    "    keras.layers.Dropout(DROP),\n",
    "    keras.layers.Dense(units=60, activation=\"relu\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(L2)),\n",
    "    keras.layers.Dropout(.5*DROP),\n",
    "    keras.layers.Dense(units=40, activation=\"relu\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(L2)),\n",
    "    keras.layers.Dropout(.5*DROP),\n",
    "    keras.layers.Dense(units=20, activation=\"relu\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(L2)),\n",
    "#     keras.layers.Dropout(DROP),\n",
    "    keras.layers.Dense(units=1),\n",
    "])\n",
    "\n",
    "\n",
    "student_model.compile(\n",
    "    keras.optimizers.adam(lr=0.01), loss=\"mean_absolute_error\",\n",
    "    metrics=[\"mean_absolute_error\", \"mean_squared_error\"])\n",
    "student_model.summary()\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:10:37.593774Z",
     "start_time": "2019-05-09T21:10:08.154454Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1647 samples, validate on 550 samples\n",
      "Epoch 1/500\n",
      "1647/1647 [==============================] - 1s 738us/step - loss: 183564.5722 - mean_absolute_error: 179643.4958 - mean_squared_error: 38563740815.6211 - val_loss: 181327.0994 - val_mean_absolute_error: 181263.5056 - val_mean_squared_error: 39110209148.7418\n",
      "Epoch 2/500\n",
      "1647/1647 [==============================] - 0s 151us/step - loss: 179646.4070 - mean_absolute_error: 179633.1181 - mean_squared_error: 38559990652.8136 - val_loss: 181246.7716 - val_mean_absolute_error: 181245.8185 - val_mean_squared_error: 39103797672.4945\n",
      "Epoch 3/500\n",
      "1647/1647 [==============================] - 0s 151us/step - loss: 179606.1550 - mean_absolute_error: 179602.3432 - mean_squared_error: 38548973148.9496 - val_loss: 181204.2631 - val_mean_absolute_error: 181193.1224 - val_mean_squared_error: 39084697801.0764\n",
      "Epoch 4/500\n",
      "1647/1647 [==============================] - 0s 155us/step - loss: 179400.8455 - mean_absolute_error: 179228.6779 - mean_squared_error: 38417819150.6108 - val_loss: 173423.4213 - val_mean_absolute_error: 170982.4369 - val_mean_squared_error: 35552116408.3200\n",
      "Epoch 5/500\n",
      "1647/1647 [==============================] - 0s 189us/step - loss: 79914.2933 - mean_absolute_error: 67238.8928 - mean_squared_error: 8610171300.7602 - val_loss: 43993.5119 - val_mean_absolute_error: 29008.0628 - val_mean_squared_error: 2282301249.8618\n",
      "Epoch 6/500\n",
      "1647/1647 [==============================] - 0s 150us/step - loss: 48755.8861 - mean_absolute_error: 35621.2739 - mean_squared_error: 2480633553.6806 - val_loss: 41084.1098 - val_mean_absolute_error: 29290.4735 - val_mean_squared_error: 2356481835.9855\n",
      "Epoch 7/500\n",
      "1647/1647 [==============================] - 0s 159us/step - loss: 39173.7154 - mean_absolute_error: 27883.6873 - mean_squared_error: 1545291840.6606 - val_loss: 37026.1791 - val_mean_absolute_error: 25920.1715 - val_mean_squared_error: 1994425603.0255\n",
      "Epoch 8/500\n",
      "1647/1647 [==============================] - 0s 144us/step - loss: 34943.1517 - mean_absolute_error: 24551.7204 - mean_squared_error: 1254030234.0546 - val_loss: 31850.3233 - val_mean_absolute_error: 22059.5453 - val_mean_squared_error: 1869608242.3273\n",
      "Epoch 9/500\n",
      "1647/1647 [==============================] - 0s 155us/step - loss: 35770.3803 - mean_absolute_error: 25795.4495 - mean_squared_error: 1556434282.2004 - val_loss: 39130.1061 - val_mean_absolute_error: 28804.8668 - val_mean_squared_error: 2643748909.7309\n",
      "Epoch 10/500\n",
      "1647/1647 [==============================] - 0s 149us/step - loss: 34600.4923 - mean_absolute_error: 24848.9039 - mean_squared_error: 1407538695.7717 - val_loss: 37094.8297 - val_mean_absolute_error: 27708.4220 - val_mean_squared_error: 1786971374.7782\n",
      "Epoch 11/500\n",
      "1647/1647 [==============================] - 0s 143us/step - loss: 33761.2172 - mean_absolute_error: 24154.6707 - mean_squared_error: 1292391769.1026 - val_loss: 32688.8194 - val_mean_absolute_error: 23130.7374 - val_mean_squared_error: 1563427713.8618\n",
      "Epoch 12/500\n",
      "1647/1647 [==============================] - 0s 154us/step - loss: 32104.2542 - mean_absolute_error: 23034.4317 - mean_squared_error: 1093376248.3837 - val_loss: 28411.6174 - val_mean_absolute_error: 19857.8147 - val_mean_squared_error: 1219515362.5018\n",
      "Epoch 13/500\n",
      "1647/1647 [==============================] - 0s 200us/step - loss: 31862.2958 - mean_absolute_error: 23246.1738 - mean_squared_error: 1175902734.2999 - val_loss: 32912.1959 - val_mean_absolute_error: 24304.8906 - val_mean_squared_error: 1720806825.7745\n",
      "Epoch 14/500\n",
      "1647/1647 [==============================] - 0s 142us/step - loss: 33393.6115 - mean_absolute_error: 24827.2530 - mean_squared_error: 1373889660.3862 - val_loss: 31267.5214 - val_mean_absolute_error: 23041.3339 - val_mean_squared_error: 1347012422.8655\n",
      "Epoch 15/500\n",
      "1647/1647 [==============================] - 0s 153us/step - loss: 32537.1836 - mean_absolute_error: 24218.9364 - mean_squared_error: 1293949155.7110 - val_loss: 28554.1001 - val_mean_absolute_error: 20342.7290 - val_mean_squared_error: 1624943802.7055\n",
      "Epoch 16/500\n",
      "1647/1647 [==============================] - 0s 139us/step - loss: 29094.7652 - mean_absolute_error: 21077.0751 - mean_squared_error: 1089652146.6521 - val_loss: 35324.2791 - val_mean_absolute_error: 27263.4173 - val_mean_squared_error: 2551191606.2255\n",
      "Epoch 17/500\n",
      "1647/1647 [==============================] - 0s 189us/step - loss: 30246.0685 - mean_absolute_error: 22318.0360 - mean_squared_error: 1015936242.9435 - val_loss: 34084.9516 - val_mean_absolute_error: 26365.4516 - val_mean_squared_error: 1877877458.8509\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "Epoch 18/500\n",
      "1647/1647 [==============================] - 0s 140us/step - loss: 29083.2949 - mean_absolute_error: 21575.5204 - mean_squared_error: 1042390467.8664 - val_loss: 25282.1289 - val_mean_absolute_error: 18145.6727 - val_mean_squared_error: 1310264778.5309\n",
      "Epoch 19/500\n",
      "1647/1647 [==============================] - 0s 184us/step - loss: 26717.3396 - mean_absolute_error: 19760.9392 - mean_squared_error: 851161542.6837 - val_loss: 25681.0564 - val_mean_absolute_error: 18878.5349 - val_mean_squared_error: 1553789475.1418\n",
      "Epoch 20/500\n",
      "1647/1647 [==============================] - 0s 154us/step - loss: 26606.2875 - mean_absolute_error: 20005.4963 - mean_squared_error: 930022243.2447 - val_loss: 24341.9291 - val_mean_absolute_error: 17807.2968 - val_mean_squared_error: 1294120172.5382\n",
      "Epoch 21/500\n",
      "1647/1647 [==============================] - 0s 163us/step - loss: 25948.0465 - mean_absolute_error: 19558.2361 - mean_squared_error: 911294458.4044 - val_loss: 23310.5816 - val_mean_absolute_error: 17023.0991 - val_mean_squared_error: 1371904344.9018\n",
      "Epoch 22/500\n",
      "1647/1647 [==============================] - 0s 206us/step - loss: 25521.8573 - mean_absolute_error: 19250.4425 - mean_squared_error: 806292551.8106 - val_loss: 24519.7132 - val_mean_absolute_error: 18203.2716 - val_mean_squared_error: 1483632701.0327\n",
      "Epoch 23/500\n",
      "1647/1647 [==============================] - 0s 155us/step - loss: 27106.2777 - mean_absolute_error: 20885.4385 - mean_squared_error: 962305915.9976 - val_loss: 25202.6043 - val_mean_absolute_error: 18952.0142 - val_mean_squared_error: 1527210298.5309\n",
      "Epoch 24/500\n",
      "1647/1647 [==============================] - 0s 151us/step - loss: 24916.0625 - mean_absolute_error: 18794.5523 - mean_squared_error: 893189225.6369 - val_loss: 24942.0582 - val_mean_absolute_error: 18920.0340 - val_mean_squared_error: 1437966925.6727\n",
      "Epoch 25/500\n",
      "1647/1647 [==============================] - 0s 164us/step - loss: 25748.7892 - mean_absolute_error: 19697.4473 - mean_squared_error: 898200790.5962 - val_loss: 23733.4712 - val_mean_absolute_error: 17703.4451 - val_mean_squared_error: 1517772503.8545\n",
      "Epoch 26/500\n",
      "1647/1647 [==============================] - 0s 158us/step - loss: 25869.6408 - mean_absolute_error: 19861.4783 - mean_squared_error: 930387418.4238 - val_loss: 23949.7085 - val_mean_absolute_error: 18009.3376 - val_mean_squared_error: 1310571382.0509\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 27/500\n",
      "1647/1647 [==============================] - 0s 150us/step - loss: 24496.0187 - mean_absolute_error: 18581.5614 - mean_squared_error: 806709544.8792 - val_loss: 24670.3749 - val_mean_absolute_error: 18870.3360 - val_mean_squared_error: 1402259343.2436\n",
      "Epoch 28/500\n",
      "1647/1647 [==============================] - 0s 188us/step - loss: 24057.3001 - mean_absolute_error: 18242.7676 - mean_squared_error: 794913683.1767 - val_loss: 22729.8836 - val_mean_absolute_error: 16905.4938 - val_mean_squared_error: 1365593434.3127\n",
      "Epoch 29/500\n",
      "1647/1647 [==============================] - 0s 156us/step - loss: 24782.2983 - mean_absolute_error: 19018.2732 - mean_squared_error: 853049282.9144 - val_loss: 23739.7544 - val_mean_absolute_error: 18091.6369 - val_mean_squared_error: 1346159638.6909\n",
      "Epoch 30/500\n",
      "1647/1647 [==============================] - 0s 168us/step - loss: 24132.7597 - mean_absolute_error: 18434.4046 - mean_squared_error: 733740407.1791 - val_loss: 22290.6866 - val_mean_absolute_error: 16602.1400 - val_mean_squared_error: 1426060461.8473\n",
      "Epoch 31/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1647/1647 [==============================] - 0s 160us/step - loss: 24000.2095 - mean_absolute_error: 18352.9911 - mean_squared_error: 750750619.3467 - val_loss: 22313.4133 - val_mean_absolute_error: 16660.0972 - val_mean_squared_error: 1506638253.8473\n",
      "Epoch 32/500\n",
      "1647/1647 [==============================] - 0s 150us/step - loss: 23501.5377 - mean_absolute_error: 17853.2444 - mean_squared_error: 703639032.8986 - val_loss: 22503.8447 - val_mean_absolute_error: 16858.8276 - val_mean_squared_error: 1588448907.2291\n",
      "Epoch 33/500\n",
      "1647/1647 [==============================] - 0s 145us/step - loss: 24487.9308 - mean_absolute_error: 18872.9415 - mean_squared_error: 780795703.7814 - val_loss: 22677.3524 - val_mean_absolute_error: 17089.9118 - val_mean_squared_error: 1583393741.2073\n",
      "Epoch 34/500\n",
      "1647/1647 [==============================] - 0s 160us/step - loss: 23601.6093 - mean_absolute_error: 18018.9452 - mean_squared_error: 686497310.7565 - val_loss: 23138.4268 - val_mean_absolute_error: 17623.3631 - val_mean_squared_error: 1410267759.2436\n",
      "Epoch 35/500\n",
      "1647/1647 [==============================] - 0s 150us/step - loss: 23026.7362 - mean_absolute_error: 17483.0997 - mean_squared_error: 758608539.8033 - val_loss: 25118.3744 - val_mean_absolute_error: 19668.5950 - val_mean_squared_error: 1462580426.4727\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "Epoch 36/500\n",
      "1647/1647 [==============================] - 0s 193us/step - loss: 22990.5494 - mean_absolute_error: 17482.8420 - mean_squared_error: 758871119.2325 - val_loss: 23394.1051 - val_mean_absolute_error: 17854.1884 - val_mean_squared_error: 1678706550.8073\n",
      "Epoch 37/500\n",
      "1647/1647 [==============================] - 0s 170us/step - loss: 22213.4372 - mean_absolute_error: 16732.8090 - mean_squared_error: 686816946.0692 - val_loss: 22348.2841 - val_mean_absolute_error: 16902.2103 - val_mean_squared_error: 1541862813.6727\n",
      "Epoch 38/500\n",
      "1647/1647 [==============================] - 0s 147us/step - loss: 22183.1484 - mean_absolute_error: 16752.7992 - mean_squared_error: 734363755.8907 - val_loss: 22635.5770 - val_mean_absolute_error: 17163.3930 - val_mean_squared_error: 1492655662.4291\n",
      "Epoch 39/500\n",
      "1647/1647 [==============================] - 0s 185us/step - loss: 22105.1760 - mean_absolute_error: 16682.3335 - mean_squared_error: 660342703.1548 - val_loss: 22079.4353 - val_mean_absolute_error: 16638.9097 - val_mean_squared_error: 1576098109.0327\n",
      "Epoch 40/500\n",
      "1647/1647 [==============================] - 0s 138us/step - loss: 22075.0079 - mean_absolute_error: 16664.9472 - mean_squared_error: 645171014.9557 - val_loss: 22056.9778 - val_mean_absolute_error: 16654.1578 - val_mean_squared_error: 1493276400.1455\n",
      "Epoch 41/500\n",
      "1647/1647 [==============================] - 0s 168us/step - loss: 22651.1048 - mean_absolute_error: 17272.8629 - mean_squared_error: 763668298.6667 - val_loss: 21665.6230 - val_mean_absolute_error: 16296.2791 - val_mean_squared_error: 1469805023.7673\n",
      "Epoch 42/500\n",
      "1647/1647 [==============================] - 0s 149us/step - loss: 22424.6520 - mean_absolute_error: 17039.6592 - mean_squared_error: 689553577.9672 - val_loss: 23348.0568 - val_mean_absolute_error: 17931.2615 - val_mean_squared_error: 1699375088.0291\n",
      "Epoch 43/500\n",
      "1647/1647 [==============================] - 0s 160us/step - loss: 22183.5493 - mean_absolute_error: 16846.8122 - mean_squared_error: 737133267.5847 - val_loss: 21827.5795 - val_mean_absolute_error: 16484.0370 - val_mean_squared_error: 1500851882.0509\n",
      "Epoch 44/500\n",
      "1647/1647 [==============================] - 0s 160us/step - loss: 21902.0810 - mean_absolute_error: 16573.1622 - mean_squared_error: 681223046.6448 - val_loss: 22502.0202 - val_mean_absolute_error: 17149.6803 - val_mean_squared_error: 1603137466.4436\n",
      "Epoch 45/500\n",
      "1647/1647 [==============================] - 0s 190us/step - loss: 21917.1851 - mean_absolute_error: 16620.7489 - mean_squared_error: 648967753.7535 - val_loss: 22100.9514 - val_mean_absolute_error: 16780.9136 - val_mean_squared_error: 1570917625.2800\n",
      "Epoch 46/500\n",
      "1647/1647 [==============================] - 0s 152us/step - loss: 21289.8857 - mean_absolute_error: 16008.9223 - mean_squared_error: 626207574.3145 - val_loss: 22265.5003 - val_mean_absolute_error: 17001.1904 - val_mean_squared_error: 1572705274.8509\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 47/500\n",
      "1647/1647 [==============================] - 0s 145us/step - loss: 21982.9954 - mean_absolute_error: 16720.6923 - mean_squared_error: 695049750.2659 - val_loss: 22052.2675 - val_mean_absolute_error: 16805.0280 - val_mean_squared_error: 1505956900.0727\n",
      "Epoch 48/500\n",
      "1647/1647 [==============================] - 0s 159us/step - loss: 21450.7817 - mean_absolute_error: 16201.1389 - mean_squared_error: 663340152.2477 - val_loss: 22137.7210 - val_mean_absolute_error: 16894.6584 - val_mean_squared_error: 1616035504.7564\n",
      "Epoch 49/500\n",
      "1647/1647 [==============================] - 0s 146us/step - loss: 21347.6786 - mean_absolute_error: 16112.4206 - mean_squared_error: 642640308.7407 - val_loss: 23312.8470 - val_mean_absolute_error: 18133.8803 - val_mean_squared_error: 1470140672.3491\n",
      "Epoch 50/500\n",
      "1647/1647 [==============================] - 0s 148us/step - loss: 21607.0095 - mean_absolute_error: 16377.7908 - mean_squared_error: 616600567.7231 - val_loss: 21717.6978 - val_mean_absolute_error: 16487.0243 - val_mean_squared_error: 1451483921.6000\n",
      "Epoch 51/500\n",
      "1647/1647 [==============================] - 0s 155us/step - loss: 21327.0427 - mean_absolute_error: 16106.1790 - mean_squared_error: 622387318.6740 - val_loss: 22026.8826 - val_mean_absolute_error: 16801.5714 - val_mean_squared_error: 1477672536.5236\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "Epoch 52/500\n",
      "1647/1647 [==============================] - 0s 145us/step - loss: 21310.2397 - mean_absolute_error: 16100.4033 - mean_squared_error: 637376322.0012 - val_loss: 22070.6834 - val_mean_absolute_error: 16843.4144 - val_mean_squared_error: 1544580787.5200\n",
      "Epoch 53/500\n",
      "1647/1647 [==============================] - 0s 155us/step - loss: 21260.6780 - mean_absolute_error: 16045.7232 - mean_squared_error: 647499040.0000 - val_loss: 22152.7625 - val_mean_absolute_error: 16933.3605 - val_mean_squared_error: 1561214101.9055\n",
      "Epoch 54/500\n",
      "1647/1647 [==============================] - 0s 156us/step - loss: 21258.4883 - mean_absolute_error: 16049.8938 - mean_squared_error: 600472083.9733 - val_loss: 22366.5443 - val_mean_absolute_error: 17137.8548 - val_mean_squared_error: 1635124237.5709\n",
      "Epoch 55/500\n",
      "1647/1647 [==============================] - 0s 162us/step - loss: 21557.7631 - mean_absolute_error: 16356.1109 - mean_squared_error: 666486586.8707 - val_loss: 22011.7025 - val_mean_absolute_error: 16812.5345 - val_mean_squared_error: 1560872250.1527\n",
      "Epoch 56/500\n",
      "1647/1647 [==============================] - 0s 171us/step - loss: 20842.8796 - mean_absolute_error: 15657.8071 - mean_squared_error: 635984920.2380 - val_loss: 21961.0893 - val_mean_absolute_error: 16770.9718 - val_mean_squared_error: 1537003267.6655\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 57/500\n",
      "1647/1647 [==============================] - 0s 157us/step - loss: 21351.7213 - mean_absolute_error: 16174.6307 - mean_squared_error: 683361421.7171 - val_loss: 21924.7584 - val_mean_absolute_error: 16747.3213 - val_mean_squared_error: 1500427746.5309\n",
      "Epoch 00057: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x155c112b4e0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model.fit(\n",
    "    x, y, epochs=500, validation_split=.25, verbose=1, callbacks=[\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            patience=16,\n",
    "            verbose=1,\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            factor=.5,\n",
    "            patience=5,\n",
    "            verbose=1,\n",
    "        ),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:10:38.346928Z",
     "start_time": "2019-05-09T21:10:37.603312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae: 14,219.07\n",
      "mse: 457,268,290.94\n"
     ]
    }
   ],
   "source": [
    "benchmark(student_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9253ae6b098e7b42",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Question: addressing overfitting in deep learning\n",
    "\n",
    "Given the tools you've seen so far, if you have a deep learning model that's overfitting, how can you address this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-05c97ffa1cb3e051",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "# Question: addressing overfitting in deep learning\n",
    "\n",
    "- Reduce parameters by reducing the number of units in each layer or reducing the number of layers.\n",
    "- Add dropout layers (or increase the dropout rate)\n",
    "- Add regularization (or increase the strength)\n",
    "- There are others that we haven't discussed yet!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aside: How do you choose the size of each layer?\n",
    "\n",
    "The model in the above solution has the following layer sizes.\n",
    "\n",
    "- (input) 387\n",
    "- 120\n",
    "- 100\n",
    "- 80\n",
    "- 60\n",
    "- 40\n",
    "- 20\n",
    "- 1 (output layer)\n",
    "\n",
    "How should you choose these sizes when designing your own model?\n",
    "\n",
    "First, your input layer size is fixed by your input. This is also true of your output layer size which is fixed based on the task. Here's two examples of common tasks:\n",
    "- Regression on a single variable: `units=1`\n",
    "- Categorization with multiple categories: `units=len(unique(y))`\n",
    "\n",
    "That gives us our start and end point. Here are two rules of thumb:\n",
    "\n",
    "**Linear increase**: increase the size of each layer by a fixed amount. (like the above)\n",
    "\n",
    "**Geometric increase**: increase the size of each layer by a fixed multiplier. (like the below)\n",
    "\n",
    "- (input) 387\n",
    "- 64\n",
    "- 32\n",
    "- 16\n",
    "- 8\n",
    "- 4\n",
    "- 2\n",
    "- 1 (output layer)\n",
    "\n",
    "*How do you choose between the two?* I would consider them a generic hyperparameter that can be tweaked. If you're trying to squeeze every ounce of performance out of a model, compare the cross-validated performance of each. Otherwise, just pick the one you're most comfortable with and stick with it.\n",
    "\n",
    "*How do you choose the size of increase?* This should be considered a hyperparameter that affects model complexity. A bigger step size and you'll have more parameters and a more complex model that is more likely to overfit. You can use this to tune your model.  \n",
    "<small>(I like to limit the step size so that the first layer is smaller than the input size, mostly for aesthetic reasons.)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concluding\n",
    "\n",
    "Okay, that's a lot of information and a lot of (hyper)parameters to worry about! Technically, because we can always add more layers, there is an infinite number of hyperparameters in every model. That's kind of daunting! 🙅‍ Here's some final advice on how to deal with this problem and a little demo of how to use sklearn's `GridSearchCV`.\n",
    "\n",
    "## Advice\n",
    "- <mark>Start with an existing model that works</mark>. If there's a working model in a paper, blog-post, Metis lesson notebook, by all means *start* there. Then, as you're making this model your own, change it one step at a time and make sure each step doesn't break things.\n",
    "- Look for ways to <mark>reduce the things you need to worry about</mark>. Examples: I almost always leave `batch_size` at the default because it (typically) doesn't meaningfully affect training. I also use `EarlyStopping` in every model so I don't have to set the number of epochs.\n",
    "- <mark>Every change that you make should be for a reason</mark>. It's easy to get lost in the infinite tiny decisions in every deep learning model. Being strategic about the changes that you're making will help you navigate this problem. Examples:\n",
    "    - I want give my model room to be more complex *because I think it's underfit*. I'll add layers and/or increase the size of existing layers. I might also reduce my regularization settings if I already am using that.\n",
    "    - My model performance is not reliably improving between epochs. I want to test out different optimizers and learning rates *because I think there's an optimization problem*.\n",
    "    - I want to use regularization on my model *because I think it's overfit*. I'll add dropout or regulariation or increase the strength of those layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## `GridSearchCV` demo\n",
    "\n",
    "Even taking the advice above, we still have a bunch of decisions to make. Here's a demo using sklearn's grid search to tackle a bunch of hyperparameters. We just set up a function to build a model based on the hyperparameters and then let it go to town 💁‍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:12:11.848766Z",
     "start_time": "2019-05-09T21:12:11.795281Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_model(base=20, add=0, noise=.0, drop=0, depth=10, batchnorm=False,\n",
    "               act_reg=0.01, kern_reg=0.01, lr=0.01):\n",
    "\n",
    "    layers = [keras.layers.InputLayer(input_shape=x.shape[1:])]\n",
    "\n",
    "    if noise > 0:\n",
    "        layers.append(keras.layers.GaussianNoise(noise))\n",
    "\n",
    "    # using for loop to create layers instead of manually typing them\n",
    "    for mult in range(depth, 0, -1):\n",
    "        layers.append(\n",
    "            keras.layers.Dense(\n",
    "                units=mult * base + add, activation=\"relu\",\n",
    "                kernel_regularizer=keras.regularizers.l2(kern_reg),\n",
    "                activity_regularizer=keras.regularizers.l2(act_reg)),\n",
    "        )\n",
    "        if batchnorm and (mult % batchnorm == 0):\n",
    "            layers.append(keras.layers.BatchNormalization())\n",
    "        if drop > 0:\n",
    "            if mult == 0:\n",
    "                pass\n",
    "            if mult == 1:\n",
    "                pass\n",
    "            if mult == 2:\n",
    "                layers.append(keras.layers.Dropout(.5*drop))\n",
    "            else:\n",
    "                layers.append(keras.layers.Dropout(drop))\n",
    "\n",
    "    layers.append(keras.layers.Dense(1))\n",
    "\n",
    "    model = keras.Sequential(layers)\n",
    "\n",
    "    model.compile(keras.optimizers.adam(lr=lr), loss=\"mae\", metrics=[\"mae\", \"mse\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:12:29.598108Z",
     "start_time": "2019-05-09T21:12:14.307206Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 60)                23280     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 50)                3050      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 40)                2040      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 30)                1230      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 20)                620       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 30,241\n",
      "Trainable params: 30,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1647 samples, validate on 550 samples\n",
      "Epoch 1/200\n",
      "1647/1647 [==============================] - 1s 749us/step - loss: 181342.9945 - mean_absolute_error: 179642.8968 - mean_squared_error: 38563523391.8834 - val_loss: 181287.5994 - val_mean_absolute_error: 181262.2556 - val_mean_squared_error: 39109756056.6691\n",
      "Epoch 2/200\n",
      "1647/1647 [==============================] - 0s 99us/step - loss: 179635.2112 - mean_absolute_error: 179629.8863 - mean_squared_error: 38558851525.8676 - val_loss: 181240.5056 - val_mean_absolute_error: 181239.4744 - val_mean_squared_error: 39101493307.5782\n",
      "Epoch 3/200\n",
      "1647/1647 [==============================] - 0s 129us/step - loss: 179594.1745 - mean_absolute_error: 179588.7741 - mean_squared_error: 38544039819.1135 - val_loss: 181183.0298 - val_mean_absolute_error: 181166.9516 - val_mean_squared_error: 39075216022.8073\n",
      "Epoch 4/200\n",
      "1647/1647 [==============================] - 0s 135us/step - loss: 177972.0024 - mean_absolute_error: 177481.6550 - mean_squared_error: 37922778917.1488 - val_loss: 129443.7116 - val_mean_absolute_error: 121138.9707 - val_mean_squared_error: 21701292195.8400\n",
      "Epoch 5/200\n",
      "1647/1647 [==============================] - 0s 128us/step - loss: 65298.7331 - mean_absolute_error: 51314.6707 - mean_squared_error: 4965582635.3661 - val_loss: 43324.9853 - val_mean_absolute_error: 29227.5170 - val_mean_squared_error: 2442437937.3382\n",
      "Epoch 6/200\n",
      "1647/1647 [==============================] - 0s 105us/step - loss: 41828.8730 - mean_absolute_error: 28994.6030 - mean_squared_error: 1734555539.8179 - val_loss: 41684.2727 - val_mean_absolute_error: 29702.6384 - val_mean_squared_error: 1917759057.6873\n",
      "Epoch 7/200\n",
      "1647/1647 [==============================] - 0s 104us/step - loss: 36497.3732 - mean_absolute_error: 25422.6259 - mean_squared_error: 1316166407.4608 - val_loss: 37655.2369 - val_mean_absolute_error: 26857.5908 - val_mean_squared_error: 2198917524.0727\n",
      "Epoch 8/200\n",
      "1647/1647 [==============================] - 0s 97us/step - loss: 34373.7709 - mean_absolute_error: 23853.1511 - mean_squared_error: 1148318791.1111 - val_loss: 42432.1191 - val_mean_absolute_error: 31682.9525 - val_mean_squared_error: 2132204339.6655\n",
      "Epoch 9/200\n",
      "1647/1647 [==============================] - 0s 110us/step - loss: 32594.0723 - mean_absolute_error: 22531.9593 - mean_squared_error: 1040373133.2119 - val_loss: 40556.2859 - val_mean_absolute_error: 30235.6186 - val_mean_squared_error: 2768019919.0109\n",
      "Epoch 10/200\n",
      "1647/1647 [==============================] - 0s 113us/step - loss: 32321.2188 - mean_absolute_error: 22412.4886 - mean_squared_error: 1066019415.9369 - val_loss: 32673.4126 - val_mean_absolute_error: 23006.2364 - val_mean_squared_error: 1632871368.8727\n",
      "Epoch 11/200\n",
      "1647/1647 [==============================] - 0s 106us/step - loss: 29974.7088 - mean_absolute_error: 20277.2028 - mean_squared_error: 872543518.7760 - val_loss: 36842.8618 - val_mean_absolute_error: 27597.7305 - val_mean_squared_error: 1963599665.3382\n",
      "Epoch 12/200\n",
      "1647/1647 [==============================] - 0s 112us/step - loss: 31752.1622 - mean_absolute_error: 22282.4198 - mean_squared_error: 1043449016.9666 - val_loss: 41237.2509 - val_mean_absolute_error: 32259.3758 - val_mean_squared_error: 2151723320.7855\n",
      "Epoch 13/200\n",
      "1647/1647 [==============================] - 0s 104us/step - loss: 30037.1823 - mean_absolute_error: 20651.5864 - mean_squared_error: 964505495.1014 - val_loss: 30579.7164 - val_mean_absolute_error: 21154.2863 - val_mean_squared_error: 1983944663.4764\n",
      "Epoch 14/200\n",
      "1647/1647 [==============================] - 0s 101us/step - loss: 28159.6904 - mean_absolute_error: 19010.8028 - mean_squared_error: 787423439.0383 - val_loss: 34170.2471 - val_mean_absolute_error: 24805.6189 - val_mean_squared_error: 2113302627.1418\n",
      "Epoch 15/200\n",
      "1647/1647 [==============================] - 0s 104us/step - loss: 29434.0652 - mean_absolute_error: 20421.9553 - mean_squared_error: 961395831.3345 - val_loss: 29722.1106 - val_mean_absolute_error: 20876.5211 - val_mean_squared_error: 1675156892.8582\n",
      "Epoch 16/200\n",
      "1647/1647 [==============================] - 0s 106us/step - loss: 26868.4771 - mean_absolute_error: 17913.5877 - mean_squared_error: 796851445.5276 - val_loss: 28998.1642 - val_mean_absolute_error: 19972.9611 - val_mean_squared_error: 1551495356.5091\n",
      "Epoch 17/200\n",
      "1647/1647 [==============================] - 0s 97us/step - loss: 27566.5833 - mean_absolute_error: 18491.3963 - mean_squared_error: 738961672.6849 - val_loss: 31459.3247 - val_mean_absolute_error: 22503.8181 - val_mean_squared_error: 1680498066.6764\n",
      "Epoch 18/200\n",
      "1647/1647 [==============================] - 0s 98us/step - loss: 27043.5880 - mean_absolute_error: 18103.2666 - mean_squared_error: 764388443.8421 - val_loss: 30773.8295 - val_mean_absolute_error: 21702.1910 - val_mean_squared_error: 1741789887.1564\n",
      "Epoch 19/200\n",
      "1647/1647 [==============================] - 0s 97us/step - loss: 27936.9234 - mean_absolute_error: 19158.8899 - mean_squared_error: 847216116.4202 - val_loss: 29655.1274 - val_mean_absolute_error: 20827.0380 - val_mean_squared_error: 1936429923.7236\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "Epoch 20/200\n",
      "1647/1647 [==============================] - 0s 107us/step - loss: 23641.2904 - mean_absolute_error: 15079.8094 - mean_squared_error: 546093630.5039 - val_loss: 25641.0735 - val_mean_absolute_error: 17170.9066 - val_mean_squared_error: 1554884987.0400\n",
      "Epoch 21/200\n",
      "1647/1647 [==============================] - 0s 100us/step - loss: 22020.3654 - mean_absolute_error: 13685.0250 - mean_squared_error: 490770357.0419 - val_loss: 25281.5824 - val_mean_absolute_error: 16999.2572 - val_mean_squared_error: 1606791202.1091\n",
      "Epoch 22/200\n",
      "1647/1647 [==============================] - 0s 102us/step - loss: 21536.1952 - mean_absolute_error: 13374.9886 - mean_squared_error: 493996885.7122 - val_loss: 25825.8389 - val_mean_absolute_error: 17817.1934 - val_mean_squared_error: 1515282179.7818\n",
      "Epoch 23/200\n",
      "1647/1647 [==============================] - 0s 104us/step - loss: 20939.6145 - mean_absolute_error: 12937.3833 - mean_squared_error: 448471255.4706 - val_loss: 24904.0276 - val_mean_absolute_error: 16963.5871 - val_mean_squared_error: 1510706461.6145\n",
      "Epoch 24/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1647/1647 [==============================] - 0s 111us/step - loss: 20879.8248 - mean_absolute_error: 12973.7685 - mean_squared_error: 482120273.2920 - val_loss: 24935.1208 - val_mean_absolute_error: 17078.8595 - val_mean_squared_error: 1642806337.3091\n",
      "Epoch 25/200\n",
      "1647/1647 [==============================] - 0s 99us/step - loss: 20974.8496 - mean_absolute_error: 13155.4886 - mean_squared_error: 495302933.1390 - val_loss: 24466.3347 - val_mean_absolute_error: 16675.6786 - val_mean_squared_error: 1557293472.3927\n",
      "Epoch 26/200\n",
      "1647/1647 [==============================] - 0s 101us/step - loss: 20633.8814 - mean_absolute_error: 12872.5459 - mean_squared_error: 468560171.9684 - val_loss: 24247.9288 - val_mean_absolute_error: 16540.9211 - val_mean_squared_error: 1440464466.9236\n",
      "Epoch 27/200\n",
      "1647/1647 [==============================] - 0s 107us/step - loss: 20234.9449 - mean_absolute_error: 12544.5602 - mean_squared_error: 436268765.0322 - val_loss: 24011.5391 - val_mean_absolute_error: 16362.8309 - val_mean_squared_error: 1543760407.8836\n",
      "Epoch 28/200\n",
      "1647/1647 [==============================] - 0s 126us/step - loss: 20169.5211 - mean_absolute_error: 12554.0253 - mean_squared_error: 453891325.7753 - val_loss: 24116.2593 - val_mean_absolute_error: 16479.8304 - val_mean_squared_error: 1507336458.7636\n",
      "Epoch 29/200\n",
      "1647/1647 [==============================] - 0s 98us/step - loss: 20005.9907 - mean_absolute_error: 12426.2096 - mean_squared_error: 429953699.1670 - val_loss: 24334.5742 - val_mean_absolute_error: 16748.6425 - val_mean_squared_error: 1509264519.2000\n",
      "Epoch 30/200\n",
      "1647/1647 [==============================] - 0s 99us/step - loss: 20278.1206 - mean_absolute_error: 12728.2494 - mean_squared_error: 444195108.4882 - val_loss: 24089.4962 - val_mean_absolute_error: 16570.5220 - val_mean_squared_error: 1522389080.4364\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "Epoch 31/200\n",
      "1647/1647 [==============================] - 0s 101us/step - loss: 19594.6171 - mean_absolute_error: 12079.8415 - mean_squared_error: 452122041.8215 - val_loss: 23686.1735 - val_mean_absolute_error: 16157.2936 - val_mean_squared_error: 1475827371.0400\n",
      "Epoch 32/200\n",
      "1647/1647 [==============================] - 0s 107us/step - loss: 19290.5221 - mean_absolute_error: 11784.9510 - mean_squared_error: 406718530.8172 - val_loss: 23619.1411 - val_mean_absolute_error: 16123.9912 - val_mean_squared_error: 1474530026.4000\n",
      "Epoch 33/200\n",
      "1647/1647 [==============================] - 0s 115us/step - loss: 19284.2135 - mean_absolute_error: 11812.7523 - mean_squared_error: 424709623.0043 - val_loss: 23607.0636 - val_mean_absolute_error: 16135.3029 - val_mean_squared_error: 1432973078.8073\n",
      "Epoch 34/200\n",
      "1647/1647 [==============================] - 0s 112us/step - loss: 19286.4620 - mean_absolute_error: 11809.0552 - mean_squared_error: 415928774.0814 - val_loss: 23765.2949 - val_mean_absolute_error: 16264.7995 - val_mean_squared_error: 1503258489.3818\n",
      "Epoch 35/200\n",
      "1647/1647 [==============================] - 0s 110us/step - loss: 19237.6428 - mean_absolute_error: 11774.8598 - mean_squared_error: 448605577.8118 - val_loss: 23762.8704 - val_mean_absolute_error: 16322.6779 - val_mean_squared_error: 1457810558.2109\n",
      "Epoch 36/200\n",
      "1647/1647 [==============================] - 0s 116us/step - loss: 19059.4284 - mean_absolute_error: 11612.9462 - mean_squared_error: 351575398.1979 - val_loss: 23736.6161 - val_mean_absolute_error: 16295.1218 - val_mean_squared_error: 1493065362.0800\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "Epoch 37/200\n",
      "1647/1647 [==============================] - 0s 113us/step - loss: 18968.5348 - mean_absolute_error: 11530.1410 - mean_squared_error: 424587658.9217 - val_loss: 23710.6593 - val_mean_absolute_error: 16262.3734 - val_mean_squared_error: 1500265296.6545\n",
      "Epoch 38/200\n",
      "1647/1647 [==============================] - 0s 114us/step - loss: 18729.7096 - mean_absolute_error: 11278.3575 - mean_squared_error: 392653079.6260 - val_loss: 23737.4084 - val_mean_absolute_error: 16289.9633 - val_mean_squared_error: 1512135786.2545\n",
      "Epoch 39/200\n",
      "1647/1647 [==============================] - 0s 116us/step - loss: 19211.0275 - mean_absolute_error: 11770.3931 - mean_squared_error: 431936977.6612 - val_loss: 23749.9101 - val_mean_absolute_error: 16311.7633 - val_mean_squared_error: 1505467191.4909\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "Epoch 40/200\n",
      "1647/1647 [==============================] - 0s 101us/step - loss: 18899.5463 - mean_absolute_error: 11461.4223 - mean_squared_error: 418153002.0935 - val_loss: 23753.5368 - val_mean_absolute_error: 16317.1255 - val_mean_squared_error: 1504933594.4582\n",
      "Epoch 41/200\n",
      "1647/1647 [==============================] - 0s 101us/step - loss: 18923.8527 - mean_absolute_error: 11487.8490 - mean_squared_error: 432230462.4457 - val_loss: 23743.9909 - val_mean_absolute_error: 16305.9178 - val_mean_squared_error: 1505993803.3018\n",
      "Epoch 00041: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x155c389f748>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = make_model(base=10, add=10, depth=5, drop=0.01, batchnorm=False, act_reg=0, kern_reg=50)\n",
    "test_model.summary()\n",
    "test_model.fit(\n",
    "    x, y, epochs=200, validation_split=.25, verbose=1, callbacks=[\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            patience=8,\n",
    "            verbose=1,\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            factor=.2,\n",
    "            patience=3,\n",
    "            verbose=1,\n",
    "        ),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:11:02.642798Z",
     "start_time": "2019-05-09T21:11:02.306966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae: 14,306.84\n",
      "mse: 436,571,329.09\n"
     ]
    }
   ],
   "source": [
    "benchmark(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:13:13.840706Z",
     "start_time": "2019-05-09T21:13:13.801208Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.wrappers import scikit_learn as k_sklearn\n",
    "from sklearn import model_selection\n",
    "\n",
    "keras_model = k_sklearn.KerasRegressor(make_model)\n",
    "\n",
    "validator = model_selection.GridSearchCV(\n",
    "    keras_model, param_grid={\n",
    "        'base': [10, 20], \n",
    "        'noise': [0],\n",
    "        'depth': [5, 10],\n",
    "        'drop': [0, 0.01],\n",
    "        'act_reg':[0], \n",
    "        'kern_reg':[0,10],\n",
    "        'batchnorm': [False],\n",
    "    }, scoring='neg_mean_absolute_error', n_jobs=-1, cv=3, verbose=2)\n",
    "\n",
    "# Uncomment when you're ready to run. This one will take a while\n",
    "\n",
    "# validator.fit(\n",
    "#     x, y, epochs=200, validation_split=.25, verbose=0, callbacks=[\n",
    "#         keras.callbacks.EarlyStopping(\n",
    "#             patience=8,\n",
    "#             verbose=0,\n",
    "#         ),\n",
    "#         keras.callbacks.ReduceLROnPlateau(\n",
    "#             factor=.2,\n",
    "#             patience=3,\n",
    "#             verbose=0,\n",
    "#         ),\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:11:42.657298Z",
     "start_time": "2019-05-09T21:09:16.095Z"
    }
   },
   "outputs": [],
   "source": [
    "# y_pred = validator.predict(x_test)\n",
    "# print(f\"mae: {metrics.mean_absolute_error(y_test, y_pred):,.2f}\")\n",
    "# print(f\"mse: {metrics.mean_squared_error(y_test, y_pred):,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:11:42.659701Z",
     "start_time": "2019-05-09T21:09:16.099Z"
    }
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(validator.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
