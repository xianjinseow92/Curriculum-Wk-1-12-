{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Introduction\n",
    "\n",
    "Structured Query Language (**SQL**) is a [declaritive language](http://en.wikipedia.org/wiki/Declarative_programming) for working with data. This differs from the [imperative style](https://en.wikipedia.org/wiki/Imperative_programming) of programming that we have used with Python so far. The biggest impact of this difference is that it is much harder to step \"line-by-line\" through an SQL statement to debug it, but the upside is your focus is on what you want to do rather than how to do it.\n",
    "\n",
    "The most common use cases for SQL are the \"CRUD\" operations: \n",
    "\n",
    "* Creating, \n",
    "* Reading, \n",
    "* Updating  \n",
    "* Deleting \n",
    "\n",
    "data. The majority of the time data scientists and data analyists will be concerned with **READING** data from the database, to use in your modeling. \n",
    "\n",
    "## Why SQL?\n",
    "\n",
    "The data in an SQL database is organized into _tables_, which can be thought of as similar to a Pandas DataFrame. One of the ongoing struggles is convincing students (**like YOU!**) to use SQL, when it seems to do the same thing as a reading a CSV into a dataframe.\n",
    "\n",
    "#### The typical Pandas workflow:\n",
    "* Grab a CSV on your machine. It has all the different columns that you want, and is a \"frozen\" set of the data as it was when the CSV was made.\n",
    "* Load it into a DataFrame\n",
    "* Do the analysis.\n",
    "\n",
    "This is simple and straight-forward, but has several drawbacks:\n",
    "1. It is difficult to get up-to-date data. You could try and solve this by grabbing CSVs that are automatically created online, and loading from a web-address.\n",
    "2. If working on a team, your CSVs will become out-of-date and out of sync with one another. Any changes made to the data won't be reflected in your work until you download a new CSV.\n",
    "4. You need to load in _all_ the columns from the CSV. You can drop them later, but Pandas isn't memory efficient. \n",
    "5. Related to the last point, if you have to download _all_ the data before you start filtering it and throwing away what you don't want, this is very inefficient. For example, if you were looking for average traffic per hour over the last week, and each location has a sensor that took data every minute, you would be downloading \n",
    "  $$(1\\text{ week}) \\times (10080\\text{ minutes/week}) = 10080\\text{ records}$$\n",
    "  per sensor to get 168 numbers!\n",
    "6. If you need to make a change to the data, you cannot just modify a CSV and push it up to a server, because you will all be over writing each others changes.\n",
    "\n",
    "#### Database workflow\n",
    "Databases address these issues by\n",
    "* Being a single source of truth\n",
    "* Allow multiple users to modify them at the same time while ensuring consistency\n",
    "* Living \"close\" to the data (i.e. worry less about processes within the database using significant network traffic)\n",
    "* Designed to scale with large data volumes.\n",
    "\n",
    "A workflow for using SQL and Pandas together is\n",
    "1. Download a sample of your data from SQL into Pandas\n",
    "2. Do EDA on the sample. In particular\n",
    "    1. What sorts of missing data do you have? How are you going to impute?\n",
    "    2. What are the types of each data?\n",
    "    3. Are there any obvious outliers? Are you going to correct them? Limit them? Filter them out?\n",
    "3. Determine, where possible, how to either select cleaned data, or update the data in batches and push back to the database\n",
    "4. In a *new* notebook, select only the columns that you want, and filter to the rows that you want, and retrieve that data from the database. Load this data into a dataframe in order to create visualizations, or run your models on them.\n",
    "\n",
    "#### Warning\n",
    "\n",
    "Because the projects you work on at Metis prior to project 4 typically deal with small datasets (i.e. all the data can fit into memory), and only have a single developer (**YOU!**) it can be difficult to see what the benefits of using a database is in your own project.\n",
    "\n",
    "With **large** data and **mutiple** consumers of the data, using CSVs just won't cut it. SQL is **NOT** optional!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas to SQL translator\n",
    "\n",
    "Pandas and SQL have a lot of similarities, and we can use this to familiarize ourselves with the standard objects in SQL:\n",
    "\n",
    "| Pandas | SQL equivalent |\n",
    "| --- | --- |\n",
    "| Dataframe | Table |\n",
    "| Column (of a dataframe) | Field |\n",
    "| Row (of a dataframe) | Record |\n",
    "\n",
    "#### Pandas:\n",
    "* Can feed data directly into sklearn's models\n",
    "* Easy to do visualization\n",
    "\n",
    "#### SQL\n",
    "* Scales well with huge amounts of data\n",
    "* Can summarize / group data close to source, lowering network traffic\n",
    "* Allows consistency when working with data that is updating (multiple team members, or streaming updates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Setup SQL (to do!)*\n",
    "\n",
    "Today we are going to setup Postgres on our local machine, and load some data into it. Today's focus is on learning how to query the database, so we will run some setup scripts to create a `names` database and put some information in there.\n",
    "\n",
    "In a terminal in the directory for today's lectures, run the following commands (for OSX):\n",
    "```bash\n",
    "# unzips the data file into a 90 MB CSV\n",
    "gunzip data/all_state_1950.csv.gz\n",
    "\n",
    "# installs postgres \n",
    "brew install postgresql\n",
    "\n",
    "# start postgres so it \"listens\" to connections\n",
    "brew services start postgres\n",
    "\n",
    "# allows the current user to access postgres.\n",
    "# it does this by creating a database for the active user\n",
    "createdb\n",
    "\n",
    "# run the setup script to load the data into postgres\n",
    "psql -f setup.sql\n",
    "```\n",
    "\n",
    "You can check that this works by typing `psql` at the prompt. You should see something similar to \n",
    "```\n",
    "psql (10.4)\n",
    "Type \"help\" for help.\n",
    "\n",
    "damien=# \n",
    "```\n",
    "Type `\\q` to quit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next up....\n",
    "\n",
    "Now that we have Postgres installed and data loaded, we are ready to look at [some exercises that use SQL](../sql-intro/02_SQL_Exercises.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
