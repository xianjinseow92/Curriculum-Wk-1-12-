{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Hadoop\n",
    "\n",
    "Now let's try out Hadoop. The Docker image we just setup should currently be running. \n",
    "\n",
    "## Verify that Hadoop is running\n",
    "\n",
    "In the terminal window that is running Docker, run the following:\n",
    "\n",
    "```bash\n",
    "jps\n",
    "```\n",
    "\n",
    "This will show you that `DataNode`, `NameNode` and `SecondaryNameNode` are running:\n",
    "\n",
    "```bash\n",
    "141 NameNode\n",
    "574 Jps\n",
    "267 DataNode\n",
    "435 SecondaryNameNode\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine data\n",
    "\n",
    "Alright, we already have some data in this Docker container that we'll be using. Let's examine it. \n",
    "\n",
    "The data is located in the text file `/root/textdata/44604.txt.utf-8` and comes from this Gutenberg ebook: \n",
    "\n",
    " * [How to Become an Engineer by Frank W. Doughty](http://www.gutenberg.org/ebooks/44604.txt.utf-8)\n",
    " \n",
    "Note: We are using the plain text `utf-8` encoding!\n",
    " \n",
    "```bash\n",
    "head /root/textdata/44604.txt.utf-8\n",
    "```\n",
    "\n",
    "should print something like this:\n",
    "\n",
    "```\n",
    "The Project Gutenberg eBook, How to Become an Engineer, by Frank W. Doughty\n",
    "\n",
    "\n",
    "This eBook is for the use of anyone anywhere at no cost and with\n",
    "almost no restrictions whatsoever.  You may copy it, give it away or\n",
    "re-use it under the terms of the Project Gutenberg License included\n",
    "with this eBook or online at www.gutenberg.org\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put data in HDFS\n",
    "\n",
    "First, make some directories **in the hadoop distributed file system!**\n",
    "\n",
    "```bash\n",
    "hdfs dfs -mkdir /user/root/\n",
    "hdfs dfs -mkdir /user/root/gutenberg\n",
    "```\n",
    "\n",
    "Letâ€™s check that they exist\n",
    "\n",
    "```bash\n",
    "hdfs dfs -ls /\n",
    "hdfs dfs -ls /user/\n",
    "```\n",
    "\n",
    "There may be directories printed other than the ones we created.\n",
    "\n",
    "Yay!\n",
    "\n",
    "Ok, now put some data into hdfs:\n",
    "\n",
    "```bash\n",
    "hdfs dfs -put /root/textdata/* /user/root/gutenberg\n",
    "```\n",
    "\n",
    "Make sure the data is in the hdfs:\n",
    "\n",
    "```bash\n",
    "hdfs dfs -ls /user/root/gutenberg\n",
    "```\n",
    "\n",
    "Yay!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the mapper and reducer \n",
    "\n",
    "This will be done directly on our Docker instance. First, change to the root's home directory.\n",
    "\n",
    "```bash\n",
    "cd ~\n",
    "```\n",
    "\n",
    "You can use vi or nano to create the following two files. Note that these files use Python 2. Python 3 will not work as TextBlob has only been installed for Python 2 in this Docker container.\n",
    "\n",
    "Our mapper `count_mapper.py` includes the following code:\n",
    "\n",
    "```python\n",
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "from textblob import TextBlob\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.decode('utf-8')\n",
    "    words = TextBlob(line).words\n",
    "    for word in words:\n",
    "        word = word.encode('utf-8')\n",
    "        print(\"{}\\t{}\".format(word, 1))\n",
    "```\n",
    "\n",
    "And our reducer `count_reducer.py` looks like this:\n",
    "\n",
    "```python\n",
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    word, count = line.split('\\t')\n",
    "    count = int(count)\n",
    "    if word == current_word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            print('{}\\t{}'.format(current_word, current_count))\n",
    "        current_word = word\n",
    "        current_count = count\n",
    "\n",
    "if current_word == word:\n",
    "    print('%s\\t%i' % (current_word, current_count))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's run it!\n",
    "\n",
    "Before giving the following command, it's a good idea to ensure the map and reduce files are  executable: \n",
    "\n",
    "```bash\n",
    "chmod +x /root/count_mapper.py\n",
    "chmod +x /root/count_reducer.py\n",
    "```\n",
    "\n",
    "Now, run the map-reduce job:\n",
    "\n",
    "\n",
    "```bash\n",
    "hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-file /root/count_mapper.py \\\n",
    "-mapper /root/count_mapper.py \\\n",
    "-file /root/count_reducer.py \\\n",
    "-reducer /root/count_reducer.py \\\n",
    "-input /user/root/gutenberg/* \\\n",
    "-output /user/root/book-output\n",
    "```\n",
    "\n",
    "The output should something like the following:\n",
    "\n",
    "```bash\n",
    "16/12/18 19:45:12 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
    "packageJobJar: [/root/count_mapper.py, /root/count_reducer.py] [] /tmp/streamjob7522181193963040200.jar tmpDir=null\n",
    "16/12/18 19:45:13 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
    "16/12/18 19:45:13 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
    "16/12/18 19:45:13 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
    "16/12/18 19:45:14 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
    "16/12/18 19:45:14 INFO mapreduce.JobSubmitter: number of splits:1\n",
    "16/12/18 19:45:14 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local236049766_0001\n",
    "...  \n",
    "...\n",
    "16/12/18 19:46:10 INFO streaming.StreamJob: Output directory: /user/root/book-output\n",
    "```\n",
    "\n",
    "Booom! :boom: It's running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the output\n",
    "\n",
    "Once it's done,\n",
    "\n",
    "```bash\n",
    "hdfs dfs -ls /user/root/book-output\n",
    "```\n",
    "\n",
    "should show that there is a `_SUCCESS` file (showing we did it!) and\n",
    "another file called `part-00000`\n",
    "\n",
    "This `part-00000` is our output. To look in:\n",
    "\n",
    "```bash\n",
    "hdfs dfs -cat /user/root/book-output/part-00000\n",
    "```\n",
    "\n",
    "or just\n",
    "\n",
    "```\n",
    "hdfs dfs -cat /user/root/book-output/*\n",
    "```\n",
    "\n",
    "will show the output of our job!\n",
    "\n",
    "If you want to see the most common words, run:\n",
    "\n",
    "```bash\n",
    "hdfs dfs -cat /user/root/book-output/* | sort -rnk2 | head\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE\n",
    "\n",
    "If something went wrong with the mapreduce job, or you fix something and want to run it again, it will throw a different error the second time. This error will say that the book-output directory already exists in hdfs. \n",
    "\n",
    "This error is thrown to avoid overwriting previous results. If you want to just rerun it anyway, you need to delete the output first, so it can be created again:\n",
    "\n",
    "```bash\n",
    "hdfs dfs -rm -r /user/root/book-output\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metis",
   "language": "python",
   "name": "metis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
