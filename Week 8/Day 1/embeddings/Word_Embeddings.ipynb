{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will get an overview of how to generate word vectors using the various word embedding methods discussed in the lecture\n",
    "\n",
    "### Objectives:\n",
    "- Implement Count Vectors with sklearn\n",
    "- Implement TF-IDF Vectors with sklearn and gensim\n",
    "- Train and save word2vec model with gensim\n",
    "- Load Google's pretrained word2vec model\n",
    "- Load Stanford's pretrained GloVe model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch 4 news groups articles from this dataset\n",
    "categories = ['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med'] \n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "corpus = twenty_train.data[0:50]   # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n",
      "Organization: The City University\n",
      "Lines: 14\n",
      "\n",
      "Does anyone know of a good way (standard PC application/PD utility) to\n",
      "convert tif/img/tga files into LaserJet III format.  We would also like to\n",
      "do the same, converting to HPGL (HP plotter) files.\n",
      "\n",
      "Please email any response.\n",
      "\n",
      "Is this the correct group?\n",
      "\n",
      "Thanks in advance.  Michael.\n",
      "-- \n",
      "Michael Collier (Programmer)                 The Computer Unit,\n",
      "Email: M.P.Collier@uk.ac.city                The City University,\n",
      "Tel: 071 477-8000 x3769                      London,\n",
      "Fax: 071 477-8565                            EC1V 0HB.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"{corpus[0]}\")\n",
    "\n",
    "# This email seems to be about computer graphics\n",
    "# This is not good neough for us to fit into any model.\n",
    "# We have to do some processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of Document-term matrix: (50, 3075)\n"
     ]
    }
   ],
   "source": [
    "# We use a count-vectorizer to do so\n",
    "vectorizer = CountVectorizer() #initialize\n",
    "# lowercase parameter\n",
    "# token_pattern parameter\n",
    "# \n",
    "X = vectorizer.fit_transform(corpus) # fit\n",
    "\n",
    "print(f\"Dimensions of Document-term matrix: {X.toarray().shape}\")\n",
    "\n",
    "# we only have 50 documents and only 3000+ different vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result above, we can see that the second dimension gives us the size of our vocabulary.\n",
    "\n",
    "But why restrict ourselves to single words? We can pass an additional argument to the CountVectorizer() object to add n-grams to our vocabulary.\n",
    "\n",
    "What is an n-gram?  It's just a collection of n consecutive words. For example:\n",
    "\"New\", \"York\", \"City\", \"subway\" are all unigrams\n",
    "\"New York\", \"York City\", \"City subway\" are bigrams\n",
    "\"New York City\", \"York City subway\" are trigrams\n",
    "\"New York City subway\" is a 4-gram\n",
    "\n",
    "We can specify to include n-grams with the ngram_range argument.  This takes a tuple which specifies the range of n-grams that we should include (inclusively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of Document-term matrix: (50, 23397)\n"
     ]
    }
   ],
   "source": [
    "# Include unigrams, bigrams, and trigrams\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,3)) # (1,3) is a range that generates 1, 2, 3-grams for you\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(f\"Dimensions of Document-term matrix: {X.toarray().shape}\")  \n",
    "# Now we can see that our number of columns exploded because we added 1,2,3-grams!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common preprocessing step in many NLP applications is stop-word removal.\n",
    "Common words like \"a\", \"the\", \"and\" often add a lot of noise, and don't typiccally contribute much to the task we are trying to solve.\n",
    "\n",
    "CountVectorizer also comes equipped with a way of dealing with common English stop words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of Document-term matrix: (50, 14300)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,3), stop_words='english') \n",
    "# we can use the countvectorizer to remove stopwords for us here.\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(f\"Dimensions of Document-term matrix: {X.toarray().shape}\")\n",
    "# By removing the stop-words, we see that we reduced the number of words by 10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's look at tf-idf!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Vectors\n",
    "\n",
    "Here we will demonstrate two ways to generate TF-IDF vectors with both sklearn and gensim.  It's good to be aware of both methods because depending on your specific workflow, one method might be easier than the other!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sklearn, it's VERY similar to how we did CountVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,3), stop_words='english') # just replace 'Count' with 'Tfidf'\n",
    "X_tfidf = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVector: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 2 2 1 1 0 0 0 0 0 0 0]\n",
      "\n",
      "\n",
      "TFIDF: [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.127 0.127 0.063 0.063 0.    0.    0.    0.    0.\n",
      " 0.    0.   ]\n"
     ]
    }
   ],
   "source": [
    "# How do these two representations compare?\n",
    "# Let's look at the first 50 dimensions of the first document to gain some intuition\n",
    "\n",
    "np.set_printoptions(precision=3) # This just makes things a little easier to read\n",
    "\n",
    "print(f\"CountVector: {X.toarray()[0,0:50]}\\n\\n\")\n",
    "\n",
    "print(f\"TFIDF: {X_tfidf.toarray()[0,0:50]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn isn't our only option for doing TF-IDF.  Gensim is another popular library for many NLP tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use gensim. Not just Tfidf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the documents (run this line to tokenize your documents (split into BOW))\n",
    "tokenized_docs = [gensim.utils.simple_preprocess(d) for d in corpus]  \n",
    "\n",
    "\n",
    "# Create a Gensim Dictionary.  This creates an id to word mapping for everything in our vocbulary\n",
    "# It is NOT the same as the dictionary object in the Python standard library\n",
    "\n",
    "# This is LITERALLY dictionary. Not a dictionary object.\n",
    "mydict = gensim.corpora.Dictionary()\n",
    "\n",
    "\n",
    "# Create a Gensim Corpus object.  This creates a list of tuples for each document.\n",
    "# The first element of the tuple is the word id, the second is the number of counts\n",
    "mycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the doc-term matrix as a numpy array.\n",
    "# Typically these matrices are HUGE so, it's usuall not a great idea to create the full dense doc-term matrix.\n",
    "# We do it here to illustrate that you can get the same info as we obtained in scikit-learn!\n",
    "\n",
    "doc_term_matrix = gensim.matutils.corpus2dense(mycorpus, num_terms=len(mydict))\n",
    "\n",
    "# When we see a \"2dense\", it is basically converting it into a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 0., 0., ..., 2., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 3., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a tf-idf model is very simple!\n",
    "tfidf = gensim.models.TfidfModel(mycorpus)\n",
    "tfidf_matrix = gensim.matutils.corpus2dense(tfidf[mycorpus], num_terms=len(mydict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.146, 0.   , 0.   , ..., 0.141, 0.   , 0.   ],\n",
       "       [0.093, 0.   , 0.   , ..., 0.   , 0.   , 0.   ],\n",
       "       [0.037, 0.   , 0.056, ..., 0.   , 0.   , 0.   ],\n",
       "       ...,\n",
       "       [0.   , 0.   , 0.   , ..., 0.   , 0.   , 0.072],\n",
       "       [0.   , 0.   , 0.   , ..., 0.   , 0.   , 0.072],\n",
       "       [0.   , 0.   , 0.   , ..., 0.   , 0.   , 0.072]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are going to look at how Word2Vec and GloVe works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec and GloVe\n",
    "\n",
    "Word2Vec is a very powerful and useful word embedding method.  The math can get a little sticky, but luckily Gensim comes equipped with ways for us to train our own Word2Vec model, or load in a pre-trained word2vec model.  Let's check it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagine we have a corpus of 9 documents\n",
    "\n",
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",\n",
    "              \"Relation of user perceived response time to error measurement\",\n",
    "              \"The generation of random binary unordered trees\",\n",
    "              \"The intersection graph of paths in trees\",\n",
    "              \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "              \"Graph minors A survey\"]\n",
    "\n",
    "tokenized_docs = [gensim.utils.simple_preprocess(d) for d in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a function in gensim that trains your own embedding for you\n",
    "model = gensim.models.Word2Vec(tokenized_docs, size=10, # size refers to the desired dimension of our word vectors\n",
    "                               window=2, # window refers to the size of our context window\n",
    "                               min_count=1, \n",
    "                               sg=1)     # sg=1 means that we are using the Skip-gram architecture\n",
    "                                        # sg=0 means that we are using the CBOW architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.041 -0.028  0.037 -0.05   0.007 -0.034  0.023 -0.014  0.005 -0.021]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xianj\\Anaconda3\\envs\\metis\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(model['human'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training our own model with word2vec is pretty cool, but it requires us to have a large corpus of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, research groups at Stanford and Google have made their **pre-trained word embeddings publicly available for us to use!**\n",
    "\n",
    "Google's word2vec: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "\n",
    "GloVe:  https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "Just note that these model's will require **~4 GB of RAM to fit in memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to where the word2vec file lives\n",
    "google_vec_file = '/Users/adam/Downloads/GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load it!  This might take a few minutes...\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(google_vec_file, binary=True)\n",
    "\n",
    "# it is just loading all the different weights (embedding) into python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can access individual word vectors using a dictionary-like syntax\n",
    "model['Metis']\n",
    "\n",
    "# this is already trained. basically what we are doing is just a look-up table of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some cool results!\n",
    "\n",
    "model.most_similar('meeting' ,topn=8)  # give the top8 words that are closest to meeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar('president' ,topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's an analogy task!\n",
    "\n",
    "model.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=['Paris', 'England'], negative=['London'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using GloVe with Gensim requires a little extra leg work, but it's not too bad.\n",
    "The problem is that the file format that is publicly available doesn't play nice with Gensim.\n",
    "Luckily, Gensim provides a handy method of converting it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file = glove_dir = '/Users/adam/Downloads/glove.6B/glove.6B.100d.txt'\n",
    "\n",
    "w2v_output_file = '/Users/adam/Downloads/glove.6B/glove.6B.100d.txt.w2v'\n",
    "\n",
    "# The following utility converts file formats\n",
    "gensim.scripts.glove2word2vec.glove2word2vec(glove_file, w2v_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can load it!\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(w2v_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does it compare to the previous examples we did with word2vec?\n",
    "model.most_similar('meeting' ,topn=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar('president' ,topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn!\n",
    "\n",
    "- Using either word2vec or GloVe, what interesting analogies or relationships?\n",
    "\n",
    "- Given a short piece of text (like a tweet) what strategies can you think of to create a \"tweet vector\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
