{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pair Problem\n",
    "\n",
    "You are given documents as probability distributions over topics, and topics as probability distributions over words.\n",
    "\n",
    "Implement a function `make_doc` that takes a document (as `topic_probs`) and a number of words. The function should randomly generate a document by choosing a topic for each word using the document's topic probabilities and then choosing a particular word using that topic's word probabilities. The function should return a string containing all the generated document's words.\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "docs = [[0.98, 0.01, 0.01],\n",
    "        [0.01, 0.98, 0.01],\n",
    "        [0.01, 0.01, 0.98]]\n",
    "topics = [[ 0.4,      0.4,   0.01,        0.01,    0.01,       0.01,\n",
    "            0.1,     0.04,   0.01,        0.01],\n",
    "          [0.01,     0.01,    0.4,         0.4,    0.01,       0.01,\n",
    "            0.1,     0.04,   0.01,        0.01],\n",
    "          [0.02,     0.02,   0.01,        0.01,     0.4,        0.4,\n",
    "           0.02,      0.1,   0.01,        0.01]]\n",
    "words =  ['cat', 'kitten',  'dog',     'puppy',  'deep', 'learning',\n",
    "          'fur',  'image',  'GPU', 'asparagus']\n",
    "\n",
    "\n",
    "def make_doc(topic_probs, n_words):\n",
    "    raise NotImplementedError\n",
    "\n",
    "for doc in docs:\n",
    "    print make_doc(topic_probs=doc, n_words=10)\n",
    "\n",
    "#  Example output:\n",
    "## cat learning kitten image kitten cat deep image cat kitten\n",
    "## puppy puppy learning dog puppy dog dog puppy image dog\n",
    "## deep learning deep image deep deep deep deep learning learning\n",
    "```\n",
    "\n",
    "Extension:\n",
    "\n",
    "Update your `make_doc` function so that if `topic_probs` isn't specified, it will draw a random set of topic probabilities from a Dirichlet distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining variables\n",
    "docs = [[0.98, 0.01, 0.01],  # topic distributiion within the document\n",
    "       [0.01, 0.98, 0.01],\n",
    "       [0.01, 0.01, 0.98]]\n",
    "\n",
    "topics = [[ 0.4,      0.4,   0.01,        0.01,    0.01,       0.01,\n",
    "            0.1,     0.04,   0.01,        0.01],  # word distribution for each topic\n",
    "          [0.01,     0.01,    0.4,         0.4,    0.01,       0.01,\n",
    "            0.1,     0.04,   0.01,        0.01],\n",
    "          [0.02,     0.02,   0.01,        0.01,     0.4,        0.4,\n",
    "           0.02,      0.1,   0.01,        0.01]]\n",
    "\n",
    "words =  ['cat', 'kitten',  'dog',     'puppy',  'deep', 'learning',\n",
    "          'fur',  'image',  'GPU', 'asparagus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0.4, 'cat'),\n",
       "  (0.4, 'kitten'),\n",
       "  (0.01, 'dog'),\n",
       "  (0.01, 'puppy'),\n",
       "  (0.01, 'deep'),\n",
       "  (0.01, 'learning'),\n",
       "  (0.1, 'fur'),\n",
       "  (0.04, 'image'),\n",
       "  (0.01, 'GPU'),\n",
       "  (0.01, 'asparagus')],\n",
       " [(0.01, 'cat'),\n",
       "  (0.01, 'kitten'),\n",
       "  (0.4, 'dog'),\n",
       "  (0.4, 'puppy'),\n",
       "  (0.01, 'deep'),\n",
       "  (0.01, 'learning'),\n",
       "  (0.1, 'fur'),\n",
       "  (0.04, 'image'),\n",
       "  (0.01, 'GPU'),\n",
       "  (0.01, 'asparagus')],\n",
       " [(0.02, 'cat'),\n",
       "  (0.02, 'kitten'),\n",
       "  (0.01, 'dog'),\n",
       "  (0.01, 'puppy'),\n",
       "  (0.4, 'deep'),\n",
       "  (0.4, 'learning'),\n",
       "  (0.02, 'fur'),\n",
       "  (0.1, 'image'),\n",
       "  (0.01, 'GPU'),\n",
       "  (0.01, 'asparagus')]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def topic_word_proba_join(topics, words):\n",
    "    topic_words_proba_pair_list = list()\n",
    "    for topic in topics:\n",
    "        topic_words_proba_pair = list(zip(topic, words))\n",
    "        topic_words_proba_pair_list.append(topic_words_proba_pair)\n",
    "    return topic_words_proba_pair_list    \n",
    "\n",
    "topic_word_proba_join(topics, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_doc(message2, n_words=10):\n",
    "    \"\"\"\n",
    "    Randomly generates a document by:\n",
    "    - choosing a topic for each word using the document's topic probabilities.\n",
    "    \n",
    "    Then choosing a particular WORD using that TOPIC'S word probabilities. \n",
    "    \n",
    "    The function should return a string containing all the generated document's words.\n",
    "    \"\"\"\n",
    "    randomly_chosen_topic = np.randomchoice[message2]\n",
    "    \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proper Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Today's pair is to reinforce what we learned for last week\n",
    "\n",
    "# We learned a few topic modelling techniques\n",
    "# LDA, LSA, NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "# How do we gain intuition about LDA?\n",
    "\n",
    "# Imagine yourself as a writer. You want to write about a few topics - you won't write on all of them\n",
    "# Iternally, you have a whole reach of vocabulary you want to use. \n",
    "# But if you're writing on a topic about cats and dogs, would you want to use a topic on deep learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What humans see in real life, we infer what are the topics that are being written about from the WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this pair, we are trying to do it in the opposite direction.\n",
    "# We give you the distribution of topics, and you create a word from that distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# topic distributiion within the document\n",
    "docs = [[0.98, 0.01, 0.01],  # first entry has 98% chance of being first topic, next entry has 0.01% chance of being second toppic.. etc\n",
    "       [0.01, 0.98, 0.01],  # second row, second document (dogs and puppies)\n",
    "       [0.01, 0.01, 0.98]] # thidr row, third document (deep learning)\n",
    "\n",
    "topics = [[ 0.4,      0.4,   0.01,        0.01,    0.01,       0.01,\n",
    "            0.1,     0.04,   0.01,        0.01],  # word distribution for each topic\n",
    "          [0.01,     0.01,    0.4,         0.4,    0.01,       0.01,\n",
    "            0.1,     0.04,   0.01,        0.01],\n",
    "          [0.02,     0.02,   0.01,        0.01,     0.4,        0.4,\n",
    "           0.02,      0.1,   0.01,        0.01]]\n",
    "\n",
    "words =  ['cat', 'kitten',  'dog',     'puppy',  'deep', 'learning',\n",
    "          'fur',  'image',  'GPU', 'asparagus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.36552452, 0.12355184, 0.51092365])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.dirichlet(np.ones(len(topics)))\n",
    "\n",
    "# just to remind us what distributions we are using\n",
    "# Give you probabilities for multiclass but will sum up to one\n",
    "\n",
    "# in this code, we are assuming any of them are equally likely to be chosen\n",
    "# We are just randomizing a document-topic probability distribution\n",
    "\n",
    "# However we defined it above in docs already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones(len(topics))   # they are equally likely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "object too deep for desired array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-5589caf6562d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirichlet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.dirichlet\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: object too deep for desired array"
     ]
    }
   ],
   "source": [
    "np.random.dirichlet(np.ones(np.array([5,1,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA is doing the reverse of what we do.\n",
    "# We use the WORDS in a document to generate an understanding of the TOPICS in a DOCUMENT/ARTICLE\n",
    "\n",
    "# LDA uses a set of probabilities from the DOCUMENT-TOPIC distribution get TOPICS, \n",
    "# and then use the TOPIC-WORD distribution\n",
    "# to get WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_doc(topic_probs=None, n_words=40, verbose=True):\n",
    "    if topic_probs is None:\n",
    "        topic_probs = np.random.dirichlet(np.ones(len(topics)))\n",
    "    if verbose:\n",
    "        print('topic_probs:', topic_probs)\n",
    "    results = []\n",
    "    for _in range:\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
