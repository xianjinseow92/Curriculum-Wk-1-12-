{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Testing\n",
    "\n",
    "In this lesson, we will be looking at designing an A/B test. \n",
    "\n",
    "### Goals\n",
    "- Review hypothesis testing, type-I and type-II errors, and in particular what statistical guarantees hypothesis testing makes.\n",
    "- Introduce the concept of statistical power, and how it is used to determine sample size\n",
    "- Best practices for \"traditional\" A/B tests\n",
    "- Introduce Multi-Armed Bandits as 'ongoing' A/B tests without a control\n",
    "- Discuss advantages and disadvantages of A/B tests\n",
    "\n",
    "### Code\n",
    "- Introduce `namedtuples`\n",
    "- Do power calculations \"by hand\" (i.e. explicit formula)\n",
    "- Introduce \n",
    "- Demonstrate results via simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning outcomes \n",
    "\n",
    "- Be able to simulate the process of a hypothetical experiment with given effect sizes (including 0 effect size -- an A/A test)\n",
    "- Be able to calculate the sample size needed for a given significance, power, and minimum detectible size\n",
    "- Be able to take the results of an experiment (at a given Type-I error $\\alpha$) and determine if the effect was statistically significant. Note this is not a new goal; this is a review from the Hypothesis Testing lecture.\n",
    "- Be able to use `namedtuples`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study\n",
    "Let's say we made a minor modification to a website, or to an ad. For example, we might have an email campaign with two slightly different headings\n",
    "\n",
    "|Variation | Text | Clicks | Impressions |\n",
    "|---|:---:|---|---|\n",
    "|A|\t\"Great savings inside\" |127 | 5734 |\n",
    "|B|\t\"Save up to 10% on your next order\" |174\t| 5851 |\n",
    "\n",
    "Here \"Impression\" means someone saw the variation, and \"click\" means they actually opened it and read it. \n",
    "\n",
    "How effective were the two campaigns? For the two campaigns, we have slightly different measured click-through rates (CTRs). Let' s calculate them below. \n",
    "\n",
    "### Data structure: an aside\n",
    "We will need a way to keep track of the clicks and the impressions. We _could_ store them in a tuple, like so:\n",
    "```python\n",
    "A = (127, 5734)\n",
    "```\n",
    "but then we need to remember that `A[0]` is the number of clicks and `A[1]` is the number of impressions.\n",
    "\n",
    "A `namedtuple` `variation` is defined so that the fields have names:\n",
    "```python\n",
    "A = variation(clicks=127, impressions=5734)\n",
    "```\n",
    "Here both `A[0]`  and  `A.clicks` are equal to 127,  and `A[1] = A.impressions = 5734`. By writing out field names, it is more difficult to accidently assign the numbers in different orders in different variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion rate of variation A is 0.022148587373561214\n",
      "Conversion rate of variation B is 0.029738506238249873\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "variation = namedtuple('variation', 'clicks impressions')\n",
    "\n",
    "A = variation(clicks=127, impressions=5734)\n",
    "B = variation(clicks=174, impressions=5851)\n",
    "\n",
    "for name, v in [('A', A), ('B', B)]:\n",
    "    rate = v.clicks/v.impressions\n",
    "    print('Conversion rate of variation {name} is {rate}'.format(name=name, rate=rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that campaign B (\"Save up to 10% on your next order\") was more effective at getting people to open the ad. Is this difference significant? To answer this, we can ask the following question:\n",
    "\n",
    "* If the ads were equally effective, what is the chance that variation B would beat variation A by this much?\n",
    "\n",
    "There are three different approaches we can take to this:\n",
    "1. Direct simulation\n",
    "2. A normal distribution test\n",
    "3. A $\\chi^2$ (chi-square) analysis\n",
    "\n",
    "We will investigate the first and second approaches here. A $\\chi^2$ analysis is a different way of reaching the same conclusion on large datasets. In small data sets (i.e. those typically not seen by data scientists) correcting for the degrees of freedom in a $\\chi^2$ analysis is important, but for large datasets (or sets with 1 \"degree of freedom\") it gives the same results as method 2. An interested reader can consult any intro stats book ([this is a free one available online](http://onlinestatbook.com/2/chi_square/Chi_Square.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance by simulation (post-hoc)\n",
    "\n",
    "1. Assume that both variations perform the same. Find the common conversion rate `conv_rate`\n",
    "2. Calculate if 5734 people come to variation A, what fraction $p_A$ of them convert (random sample with probability p)\n",
    "3. Calculate if 5851 people come to variation B, what fraction $p_B$ of them convert (random sample with probability p)\n",
    "4. Calculate the difference $|p_B - p_A|$. \n",
    "5. Repeat the steps 2 -- 4 a large number of times. Count the fraction of times that $|p_B - p_A|$ is bigger than the observed value (0.02974-0.02215 = 0.00759)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0107"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def draw_clicks_from_n_samples(impressions, prob_click, num_trials):\n",
    "    return np.random.binomial(impressions, prob_click, size=(num_trials, ))\n",
    "\n",
    "\n",
    "def draw_p_sample_from_n_samples(impressions, prob_click, num_trials):\n",
    "    return draw_clicks_from_n_samples(impressions, prob_click, num_trials)/impressions\n",
    "\n",
    "# Ensures that p value is reproducible and matches text\n",
    "np.random.seed(42)\n",
    "\n",
    "num_trials = 10000\n",
    "imp_A, imp_B = 5734, 5851\n",
    "click_A, click_B = 127, 174\n",
    "\n",
    "difference_in_rate = abs(click_A/imp_A - click_B/imp_B)\n",
    "\n",
    "# What was the total conversion rate?\n",
    "conv_rate = (click_A + click_B)/(imp_A + imp_B)\n",
    "\n",
    "p_A_array = draw_p_sample_from_n_samples(imp_A, conv_rate, num_trials)\n",
    "p_B_array = draw_p_sample_from_n_samples(imp_B, conv_rate, num_trials)\n",
    "\n",
    "result_by_chance = (abs(p_B_array - p_A_array) > difference_in_rate)\n",
    "sum(result_by_chance)/len(result_by_chance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the probability that we see a difference $|p_a - p_b|$ in our sample, if the two underlying samples are exactly the same (i.e. the difference is just a statistical fluke). This quantity is known as the _p value_.\n",
    "\n",
    "Our statement is \"if A and B performed equally effectively, the probability that samples this large would show an effect this size is 1.07%\".\n",
    "\n",
    "Let's write this in a function, so we can access the $p$ value given the results of an experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_value_from_simulation(num_simulations, variation_A, variation_B):\n",
    "    imp_A, imp_B = variation_A.impressions, variation_B.impressions\n",
    "    click_A, click_B = variation_A.clicks, variation_B.clicks\n",
    "\n",
    "    p = (click_A + click_B)/(imp_A + imp_B)\n",
    "\n",
    "    difference_in_rate = abs(click_A/imp_A - click_B/imp_B)\n",
    "\n",
    "    p_A_array = draw_p_sample_from_n_samples(imp_A, p, num_simulations)\n",
    "    p_B_array = draw_p_sample_from_n_samples(imp_B, p, num_simulations)\n",
    "    result_by_chance = (abs(p_B_array - p_A_array) > difference_in_rate)\n",
    "    return sum(result_by_chance)/len(result_by_chance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0094"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that simulation is a random process.\n",
    "# Running this multiple times will give different results\n",
    "get_p_value_from_simulation(10000, A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis by normal distribution (post-hoc)\n",
    "\n",
    "Some notation:\n",
    "- the _sample proportion_ $p_A$ is what we measure when we do the experiment\n",
    "- the _population proportion_ $\\pi_A$ is the actual value of the mean (if we did an infinite number of samples)\n",
    "- If we have a sample size of $N$ (and $N \\gg 30$, as well as a couple of other technical points) then the proportion $p_A$ should be normally distributed with a mean of $\\pi_A$ and a variance of $\\pi_A(1-\\pi_A)/N$\n",
    "\n",
    "With the sample proportions normally distributed, the _difference_ between sample proportions will also be normally distributed. We can use the _z-test_ to calculate the probability of seeing an effect \"as large as the one seen\". Specifically, below we look at a two-tailed test (which tests for the _magnitude_ of the effect). I advocate using a two-tailed test unless you have a good reason to do otherwise.\n",
    "\n",
    "We are looking for the difference between $\\pi_A$ and $\\pi_B$. The null hypothesis is that these rates are the _same_. The variance in the difference of the _sample proportion_ $p_A - p_B$ is \n",
    "$$\\sigma^2_{\\text{diff}} = \\frac{\\pi_A(1-\\pi_A)}{N_A} + \\frac{\\pi_B(1-\\pi_B)}{N_B}$$\n",
    "\n",
    "We want to how likely it is that $|p_A - p_B|$ is bigger than the value actually found in the experiment, if we assume $\\pi_A = \\pi_B$. Our $z$-score is\n",
    "$$|z| = \\frac{|p_A - p_B|}{\\sqrt{\\frac{p_A(1-p_A)}{N_A} + \\frac{p_B(1-p_B)}{N_B}}}$$\n",
    "\n",
    "We can use the CDF to find the probability that we get a $z$ score greater in magnitude than the one found. \n",
    "![normal distribution](image/normal.png)\n",
    "We want to find the total shaded area. The area to the left of $b$ is \n",
    "$$P(z > b) = 1 - cdf(b)$$\n",
    "Since the area less than $-b$ is the same (symmetry of the normal distribution with mean 0), we know the $P$ value is\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{p-value} = P(|z| > b) = P(z<-b) + P(z>b) = 2 P(z > b) = 2(1 - cdf(b))\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010242813991217181"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "p_A, p_B = A.clicks/A.impressions, B.clicks/B.impressions\n",
    "p = (A.clicks + B.clicks)/(A.impressions + B.impressions)\n",
    "variance = p*(1-p)/A.impressions + p*(1-p)/B.impressions  # null hypothesis - only one p\n",
    "\n",
    "abs_z = abs(p_A - p_B)/np.sqrt(variance)\n",
    "\n",
    "p_value = 2*(1-norm.cdf(abs_z))\n",
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function to encapsulate this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_value_analytic(variationA, variationB):\n",
    "    p_A, p_B = (variationA.clicks/variationA.impressions,\n",
    "                variationB.clicks/variationB.impressions)\n",
    "    p = (variationA.clicks + variationB.clicks)/(variationA.impressions + variationB.impressions)\n",
    "    variance = p*(1-p)/variationA.impressions + p*(1-p)/variationB.impressions\n",
    "\n",
    "    abs_z = abs(p_A - p_B)/np.sqrt(variance)\n",
    "\n",
    "    p_value = 2*(1-norm.cdf(abs_z))\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010242813991217181"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_p_value_analytic(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-hoc vs apriori\n",
    "\n",
    "The tests described so far are how we find the $p$-value _after the experiment has concluded_. The $p$-value tells us whether the result is statistically significantly different from chance (specifically, it tells us the chance of a result this large happening if we assume there is no difference in the variations). \n",
    "\n",
    "There are lots of misconceptions about $p$-values. This is a good time to address one of them: the idea that the lower the $p$-value, the better one variation is than the other. This is _mostly false_. If two variations are different (and let's face it, there is always __some__ difference), then for a large enough sample size $N$, you can find a tiny $p$-value. At a **fixed** sample size $N$, it is true that bigger differences in proportion lead to smaller $p$-values (which is the origin of this misconception).\n",
    "\n",
    "Why do we care about this? When designing an A/B test, one of the questions we should answer before collecting any data is\n",
    "> We have two variations (A and B). Before the experiment, we want to know how long we should run the experiment before drawing a conclusion. How long should that be?\n",
    "\n",
    "i.e. we don't know the size of the experiment $N$; that is what we are trying to determine. The longer we run the experiment, the more we are \"missing out\" on the opportunity costs of employing the \"better\" variation. We know we can detect the difference if we run the experiment long enough, but we should know \"is it worth it?\"\n",
    "\n",
    "To answer this, we need to know\n",
    "- what difference in proportion is worth measuring?\n",
    "- the expected rate of visitors at our site\n",
    "- the period of cycles within our buisness (often weekly or monthly)\n",
    "\n",
    "Then we can determine how long we should run the experiment in order to detect the difference.\n",
    "\n",
    "### How to do it: statistical power\n",
    "\n",
    "The $p$-value asks the question: if there is no difference, what is the probability that our experiment finds a result at least as big as the one we found? If we declare a cutoff on the $p$-value of 5%, we are claiming that we are willing to make a __Type I error__ 5% of the time. We calculate $p$-values after the experiment. The cutoff for the $p$-value is often denoted $\\alpha$.\n",
    "\n",
    "The _power_ $\\beta$ asks the question: if there __is__ a difference of size $\\Delta p$, what is the probability that our experiment (with the cutoff) finds it? In order to answer this question, we have to give the full experimental procedure. It is related to the chance of a __Type II error__, but isn't the same as it.\n",
    "\n",
    "Let's take an experiment with $\\alpha = 5\\%$ (i.e. we claim that we don't have evidence for the two variations being different if $p > 0.05$, and if $p<0.05$ we claim one a winner but acknowledge we would get this result 5% of the time if the variations are the same). Using knowledge of the normal distribution, the procedure is:\n",
    "1. Calculate the $z$-score under the null hypothesis:\n",
    "\\begin{equation*}\n",
    "|z_0| = \\frac{|p_A - p_B|}{\\sqrt{\\frac{p(1-p)}{N_A} + \\frac{p(1-p)}{N_b}}}\n",
    "\\end{equation*}\n",
    "Here $p$ is the pooled probability.\n",
    "2. If $|z_0| < 1.96$, then we claim no winner\n",
    "3. If $|z_0| \\geq 1.96$, we declare the winner to be the higher probability.\n",
    "\n",
    "#### The math background\n",
    "\n",
    "Let's analyize this same experiment under the hypothesis that there is a difference $\\Delta p$, i.e. $\\pi_B - \\pi_A = \\Delta\\pi$. We want to know, __assuming there is a difference of size $\\Delta \\pi$__, what is the chance that the experiment above gets $|z_0| > 1.96$? \n",
    "\n",
    "If $z_0= 1.96$, and assuming $p_B$ is the variation that did better, this tells us \n",
    "$$p_B - p_A = 1.96 \\sqrt{\\frac{p(1-p)}{N_A} + \\frac{p(1-p)}{N_B}}$$\n",
    "This is the \"critical value\" -- differences bigger than this will lead to acceptance, while difference smaller than this lead to rejection. What is the $z$ score of this difference under the hypothesis that there is a difference?\n",
    "\\begin{equation*}\n",
    "|z_2| = \\frac{(\\pi_B - \\pi_A) - (p_B - p_A)}{\\sigma_{\\Delta p}} = \\frac{\\Delta \\pi - 1.96 \\sqrt{\\frac{p(1-p)}{N_A} + \\frac{p(1-p)}{N_B}}}{\\sqrt{\\frac{p_A(1-p_A)}{N_A} + \\frac{p_B(1-p_B)}{N_B}}}\n",
    "\\end{equation*}\n",
    "We can simplify this a lot if we assume that $N_A = N_B$:\n",
    "\\begin{equation*}\n",
    "|z_2| =  \\frac{\\Delta \\pi - 1.96 \\sqrt{\\frac{2p(1-p)}{N}}}{\\sqrt{\\frac{p_A(1-p_A)+p_B(1-p_B)}{N}}} = \\frac{\\Delta \\pi \\sqrt{N} - 1.96 \\sqrt{2p(1-p)}}{\\sqrt{p_A(1-p_A)+p_B(1-p_B)}}\n",
    "\\end{equation*}\n",
    "We now solve for $N$, the number needed for each variation:\n",
    "\n",
    "\\begin{equation*}\n",
    "N = \\frac{(|z_2|\\sqrt{p_A(1-p_A) + p_B(1-p_B)} + 1.96\\sqrt{2p(1-p)})^2}{(\\Delta \\pi)^2}\n",
    "\\end{equation*}\n",
    "\n",
    "#### The procedure\n",
    "\n",
    "So what does all that mean? Let's walk through an example. Let's say that we have an email campaign with headline \"Great Savings inside\" that gets about 2% click-throughs. We want to run an experiment with an alternative heading (\"Save up to 10% on your next order\") at a confidence level of $\\alpha = 5\\%$. We want to be able to detect differences of 1 _percentage point_ at least $80\\%$ of the time.\n",
    "\n",
    "Our question: How many emails $N$ of each variation do we need to run the experiment?\n",
    "\n",
    "Our answer:\n",
    "1. Figure out $z_2$, so that $P(z > z_2) = 0.8$. In this case, we use `z2 = norm.ppf(1-0.8) = -0.8416`, meaning that you have a z-score higher than $-0.8416$ in 80% of trials.\n",
    "2. Get parameters: \n",
    "  We have $p_A = 2\\%$ (historical), and $p_B = 3\\%$ (detect a 1\\% improvement). We have $\\Delta \\pi = 1\\%$ as well. The pooled conversion rate is $p = (p_A + p_B)/2 = 2.5\\%$. It is a simple average because we are assuming equal numbers of people to our variations.\n",
    "3. Do the calcuation, in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3825.1497221026216"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha, beta = 0.05, 0.8\n",
    "p_A = 0.02\n",
    "DeltaPi = 0.01\n",
    "\n",
    "p_B = p_A + DeltaPi\n",
    "p = 0.5*(p_A + p_B)\n",
    "\n",
    "z2 = abs(norm.ppf(1-beta))\n",
    "z_crit = norm.ppf(1-alpha/2)  # this is 1.96\n",
    "\n",
    "root_N = (z2*np.sqrt(p_A*(1-p_A) + p_B*(1-p_B)) + z_crit*np.sqrt(2*p*(1-p))) / DeltaPi\n",
    "N = root_N**2\n",
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e. you should plan on having 3826 people in variationA and 3826 people in variationB. If we have fewer than this number of people then we probably will conclude that there is no difference. Note that larger differences (larger $\\Delta\\pi$) are easier to detect than smaller differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_function(p_A_base, DeltaPi, alpha=0.05, beta=0.8):\n",
    "    p_B = p_A_base + DeltaPi\n",
    "    p = 0.5*(p_A_base + p_B)\n",
    "\n",
    "    z2 = abs(norm.ppf(1-beta))\n",
    "    z_crit = norm.ppf(1-alpha/2)\n",
    "    root_N = (z2*np.sqrt(p_A_base*(1-p_A_base) + p_B*(1-p_B)) + z_crit*np.sqrt(2*p*(1-p))) / DeltaPi\n",
    "    return int(np.ceil(root_N**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3826"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same idea as hypothetical above, wrapped in a function\n",
    "power_function(0.02, 0.01, alpha=0.05, beta=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why $(1-\\beta)$ , the Type II error, isn't the probability of incorrectly accepted the alternative\n",
    "\n",
    "You can see a lot of statements that if $\\beta$ is the statistical power, then $1-\\beta$ is \"the probability that we incorrectly accept the alternative hypothesis\". This isn't quite right. The way we defined it, $1-\\beta$ is the probability of incorrectly accepting the alternative hypothesis _if we know the difference in proportion is exactly our threshold of caring_. If it is likely that the difference in proportion is higher than our threshold of caring, then $1-\\beta$ is an overestimate of this error (because the difference is easier to find than we cared about). Similiarly, if the difference is smaller than expected, $1-\\beta$ is an underestimate of this error.\n",
    "\n",
    "The actual probability of incorrectly accepting the altnerative hypothesis depends on the _prior_ distribution of probability differences. We haven't specified a prior here, so we cannot calculate this error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The built-in method\n",
    "\n",
    "**Statsmodels has a power method built into it as well.**  \n",
    "Different power calculations make slightly different assumptions about how \"normal\" they can assume the distribution to be, so the number is similar to (but not exactly the same) as the one we have above. \n",
    "\n",
    "Calculating the number of people needed per variation requires two steps:\n",
    "1. A call to `proportion_effectsize(prop1, prop2)`, where `prop1` is the control rate and `prop2` is the rate you want to detect.\n",
    "\n",
    "    This returns a normalized effect size. From the help of this function\n",
    "    >Effect size for `normal` is defined as ::\n",
    "    >\n",
    "    >    2 * (arcsin(sqrt(prop1)) - arcsin(sqrt(prop2)))\n",
    "    >\n",
    "    >I think other conversions to normality can be used, but I need to check.\n",
    "    \n",
    "    I prefer the \"home rolled\" version, as the author of this function isn't certain what the function is doing!\n",
    "2. A call to `smp.tt_ind_solve_power`. \n",
    "    \n",
    "    Note this is a **terrible** function, and violates many principles of 'clean code'. You are required to pass in one of the arguments as `None`, and that is the value that is returned by the function. So you specify all the values you want except one, and that is returned. **PLEASE DON'T WRITE YOUR OWN CODE LIKE THIS!!!**\n",
    "    \n",
    "    If we want to solve for the number in each group, pass `nobs1 = None`. This is the number of people needed in group 1. (The number needed in group 2 is `ratio * nobs1`, so setting `ratio = 1.0` ensures there are two equal sized groups)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The effect size is -0.06437191206463305\n",
      "The number of people needed per group is 3790\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.stats.power as smp\n",
    "from statsmodels.stats.proportion import proportion_effectsize\n",
    "\n",
    "\n",
    "effect_size = proportion_effectsize(0.02, 0.03)\n",
    "print(\"The effect size is\", effect_size)\n",
    "num_per_group = smp.tt_ind_solve_power(effect_size=effect_size, nobs1=None, alpha=0.05, power=0.8, ratio=1.0)\n",
    "print(\"The number of people needed per group is\", int(np.ceil(num_per_group)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 (you do, 3 mins)\n",
    "\n",
    "Today's pair problem had you try and detect the difference between two processes, where we \"knew\" the error rates ahead of time: 5% and 4.8%. \n",
    "\n",
    "Instead, let's say we have a standard process with an error rate of 5% (known). We have an alternative process, and we want to be able to detect if the error rate is different by more than 0.2 percentage points (i.e. `p1 < 4.8%` or `p1 > 5.2%`).\n",
    "\n",
    "* How many samples should we have if we want the probability of a type I error to be 5%, and the power to be 80%?\n",
    "\n",
    "Answer this question two ways:\n",
    "* Using the `power_function` method we wrote\n",
    "* Using the methods from `stats_models`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: check for understanding (you do, 1 min)\n",
    "\n",
    "* If you took the sample size above, and ran 10000 simulations with `p0=0.05` and `p1=0.048`, what percentage of them would you expect to **not** detect a difference?\n",
    "* If you took the sample size above, and ran 10000 simulations with `p0=p1=0.05`, what percentage of the them would you expect **to** detect a difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 (you do, 3 mins)\n",
    "\n",
    "Run a 10000 simulations with `p0=0.05` and `p1=0.048`. For each simulation, tally whether your anaylsis technique would **not** flag the two processes as having a significant difference. Compare your answer to the prediction you made in exercise 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 (you do, 1 min)\n",
    "\n",
    "Run a 10000 simulations with `p0=p1=0.05`. For each simulation, tally whether your anaylsis technique **would** flag the two processes as having a significant difference. Compare your answer to the prediction you made in exercise 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
