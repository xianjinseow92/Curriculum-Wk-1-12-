{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pair Problem\n",
    "\n",
    "### Some math to get us started\n",
    "* What is the derivative of $x^{2}$?\n",
    "* What is the derivative of log(x)?\n",
    "* What is the derivative of $e^{x}$?\n",
    "* z = f(y) & y = g(x) what is $\\frac{dz}{dx}$?\n",
    "\n",
    "### Gradient Descent for linear regression (OLS) from scratch\n",
    "\n",
    "Today we'll be writing stochastic gradient descent for linear regression from scratch! Woo! \n",
    "\n",
    "Read the notebook for starter code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting functions\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"ticks\")\n",
    "plt.style.use(u'ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Update Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update Rule**: for each step update the current $x$ coordinate $x_n$ via \n",
    "\n",
    "$$x_{n+1} = x_n - \\alpha\\frac{df}{dx}(x_n)$$, \n",
    "where $\\alpha$ is a constant called the **learning rate**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_animated(alpha=.1, tol=0.001, x=np.random.uniform(-6,6))\n",
    "    for _ in range(100):     # '_' allows you to save memory space because it doesn't save a variable\n",
    "\n",
    "        # simple animated plotting\n",
    "        plot_tangent_at_pt(f, df_dx, x, np.linspace(-6,6))    \n",
    "        display.clear_output(wait=True)\n",
    "        time.sleep(.5)\n",
    "\n",
    "        \n",
    "        ###########################   gradient descent UPDATE RULE (the updating condition)\n",
    "        x_delta = alpha * df_dx(x) # alpha = learning rate, df_dx(x) = 2*x (we have decided this is the gradient)\n",
    "        x = x - x_delta\n",
    "        ############################\n",
    "        \n",
    "        \n",
    "        # check stopping criterion\n",
    "        if np.abs(x_delta) < tol: # np.abs = absolute value of x_delta. If the value is smaller than the tolerance, then we stop the code\n",
    "            break    \n",
    "\n",
    "            \n",
    "# Parameters into the SGD plotting function\n",
    "alpha = .1 # learning rate\n",
    "tol = .0001 # tolerance level for stopping criterion\n",
    "x = np.random.uniform(-6,6) # random starting point. #-6 = min value, 6= max value\n",
    "\n",
    "sgd_animated(alpha, tol, x)\n",
    "\n",
    "display.clear_output()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create fake data for later testing\n",
    "\n",
    "def gen_data(rows = 200, gen_coefs = [2,4], gen_inter = 0):\n",
    "    X = np.random.rand(rows,len(gen_coefs))\n",
    "    y = np.sum(np.tile(np.array(gen_coefs),(X.shape[0],1))*X,axis=1)\n",
    "    y = y + np.random.normal(0,0.5, size=X.shape[0])\n",
    "    y = y + gen_inter\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x, data_y = gen_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update Rule**: for each step update the current $x$ coordinate $x_n$ via \n",
    "\n",
    "$$x_{n+1} = x_n - \\alpha\\frac{df}{dx}(x_n)$$, \n",
    "where $\\alpha$ is a constant called the **learning rate**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (<ipython-input-34-4edbe9a8e4f8>, line 157)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-34-4edbe9a8e4f8>\"\u001b[1;36m, line \u001b[1;32m157\u001b[0m\n\u001b[1;33m    return (np.dot(X,self.coef_[1:]) +\\  # this is the dot product of the betas * values of x's\u001b[0m\n\u001b[1;37m                                                                                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "class SGD_LR:\n",
    "    \n",
    "    def __init__(self, n_iter=100, alpha=0.01, verbose=False, return_steps=False, fit_intercept=True, dynamic=False):\n",
    "        \"\"\"\n",
    "        Stochastic Gradient Descent Algorithm, with OLS cost function.\n",
    "        ---\n",
    "        KWargs:\n",
    "        \n",
    "        n_iter: number of epochs to run in while fitting to the data. Total number of steps\n",
    "        will be n_iter*X.shape[0]. \n",
    "        \n",
    "        alpha: The learning rate. Moderates the step size during the gradient descent algorithm.\n",
    "        \n",
    "        verbose: Whether to print out coefficient information during the epochs\n",
    "        \n",
    "        return_steps: If True, fit returns a list of the coefficients at each update step for diagnostics\n",
    "        \n",
    "        fit_intercept: If True, an extra coefficient is added with no associated feature to act as the\n",
    "                       base prediction if all X are 0.\n",
    "                       \n",
    "        dynamic: If true, an annealing scedule is used to scale the learning rate. \n",
    "        \"\"\"\n",
    "        self.coef_ = None\n",
    "        self.trained = False\n",
    "        self.n_iter = n_iter\n",
    "        self.alpha_ = alpha\n",
    "        self.verbosity = verbose\n",
    "        self._return_steps = return_steps\n",
    "        self._fit_intercept = fit_intercept\n",
    "        self._next_alpha_shift = 0.1 # Only used if dynamic=True\n",
    "        self._dynamic = dynamic\n",
    "        \n",
    "    def update(self, x, error):\n",
    "        \"\"\"\n",
    "        Calculating the change of the coeficients for SGD. This is the derivative of the cost \n",
    "        function. B_i = B_i - alpha * dJ/dB_i. If fit_intercept=True, a slightly different \n",
    "        value is used to update the intercept coefficient, since the associated feature is \"1.\"\n",
    "        ---\n",
    "        Inputs:\n",
    "        \n",
    "        data_point: A single row of the feature matrix. Since this is Stochastic, batches are not allowed.\n",
    "        \n",
    "        error: The residual for the current data point, given the current coefficients. Prediction - True\n",
    "        for the current datapoint and coefficients.\n",
    "        \"\"\"\n",
    "        step = self.alpha_*error*x\n",
    "        if self._fit_intercept:\n",
    "            self.coef_[1:] = self.coef_[1:] - step\n",
    "            self.coef_[0] = self.coef_[0] - self.alpha_ * error\n",
    "        else:\n",
    "            self.coef_ = self.coef_ - step\n",
    "        \n",
    "    def shuffle_data(self, X, y):\n",
    "        \"\"\"\n",
    "        Given X and y, shuffle them together to get a new_X and new_y that maintain feature-target\n",
    "        correlations. \n",
    "        ---\n",
    "        Inputs:\n",
    "        \n",
    "        X: A numpy array of any shape\n",
    "        y: A numpy array of any shape\n",
    "        \n",
    "        Both X and y must have the same first dimension length.\n",
    "        \n",
    "        Returns:\n",
    "        X,y: two numpy arrays\n",
    "        \"\"\"\n",
    "        assert len(X) == len(y)\n",
    "        permute = np.random.permutation(len(y))\n",
    "        return X[permute], y[permute]\n",
    "        \n",
    "    def pandas_to_numpy(self, x):\n",
    "        \"\"\"\n",
    "        Checks if the input is a Dataframe or series, converts to numpy matrix for\n",
    "        calculation purposes.\n",
    "        ---\n",
    "        Input: X (array, dataframe, or series)\n",
    "        \n",
    "        Output: X (array)\n",
    "        \"\"\"\n",
    "        if type(x) == type(pd.DataFrame()) or type(x) == type(pd.Series()):\n",
    "            x = x.as_matrix()\n",
    "        return x  \n",
    "    \n",
    "    \n",
    "    def dynamic_learning_rate_check(self, epoch):\n",
    "        \"\"\"\n",
    "        If dynamic=True, shrink the learning rate by a factor of 2 after every 10% of\n",
    "        the total number of epochs. This should cause a more direct path to the global \n",
    "        minimum after the initial large steps.\n",
    "        ---\n",
    "        Inputs: epoch (int,float), the current iteration number. \n",
    "        \"\"\"\n",
    "        percent_of_epochs = float(epoch)/float(self.n_iter)\n",
    "        if percent_of_epochs > self._next_alpha_shift:\n",
    "            self._next_alpha_shift += 0.1\n",
    "            self.alpha_ = self.alpha_/2.\n",
    "            \n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Actually trains the model. Given feature-target combinations, gradient descent is performed\n",
    "        using the optimization stepping given in the 'update' function. At present, all epochs are \n",
    "        completed, as no tolerance is set. The learning rate is currently fixed.\n",
    "        ---\n",
    "        Inputs: \n",
    "            X (array, dataframe, series), The features to regress on using SGD\n",
    "            y (array, series), Must be a 1D set of targets.\n",
    "        Outputs:\n",
    "            steps (optional): If return_steps=True, a list of the evolution of the coefficients is returned\n",
    "        \"\"\"\n",
    "        X = self.pandas_to_numpy(X)\n",
    "        y = self.pandas_to_numpy(y)\n",
    "        self.coef_ = self.init_coef(X)\n",
    "        if self._return_steps:\n",
    "            steps = []\n",
    "            steps.append(np.copy(self.coef_))\n",
    "            \n",
    "        for epoch in range(self.n_iter):   # Outer loop runs 10 times\n",
    "            shuf_X, shuf_y = self.shuffle_data(X,y)\n",
    "            if self.verbosity:\n",
    "                print(\"Epoch \", epoch, \", Coeff: \", self.coef_)\n",
    "                \n",
    "            for data, true in zip(shuf_X,shuf_y):  # Only call in the model is at this paret\n",
    "                pred = self.predict(data)  # Prediction function below\n",
    "                error = pred - true        # \n",
    "                self.update(data, error)\n",
    "                if self._return_steps:\n",
    "                    steps.append(np.copy(self.coef_))\n",
    "            if self._dynamic:\n",
    "                self.dynamic_learning_rate_check(epoch)\n",
    "        if self._return_steps:\n",
    "            return steps\n",
    "            \n",
    "    def init_coef(self, X):\n",
    "        \"\"\"\n",
    "        Returns the initial starting values for the coefficients. At present, these are randomly\n",
    "        set. If fit_intercept = True, an extra coefficient is generated. \n",
    "        ---\n",
    "        Input: X, Feature matrix. Needed to decide how many coefficients to generate.\n",
    "        \"\"\"\n",
    "        if self._fit_intercept:\n",
    "            return np.random.rand(X.shape[1]+1)\n",
    "        return np.random.rand(X.shape[1])\n",
    "    \n",
    "    \n",
    "    def predict(self, X):  \n",
    "        \"\"\"\n",
    "        Returns a prediction for a new data set, using the model coefficients.\n",
    "        ---\n",
    "        Input: \n",
    "            X (dataframe, array): The new feature set. Must be the same number of columns\n",
    "            as the initial training features. \n",
    "        Output:\n",
    "            prediction (array): The dot product of the input data and the coeficients.\n",
    "        \"\"\"\n",
    "        if not self.coef_.all():\n",
    "            raise ValueError(\"Coefficients not defined, must fit() before predict().\")\n",
    "        if self._fit_intercept:\n",
    "            return (np.dot(X,self.coef_[1:]) +\\  # this is the dot product of the betas * values of x's\n",
    "                            self.coef_[0])  # this is the intercept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
