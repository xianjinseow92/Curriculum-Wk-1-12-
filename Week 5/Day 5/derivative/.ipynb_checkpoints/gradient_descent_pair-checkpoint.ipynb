{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Stochastic Gradient Descent from Scratch! \n",
    "\n",
    "Use the following starter code to create stochastic gradient descent for OLS from scratch. After you've done that, use your new function to run a linear regression on randomized data. \n",
    "\n",
    "We'll give you a function to generate some fake data for you to test your GD on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create fake data for later testing\n",
    "\n",
    "def gen_data(rows = 200, gen_coefs = [2,4], gen_inter = 0):\n",
    "    X = np.random.rand(rows,len(gen_coefs))\n",
    "    y = np.sum(np.tile(np.array(gen_coefs),(X.shape[0],1))*X,axis=1)\n",
    "    y = y + np.random.normal(0,0.5, size=X.shape[0])\n",
    "    y = y + gen_inter\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD_for_OLS:\n",
    "    \n",
    "    def __init__(self, n_iter=100, alpha=0.001):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        n_iter: number of epochs to run to fit the data. Total number of steps\n",
    "        will be n_iter * X.shape[0] . \n",
    "        \n",
    "        alpha: The learning rate. Moderates the step size during the gradient descent algorithm.\n",
    "        \"\"\"\n",
    "        self.coef_ = None\n",
    "        self.trained = False\n",
    "        self.n_iter = n_iter\n",
    "        self.alpha_ = alpha\n",
    "        \n",
    "    def shuffle_data(self, X, y):\n",
    "        \"\"\"\n",
    "        Given X and y, shuffle them to get a new_X and new_y that maintain feature-target relationship. \n",
    "        Do this between epochs. \n",
    "        \"\"\"\n",
    "        assert len(X) == len(y)\n",
    "        permute = np.random.permutation(len(y))\n",
    "        return X[permute], y[permute]\n",
    "    \n",
    "    def update(self, data_point, error):\n",
    "        \"\"\"\n",
    "        Update the coefficients using one step of SGD. \n",
    "        Remember the formula for updating betas: B_i = B_i - alpha * dJ/dB_i --> use the gradient/derivative!\n",
    "        ---\n",
    "        Inputs:\n",
    "        \n",
    "        data_point: One observation from the data (this is stochasitc gradient descent)\n",
    "        \n",
    "        error: The residual for the current data point, given the current coefficients. Prediction - True\n",
    "        for the current datapoint and coefficients.\n",
    "        \"\"\"\n",
    "        self.coef_ = self.coef_ - self.alpha_*error*data_point\n",
    "        # You'll want to change this to take b_0 into account \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "    \n",
    "    def init_coef(self):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
