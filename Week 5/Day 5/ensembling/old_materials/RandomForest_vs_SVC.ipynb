{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trees vs Support Vectors\n",
    "\n",
    "Both these classification algorithms are powerful. But one might outperform the other based on the nature of the input data. Here are two examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We choose a 1000 random points between (0,1) in two dimensions, and send the labels to be 0 and 1 based on if they are above or below the diagonal line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(1000, 2)\n",
    "y1 = [1 if i[0]>i[1] else 0 for i in X]\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "def visualize(X, y):\n",
    "    c = cm.rainbow(np.linspace(0, 1, 2))\n",
    "    plt.scatter([i[0] for i in X], [i[1] for i in X], color=[c[i] for i in y], alpha=.5)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.grid(True)\n",
    "    \n",
    "\n",
    "visualize(X, y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The data above is diagonally separable. And just a linear SVC can nail that. As we see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "def quick_test(model, X, y):\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3)\n",
    "    model.fit(xtrain, ytrain)\n",
    "    return model.score(xtest, ytest)\n",
    "\n",
    "def quick_test_afew_times(model, X, y, n=10):\n",
    "    return np.mean([quick_test(model, X, y) for j in range(n)])\n",
    "\n",
    "linearsvc = LinearSVC()\n",
    "# Do the test 10 times with a LinearSVC and get the average score\n",
    "quick_test_afew_times(linearsvc, X, y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is a tough problem for Decision Tree. It will easily overfit the data and perform poorly. Random Forest corrects for the overfitting and improves the performance, but never gets as good as the Linear SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisiontree = DecisionTreeClassifier(max_depth=2)\n",
    "quick_test_afew_times(decisiontree, X, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomforest = RandomForestClassifier()\n",
    "quick_test_afew_times(randomforest, X, y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we will try a different categorization. Put an orthogonal axis in the middle of x values and make two diagonal quadrants the same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = [1 if (0.5-i[0])*(0.5-i[1])>0 else 0 for i in X]\n",
    "visualize(X, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree thrives on this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisiontree = DecisionTreeClassifier()\n",
    "quick_test_afew_times(decisiontree, X, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVC struggles, since the data isn't linearly seperable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearsvc = LinearSVC()\n",
    "quick_test_afew_times(linearsvc, X, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using an RBF kernel can do better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "quick_test_afew_times(svc, X, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But something to keep in mind. SVC, especially non-linear ones perform best when the data is scaled between -1 and 1. Performance improves once we do that. But still not the perfect score of the Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = SVC()\n",
    "X2 = (0.5-X) * 2\n",
    "quick_test_afew_times(s2, X2, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We looked at two simple problems. Linear SVC and Decision Tree solved a different one perfectly and struggled with the other. When they struggled, sophistications like non-linear kernels and Random Forests helped, but still couldn't reach perfections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It's easy to throw your data into a whole bunch of algorithms and see which one does best. But gaining some intuition for your data and your models, is worth the time and effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Stuff:\n",
    "\n",
    "\n",
    "**DT pros:**\n",
    "- interpretable \n",
    "- DT is good at ignoring noise \n",
    "- NaN ( in theory)\n",
    "- compact : nodes << D after pruning\n",
    "- fast: O(depth) at test time\n",
    "\n",
    "**Cons: **\n",
    "- only axis-aligned splits of data\n",
    "- greedy (may not find the best tree) (exponentially many possible trees) \n",
    "(we're grabbing the best attribute now (not 5 steps ahead)) \n",
    "\n",
    "**Complexity**   \n",
    "At train time: O(mnlogn)   \n",
    "At test time  ~ depth of tree\n",
    "\n",
    "**why O(mn log n)**? \n",
    "(if we have cont. data: a technique for finding the best possible split:\n",
    ": sort the data into ascending order, consider\n",
    "all possible partition points between those data points and & take \n",
    "the one that minimizes entropy) : \n",
    "since we are doing this for every feautre, we have: O(mn log n). \n",
    "\n",
    "** gini is sim. to entropy**: maximal if classes are 50-50;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**hyperparameters**    \n",
    "[numerous parameters!](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)       \n",
    "[viz & definitions (towards the end of page)](https://clearpredictions.com/Home/DecisionTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metis",
   "language": "python",
   "name": "metis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
