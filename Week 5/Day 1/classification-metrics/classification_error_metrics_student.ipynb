{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error metrics for classification models\n",
    "\n",
    "**Lesson objectives**:\n",
    "* Understand why accuracy isn't always the best error metric for classification\n",
    "* Understand the confusion matrix\n",
    "* Learn the difference between precision and recall \n",
    "* Understand what changing the probability cutoff for logistic regression does to precision and recall and how one might use this in model development\n",
    "* Learn how to balance precision and recall using the F1 score\n",
    "* Understand the ROC curve: how to interpret it and how to use it for model tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "np.set_printoptions(suppress=True) # Suppress scientific notation where possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification on the iris dataset with two classifiers: kNN and logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, precision_recall_curve,f1_score, fbeta_score\n",
    "\n",
    "iris_dataset = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual, create our train/test split\n",
    "# This will become particularly important as we calculate our error metrics\n",
    "\n",
    "X_train, X_test, label_train, label_test = train_test_split(iris_dataset['data'], iris_dataset['target'], \\\n",
    "                                                            test_size=0.3, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, label_train)\n",
    "print(\"The score for kNN is\")\n",
    "print(\"Training: {:6.2f}%\".format(100*knn.score(X_train, label_train)))\n",
    "print(\"Test set: {:6.2f}%\".format(100*knn.score(X_test, label_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(C = 0.95)\n",
    "logit.fit(X_train, label_train)\n",
    "print(\"The score for logistic regression is\")\n",
    "print(\"Training: {:6.2f}%\".format(100*logit.score(X_train, label_train)))\n",
    "print(\"Test set: {:6.2f}%\".format(100*logit.score(X_test, label_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike kNN, the logistic regression model has a probabiltiy method built into it. Let's look at the predicted probabilities for the first few observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do a prediction of the first 5 observations from logistic\n",
    "\n",
    "logit.predict_proba(X_test[:5,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the type of errors that these two classifiers made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"kNN confusion matrix: \\n\\n\", confusion_matrix(label_test, knn.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e. the kNN confused two samples of species 1 as species 2, and one species 2 as a species 1, but managed to get all of the species 0 irises right. \n",
    "\n",
    "Note that `confusion[row_number][col_number]` tells us how many of the labels that were actually `row_number` were predicted to be `col_number` by our classifier. We can see that by predicting class 2 for everything (this is not data science, it's just for demonstration!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens when we just predict class 2 for everything? \n",
    "print(\"Confusion matrix for silly model where we predict all 2's: \\n\\n\", \\\n",
    "      confusion_matrix(label_test, [2]*len(label_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make this a little prettier by passing the confusion matrix to `sns.heatmap`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print confusion matrix for kNN\n",
    "knn_confusion = confusion_matrix(label_test, knn.predict(X_test))\n",
    "plt.figure(dpi=150)\n",
    "sns.heatmap(knn_confusion, cmap=plt.cm.Blues, annot=True, square=True,\n",
    "           xticklabels=iris_dataset['target_names'],\n",
    "           yticklabels=iris_dataset['target_names'])\n",
    "\n",
    "plt.xlabel('Predicted species')\n",
    "plt.ylabel('Actual species')\n",
    "plt.title('kNN confusion matrix');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print confusion matrix for logistic regression\n",
    "logit_confusion = confusion_matrix(label_test, logit.predict(X_test))\n",
    "plt.figure(dpi=150)\n",
    "sns.heatmap(logit_confusion, cmap=plt.cm.Blues, annot=True, square=True,\n",
    "           xticklabels=iris_dataset['target_names'],\n",
    "           yticklabels=iris_dataset['target_names'])\n",
    "\n",
    "plt.xlabel('Predicted species')\n",
    "plt.ylabel('Actual species')\n",
    "plt.title('Logistic regression confusion matrix');\n",
    "\n",
    "plt.savefig(\"confusion_matrix_logit_iris\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why accuracy sometimes doesn't make sense: credit card data example\n",
    "\n",
    "We are going to look at an example of credit card data, where the positive class is \"transaction is fraudulant\" and the negative class is \"transaction is not fraudulant\".\n",
    "\n",
    "That is, a false positive is a transaction we flag as fraudulant that is actually legitimite, whereas a false negative is a transaction we claim is legitimate that turns out to be fraudulant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read in some credit card data!\n",
    "df = pd.read_csv('https://s3.amazonaws.com/gamma-datasets/P3/creditcard.csv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**STUDENT EXERCISE:**\n",
    "\n",
    "(1) What's the distribution of fraud to non-fraud cases? \n",
    "\n",
    "(2) Now build a confusion matrix where you predict everything as not fraud! Is this helpful? \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a slightly better model\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, 1:-1], df.iloc[:, -1])\n",
    "lm = LogisticRegression(C = 100)\n",
    "lm.fit(X_train, y_train)\n",
    "print(\"Logistic score: {:6.4f}\".format(lm.score(X_train,y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_confusion_matrix(model, threshold=0.5):\n",
    "    # Predict class 1 if probability of being in class 1 is greater than threshold\n",
    "    # (model.predict(X_test) does this automatically with a threshold of 0.5)\n",
    "    y_predict = (model.predict_proba(X_test)[:, 1] >= threshold)\n",
    "    fraud_confusion = confusion_matrix(y_test, y_predict)\n",
    "    plt.figure(dpi=80)\n",
    "    sns.heatmap(fraud_confusion, cmap=plt.cm.Blues, annot=True, square=True, fmt='d',\n",
    "           xticklabels=['legit', 'fraud'],\n",
    "           yticklabels=['legit', 'fraud']);\n",
    "    plt.xlabel('prediction')\n",
    "    plt.ylabel('actual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_confusion_matrix(lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how our confusion matrix changes with changes to the cutoff! \n",
    "\n",
    "from ipywidgets import interactive, FloatSlider\n",
    "\n",
    "interactive(lambda threshold: make_confusion_matrix(lm, threshold), threshold=(0.0,1.0,0.02))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CHECK FOR UNDERSTANDING:**\n",
    "\n",
    "What happens if we change our threshold to .06? What does this mean? How would our confusion matrix look?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## New metrics: precision and recall\n",
    "\n",
    "We can introduce two new metrics:\n",
    "- **Precision:** The fraction of postive predictions you made that were correct. \n",
    "  High precision means that if your model predicted a positive case, you believe it with high confidence. It doesn't tell us how many postive cases we missed (i.e. it doesn't tell us how sure we are about the cases we predicted were negative).\n",
    "- **Recall**: The fraction of positive cases you predicted correctly.\n",
    "  High recall means that you are confident that you didn't miss any positive cases. \n",
    "  \n",
    "The easiest way to get high precision is to call a point positive only when you are very sure. The easiest way to get 100% recall is to predict all points to be positive, as you are guaranteed to label 100% of all positive cases. You should not optimize for either recall or precision -- you need to think about how to balance them against one another.\n",
    "\n",
    "In formula:\n",
    "$$ \\text{Precision} = \\frac{\\text{(True positives)}}{\\text{Predicted positives}} = \\frac{\\text{True positives}}{\\text{True positives} + \\text{False postitives}}$$\n",
    "and\n",
    "$$ \\text{Recall} = \\frac{\\text{True positives}}{\\text{Actual positives}} = \\frac{\\text{True positives}}{\\text{True positives} + \\text{False negatives}}$$\n",
    "\n",
    "### Example calcuation: fraud when threshold = 0.5 \n",
    "\n",
    "|_                            | predicted negative (legit) | predicted positive (fraud) | Total |\n",
    "|-----------------------------| -------------------------- | -------------------------- | ----- |\n",
    "| **actual negative (legit)** | 71058                      | 11                         | 71069 |\n",
    "| **actual positive (fraud)** | 47                         | 86                         | 133   |\n",
    "| **Total**                   | 71105                      | 97                         | 71202 |\n",
    "\n",
    "\n",
    "For precision:\n",
    "$$\\text{Precision} = \\frac{TP}{\\text{predicted positive}} = \\frac{86}{86 + 11} = 0.887$$\n",
    "i.e. this 88.7% of the tests positive predictions are correct.\n",
    "\n",
    "For recall:\n",
    "$$\\text{Recall} = \\frac{TP}{\\text{actual positives}} = \\frac{86}{86+47} = 0.647$$\n",
    "i.e. the model can only get (or 'remember'/'recall') 64.7% of fraudulent transactions.\n",
    "\n",
    "\n",
    "### Example calculation: fraud when threshold = 0.06\n",
    "\n",
    "|-                            | predicted negative (legit) | predicted positive (fraud) | Total |\n",
    "|-----------------------------| -------------------------- | -------------------------- | ----- |\n",
    "| **actual negative (legit)** | 71039                      | 30                         | 71069 |\n",
    "| **actual positive (fraud)** | 23                         | 110                        | 133   |\n",
    "| **Total**                   | 71062                      | 140                        | 71202 |\n",
    "\n",
    "Calculating precision and recall:\n",
    "* Precision = 110 / 140 = 78.5%\n",
    "* Recall = 110 / 133 = 82.7%\n",
    "i.e. by lowering the probability threshold before we label a transaction fraud, we caught more fraud (high recall), but we also investigated more innocent transactions (lower precision).\n",
    "\n",
    "## Big takeaways:\n",
    "\n",
    "* Using the **same** logistic regression model, we can change the threshold to bias toward more precision (making positives from test more relevant) or recall (increasing the fraction of postives found).\n",
    "* Precision goes down as you decrease the threshold, while recall goes up. This is called the _precision-recall tradeoff_.\n",
    "* Which is worse, low recall or low precision? Depends on the cost of making the different types of error.\n",
    "* We just need the final predictions of the model to calculate precision and recall. We can get sklearn.metrics to calculate them for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the default threshold of 0.5, which is what vanilla predict does\n",
    "y_predict = lm.predict(X_test)\n",
    "print(\"Default threshold:\")\n",
    "print(\"Precision: {:6.4f},   Recall: {:6.4f}\".format(precision_score(y_test, y_predict), \n",
    "                                                     recall_score(y_test, y_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the new threshold of 0.06\n",
    "y_predict = (lm.predict_proba(X_test)[:,1] > 0.06)\n",
    "print(\"Threshold of 0.06:\")\n",
    "print(\"Precision: {:6.4f},   Recall: {:6.4f}\".format(precision_score(y_test, y_predict), \n",
    "                                                     recall_score(y_test, y_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use the probabilities to make a curve showing us how recall \n",
    "# and thresholds trade off \n",
    "\n",
    "precision_curve, recall_curve, threshold_curve = precision_recall_curve(y_test, lm.predict_proba(X_test)[:,1] )\n",
    "\n",
    "plt.figure(dpi=80)\n",
    "plt.plot(threshold_curve, precision_curve[1:],label='precision')\n",
    "plt.plot(threshold_curve, recall_curve[1:], label='recall')\n",
    "plt.legend(loc='lower left')\n",
    "plt.xlabel('Threshold (above this probability, label as fraud)');\n",
    "plt.title('Precision and Recall Curves');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=80)\n",
    "plt.plot(recall_curve[1:], precision_curve[1:],label='precision')\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 score: balancing precision vs. recall\n",
    "\n",
    "A heuristic for finding a good balance is to use $F_1$ score: \n",
    "\n",
    "$$F_1 = \\frac{2}{\\frac{1}{\\text{precision}} + \\frac{1}{\\text{recall}}} = 2 \\frac{\\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}}$$\n",
    "\n",
    "The goal here is to find a balance between a good recall score and a good precision score. We can calculate it manually.\n",
    "\n",
    "For the default threshold we have\n",
    "\n",
    "precision = 0.8810, recall = 0.6218\n",
    "\n",
    "so\n",
    "\n",
    "$$F_1 = 2\\frac{(0.8810)(0.6218)}{0.8810 + 0.6218} = 0.7290$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**STUDENT EXERCISE:**\n",
    "\n",
    "Use `f1_score` from sklearn to check our calculation above. Look at the F1 score for (1) default threshold and (2) threshold of .06\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalizing the F1 score\n",
    "\n",
    "As the name suggests, there is actually a whole family of $F_1$ scores. We have in general:\n",
    "\n",
    "$$F_{\\beta} = \\frac{\\text{precision} \\times \\text{recall}}{\\text{weighted average of precision and recall}}$$\n",
    "\n",
    "where the weighted average is given by\n",
    "\n",
    "$$\\text{weighted average} = \\frac{\\beta^2 \\times \\text{precision} + 1 \\times \\text{recall}}{\\beta^2 + 1}$$\n",
    "\n",
    "The rough interpretation is that $F_{\\beta}$ attaches $\\beta$ times more importance to recall than precision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ROC curve\n",
    "\n",
    "We've already seen that we don't have to accept a 50% threshold cutoff. As we've seen, we can plot our models with different thresholds on the same chart and get a ROC curve. This curve plots the *true positive rate* on the y axis, and the *false positive rate* on the x axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision = TP / (TP + FP)\n",
    "# Recall = TP/P = True positive rate\n",
    "# false positive rate = FP / true negatives = FP / (FP + TN) \n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, lm.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr, tpr,lw=2)\n",
    "plt.plot([0,1],[0,1],c='violet',ls='--')\n",
    "plt.xlim([-0.05,1.05])\n",
    "plt.ylim([-0.05,1.05])\n",
    "\n",
    "\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve for fraud problem');\n",
    "print(\"ROC AUC score = \", roc_auc_score(y_test, lm.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intepretation of the _Area Under the Curve_ (AUC) is the probability that a randomly chosen positive example (in this case, fraud) has a higher score than the randomly chosen negative example (in this case, legitimate transactions).\n",
    "\n",
    "We see here all the possible TPRs and FNRs that we could have. It is a nice metric because looking at the behavior of our model, we can choose later what we want the threshold to be.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1: What would a \"perfect classifier\" look like on this diagram?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2: What is the significance of the diagonal line?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3: Is it possible to get a classifier that dips below the dotted line?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4: Do we have to worry about the class imbalance when using a ROC curve?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5: Can we read the threshold off the ROC curve above?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Loss Cross-Entropy\n",
    "\n",
    "_A more intensive metric for measuring the performance of your models._ So far, everything we've discussed uses \"hard-predictions\" to determine how well the model is doing. You apply some threshold and decided, \"yes, I believe this is class XYZ\" before checking to see if it's actually XYZ. \n",
    "\n",
    "That's an okay method, but it doesn't really give us a sense of how close the model was to being wrong. E.g., if we have a threshold of `0.5` and a record that's actually class 1, precision, recall, accuracy, etc. all give equal credit to a prediction that got the class correct, whether it was with `p = 0.51` or `p=0.99`. The second is much higher confidence though! \n",
    "\n",
    "Similarly, they give equal penalty to `p = 0.49` and `p=0.01`, though we were much more confident (in the wrong direction) in the second case. Guesses around `0.5` shouldn't be penalized as much, because our model is already admitting \"hey, I don't really know what this observation is.\" \n",
    "\n",
    "Enter log-loss. For the binary case, we can define log-loss:\n",
    "\n",
    "$$ LLCE = - \\sum_{i=1}^{N} y \\cdot log(p_{y}) + (1-y) \\cdot log((1-p_{y}))$$\n",
    "\n",
    "What does this do? It says, \"If the class is actually 1, did you predict close to p=1? If not, that's a big penalty. Though, we'll penalize you way more if you predicted close to p=0. If the class is actually 0, did you predict close to p=0? If not, that's another big penalty.\"\n",
    "\n",
    "The main take-aways here are:\n",
    "\n",
    ">- Log-loss allows us to measure not just \"did our model get XYZ right?\" but \"how confident was it in when predicting?\"\n",
    ">\n",
    ">- A lower log-loss is better\n",
    "\n",
    "The cost function typically used for Gradient Descent on classification is log-loss-cross-entropy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "print(\"Log-loss on logit: {:6.4f}\".format(log_loss(y_test, lm.predict_proba(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dc = DummyClassifier()\n",
    "dc.fit(X_train, y_train)\n",
    "print(\"Log-loss on dummy classifier: {:6.4f}\".format(log_loss(y_test, dc.predict_proba(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic significantly outperforms the dummy here (since it's much smarter about its \"how confident am I\") despite not really outperforming the dummy model much in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic accuracy: {:6.4f}\".format(lm.score(X_test, y_test)))\n",
    "print(\"Dummy accuracy: {:6.4f}\".format(dc.score(X_test, y_test)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "metis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": false,
   "nav_menu": {
    "height": "210px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": false,
   "threshold": "3",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
