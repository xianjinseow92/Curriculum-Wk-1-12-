{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%html\n",
    "#<link rel=\"stylesheet\" href=\"static/hyrule.css\" type=\"text/css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Get familiar with the theory behind Support Vector Machines\n",
    "- Learn how to Optimize with our 'C' variable\n",
    "- Work with Kernels  & more optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "term / variable | definition\n",
    "--------|----------\n",
    "Vector | a quantity having direction and magnitude\n",
    "Hyperplane |  a subspace of one dimension less than the ambient space, which allows us to split our space.\n",
    "Kernel | function that enables us to transform our euclidean geometry in some way.  (Think similiarity function ~ dot products between vectors). \n",
    "$$b$$| intercept (hyperplane translation from origin)\n",
    "$$\\vec{w}$$ | normal vector (determines hyperplane orientation)\n",
    "$$\\vec{x_i}$$ | sample vector\n",
    "$$\\alpha_i$$ | Lagrangian weight\n",
    "$$C $$|  Regularization Term ( Allows us to vary our number of support vectors, ie margin of our 'street') (default = 1)\n",
    "$$epsilon$$ | our slack variable\n",
    "$$kernel$$ |  scikit learn parameter. Specify kernel of choice : 'linear','rbf','poly' (default is rbf)\n",
    "$$order$$ | degree of polynomial kernel function (default is 3)\n",
    "$$gamma$$ | Kernel gaussian coefficient for 'rbf' kernel (default is 0) \n",
    "\n",
    "reference : http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Class Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "from sklearn import datasets, svm\n",
    "iris=datasets.load_iris()\n",
    "X=iris.data\n",
    "data=pd.DataFrame(X)\n",
    "data['label']=iris.target\n",
    "\n",
    "set0=data[data.label==0][[0,1]]\n",
    "set1=data[data.label==1][[0,1]]\n",
    "set2=data[data.label==2][[0,1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How would we split this dataset?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8), dpi=80)\n",
    "plt.scatter(set0[0],set0[1], c='g')\n",
    "plt.scatter(set1[0],set1[1], c='b',alpha =.8)\n",
    "plt.xlabel('sepal length')\n",
    "plt.ylabel('sepal width')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And How would we split THIS dataset? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8), dpi=80)\n",
    "plt.scatter(set1[0],set1[1], c='b')\n",
    "plt.scatter(set2[0],set2[1], c='r',alpha =.8)\n",
    "plt.xlabel('sepal length')\n",
    "plt.ylabel('sepal width')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HOW? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='image2.png'/>\n",
    "\n",
    "\n",
    "$$\\vec{w}\\cdot\\vec{x} +b \\geq 0   ?$$   \n",
    "\n",
    "<img src='image3.png'/>\n",
    " \n",
    "##### Our Constraints: (EQNs 1) \n",
    "\n",
    "$$ w\\cdot{{x_2}} + b = +1 $$\n",
    "$$ w\\cdot{{x_1}} + b = -1 $$\n",
    "$$ (w\\cdot{{x_2}}) -(w\\cdot{{x_1}})= 2 $$\n",
    "\n",
    "$$width =({x}_{2}-{x}_{1})\\cdot{{\\vec{w}\\above 1pt\\|w\\|}}={{2\\above 1pt\\|w\\|}}$$\n",
    "\n",
    "#####  What we are trying to minimize..  (EQNs 2) \n",
    "\n",
    "$$MIN :  {{1/2*\\|w\\|^2}}$$\n",
    "\n",
    "\n",
    " s.t. \n",
    "$$(w.x +b) >= 1 $$ (for our positive class)\n",
    "$$(w.x+b) <= -1 $$ (for our negative class)\n",
    " \n",
    " \n",
    "\n",
    " The beauty of the SVM is that if the data is linearly separable, there is a \n",
    " unique global min. value.  \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Lagrange Multipliers:\n",
    "This is constrained optimization problem, then we'll use a technique called: Lagrange Multipliers. \n",
    "Regarranging the above  inserting our Lagrange Mulipliers (alpha) we have:\n",
    "\n",
    "######  This will be our cost function (EQN 3) \n",
    "\n",
    "  $$ L=  1/2{{\\|w\\|}^2} - \\sum \\alpha_i[(y_i(\\vec{w}\\cdot{x_i} +b)-1] $$\n",
    "  \n",
    "  \n",
    "#####  And how do we find minima  ?   \n",
    "- taking partial derivative of above with respect to w), we find that :  \n",
    "(EQN 4) \n",
    "\n",
    "  $$\\vec{w} = \\sum \\alpha_i(y_i{x_i})$$\n",
    "  \n",
    "- (We see that vector w is just a linear sum of our samples!)\n",
    "\n",
    "- And taking partial derivative of above (with respect to b) we have:  \n",
    "(EQN 5) \n",
    "\n",
    "  $$ \\sum \\alpha_i(y_i)=0$$\n",
    "  \n",
    "- We can inject these derivatives back into our equation..  \n",
    "Now we have the maximum of the margin emerges!  \n",
    "(EQN 6) \n",
    " \n",
    "  $$ L= \\sum \\alpha_i -1/2\\sum\\sum\\alpha_i\\alpha_j(y_iy_j)(x_i\\cdot{x_j})$$\n",
    "\n",
    "Our optimization has complete dependence on pairs of samples..\n",
    "\n",
    "\n",
    "#####  Our final Hyperplane Equation  \n",
    "(EQN 7) \n",
    "\n",
    " \n",
    "  $$ \\sum\\alpha_iy_i(x_i\\cdot\\vec{u}) + b = 1 $$\n",
    "  (for positive samples) \n",
    "  \n",
    "  $$ \\sum\\alpha_iy_i(x_i\\cdot\\vec{u}) + b = -1 $$\n",
    "  (for negative samples)\n",
    "    \n",
    "\n",
    "Again, our final equation has complete dependence on pairs of samples!\n",
    " \n",
    "It still doesn't seem possible that we can find a solution for the dataset above, using just the above equations .. right ?    We will illustrate this in a trivial example.. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Trivial Example:\n",
    "\n",
    "- Lets take our 2 points from Figure 3.  These points are our support vectors\n",
    "- We would like to discover a simple SVM that accurately discriminates between \n",
    "our two classes.\n",
    "\n",
    " $ x_1= \\begin{bmatrix}5.4\\\\3\\end{bmatrix}$  (positive sample)\n",
    "\n",
    " $ x_2 =\\begin{bmatrix}5\\\\3\\end{bmatrix}$    (negative sample)\n",
    "\n",
    "In order to account for our bias input, we can adjust our vectors as follows:\n",
    "    \n",
    " $ {x_1}= \\begin{bmatrix}5.4\\\\3\\\\1\\end{bmatrix}$\n",
    " \n",
    " $ x_2 =\\begin{bmatrix}5\\\\3\\\\1\\end{bmatrix}$\n",
    " \n",
    " Using the structure we developed in our final equation, we have:\n",
    " \n",
    "  $$\\alpha_1(x_1\\cdot(x_1)) + \\alpha_2(x_1\\cdot(x_2)) = +1 $$\n",
    "  $$\\alpha_1(x_1\\cdot(x_2)) + \\alpha_2(x_2\\cdot(x_2)) = -1 $$\n",
    "  \n",
    "  $$ (39.16)\\alpha_1 + (37)\\alpha_2 = +1 $$\n",
    "  $$ (37)\\alpha_1 + (35)\\alpha_2 = - 1 $$\n",
    "  \n",
    "  A little algebra will give us a solution to the system of equations:\n",
    "  \n",
    "  $$\\alpha_1= 2.18$$\n",
    "  $$\\alpha_2 =-1.25$$\n",
    "  \n",
    "  \n",
    "  Using EQN 4), we can find our hyperplane \n",
    "  \n",
    "  \n",
    "   $$\\vec{w} = \\sum \\alpha_i({x_i})$$\n",
    "   \n",
    "   =$$2.18\\begin{bmatrix}5.4\\\\3\\\\1\\end{bmatrix}  - 1.25\\begin{bmatrix}5\\\\3\\\\1\\end{bmatrix} =$$\n",
    "   \n",
    "   $$\\begin{bmatrix}5.522\\\\2.79\\\\.93 \\end{bmatrix}$$\n",
    "   \n",
    "   Alas, the above vector describes our separating hyperplane equation ( (with the last value as our bias)\n",
    "   \n",
    "   y = wx + b , where w = $$\\begin{bmatrix}5.522\\\\2.79\\end{bmatrix}$$\n",
    "   and b = 0.93\n",
    "   \n",
    "   \n",
    "   Finally: we can visualize our Hyperplane!\n",
    "   \n",
    "   <img src='iris_split4.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review: \n",
    "\n",
    "-  Which parameters determine our hyperplane ?\n",
    "\n",
    "-  What Support Vectors did we use ?\n",
    "\n",
    "-  What are the advantages of using few support vectors vs. many?\n",
    "\n",
    "-  How do you feel SVMs might behave in regards to outliers ? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, svm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order gain more intuition about how we are splitting our grid: we'll build the plot_estimator \n",
    "def plot_estimator(estimator,X,y):\n",
    "    estimator.fit(X,y)\n",
    "    x_min, x_max =X[:,0].min() -1, X[:,0].max() +.1\n",
    "    y_min, y_max =X[:,1].min() -1, X[:,1].max() +.1\n",
    "    xx, yy =np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                        np.linspace(y_min, y_max, 100))\n",
    "                        \n",
    "    Z= estimator.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "                        \n",
    "    # Put the result into a color plot\n",
    "    Z=Z.reshape (xx.shape)\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.pcolormesh(xx,yy, Z, cmap=plt.cm.Paired)\n",
    "                        \n",
    "    # Lets plot our sample points\n",
    "    plt.scatter(X[:,0], X[:,1],c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How can we control the size of our margin? \n",
    "\n",
    "# Note that svm.LinearSVC uses the one vs. all methodology \n",
    "# where SVC implements one vs one \n",
    "\n",
    "iris=datasets.load_iris()\n",
    "X=iris.data\n",
    "y=iris.target\n",
    "\n",
    "X=X[y !=2,:2] \n",
    "y = iris.target[y!=2]\n",
    "\n",
    "svc = svm.SVC(kernel='linear', C=10).fit(X, y)\n",
    "#svc = svm.LinearSVC(C=1).fit(X, y)\n",
    "plot_estimator(svc,X,y)\n",
    "plt.scatter(svc.support_vectors_[:,0],svc.support_vectors_[:,1], c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C ~ is a Regularization Parameter:\n",
    "- small C allows -> large margins -> larger number of support vectors -> potentially allow for misclassified training examples\n",
    "- large C  -> smaller margin (if that margin does a better job of correctly classifying the training points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Else can we do ?   The Kernel Trick.\n",
    "\n",
    "\n",
    "If we are stuck in a space that is not linearly separable: simply switch to a new space.\n",
    "\n",
    "What we need is a transformation (i.e. a Kernel!)\n",
    "\n",
    "Remember we said that our maximization only depends on dot products?\n",
    "Luckily, a kernel method is an algorithm that depends only on dot products.\n",
    "Our dot products can be replaced by a kernel function which computes a dot product in some (potentially) higher dimensional feature space.\n",
    "\n",
    "This allows us to generate a non-linear decision boundary.   Further, this allows us to apply a classifier to data that would normally have no obvious dividing plane.\n",
    "\n",
    "Let's see what this actually looks like.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "\n",
    "def fn_kernel(x1, x2):\n",
    "    \"\"\" Implements a kernel phi(x1,y1) = [x1, y1, x1^2 + y1^2] \"\"\"\n",
    "    return np.array([x1, x2, x1**2.0 + x2**2.0])\n",
    "\n",
    "\"\"\" Generate linearly nonseparable dataset (in R^2) \"\"\"\n",
    "    \n",
    "n = 200\n",
    "\n",
    "\n",
    "# make_circles: a simple binary dataset that allows to visualize clustering and clasification\n",
    "\n",
    "X, Y = make_circles(n_samples=n, noise=0.07, factor=0.4)\n",
    "\n",
    "A=X[Y==0]\n",
    "B=X[Y==1]\n",
    "\n",
    "# where X,Ys are in regards to our cartesian coordinate \n",
    "X0_orig=A[:,0]\n",
    "Y0_orig=A[:,1]\n",
    "\n",
    "X1_orig=B[:,0]\n",
    "Y1_orig=B[:,1]\n",
    "\n",
    "frac0=len(A)/float(len(Y))\n",
    "frac1=len(B)/float(len(Y))\n",
    "\n",
    "print \"Percentage of '0' labels:\", frac0\n",
    "print \"Percentage of '1' labels:\", frac1\n",
    "\n",
    "\n",
    "# What does zip function do?\n",
    "# Returns a list of tuples, where each tuple contains the ith element.. \n",
    "\n",
    "C= np.array([fn_kernel(x,y) for x,y in zip((X0_orig),(Y0_orig))])\n",
    "X0=C[:,0]\n",
    "Y0=C[:,1]\n",
    "Z0=C[:,2]\n",
    "\n",
    "D= np.array([fn_kernel(x,y) for x,y in zip((X1_orig),(Y1_orig))])\n",
    "X1=D[:,0]\n",
    "Y1=D[:,1]\n",
    "Z1=D[:,2]\n",
    "\n",
    "def plot_projection():    \n",
    "    fig = plt.figure(figsize=(8,15))\n",
    "   \n",
    "\n",
    "    # Project data to X/Y plane\n",
    "    ax2d = fig.add_subplot(211)\n",
    "    ax2d.scatter(X0, Y0, c='r', marker='o')\n",
    "    ax2d.scatter(X1, Y1, c='b', marker='^')\n",
    "\n",
    "    ax2d.set_xlabel('X Label')\n",
    "    ax2d.set_ylabel('Y Label')\n",
    "    ax2d.set_title(\"Data projected to R^2 (nonseparable)\")\n",
    "    \n",
    "    ax = fig.add_subplot(212, projection='3d')\n",
    "\n",
    "    ax.scatter(X0, Y0, Z0, c='r', marker='o')\n",
    "    ax.scatter(X1, Y1, Z1, c='b', marker='^')\n",
    "\n",
    "    ax.set_xlabel('X Label')\n",
    "    ax.set_ylabel('Y Label')\n",
    "    ax.set_zlabel('Z Label')\n",
    "    ax.set_title(\"Data in R^3 (separable)\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def main():\n",
    "    \n",
    "    print 'Projecting dataset to R^3'\n",
    "    plot_projection()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Types of Kernels\n",
    "\n",
    "- scikit-learn provides us with several types of Kernels to work with:\n",
    "\n",
    "1) 'linear' : Linear decision boundary\n",
    "        \n",
    "    \n",
    "2) 'poly' : Polynomial decicion boundary (adjust the order via 'order' argument)\n",
    "\n",
    "    \n",
    "3) 'rbf': Radial Basis Function decision boundary ( inserts Gaussian kernel at each support vector, we can adjust the Gaussian kernel via the Gamma Feature)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# develop your \"tuned parameters\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "\n",
    "\n",
    "iris=datasets.load_iris()\n",
    "X=iris.data\n",
    "y=iris.target\n",
    "\n",
    "X=X[y !=0,:2] \n",
    "y = iris.target[y!=0]\n",
    "\n",
    "\n",
    "# To apply an classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "n_samples = len(iris)\n",
    "\n",
    "\n",
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1,1e-1,1e-2,1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},{'kernel': ['linear'], 'C': [1, 10, 100, 1000]},{'kernel':['poly'],'degree':[1,2,3]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=5, scoring=score)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_estimator_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    for params, mean_score, scores in clf.grid_scores_:\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean_score, scores.std() / 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "metis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
